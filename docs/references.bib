@article{stringer2021,
	title        = {Cellpose: a generalist algorithm for cellular segmentation},
	shorttitle   = {Cellpose},
	author       = {Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},
	year         = 2021,
	month        = 1,
	journal      = {Nature Methods},
	publisher    = {Nature Publishing Group},
	volume       = 18,
	number       = 1,
	pages        = {100--106},
	doi          = {10.1038/s41592-020-01018-x},
	issn         = {1548-7105},
	url          = {https://www.nature.com/articles/s41592-020-01018-x},
	urldate      = {2024-06-03},
	language     = {en}
}
@article{von_chamier2021,
	title        = {Democratising deep learning for microscopy with {ZeroCostDL4Mic}},
	author       = {von Chamier, Lucas and Laine, Romain F. and Jukkala, Johanna and Spahn, Christoph and Krentzel, Daniel and Nehme, Elias and Lerche, Martina and Hernández-Pérez, Sara and Mattila, Pieta K. and Karinou, Eleni and Holden, Séamus and Solak, Ahmet Can and Krull, Alexander and Buchholz, Tim-Oliver and Jones, Martin L. and Royer, Loïc A. and Leterrier, Christophe and Shechtman, Yoav and Jug, Florian and Heilemann, Mike and Jacquemet, Guillaume and Henriques, Ricardo},
	year         = 2021,
	month        = 4,
	journal      = {Nature Communications},
	volume       = 12,
	number       = 1,
	pages        = 2276,
	doi          = {10.1038/s41467-021-22518-0},
	issn         = {2041-1723},
	url          = {https://www.nature.com/articles/s41467-021-22518-0},
	urldate      = {2022-06-24},
	language     = {en},
	ublisher     = {Nature Publishing Group}
}
@article{guo_deep_2025,
	title        = {Deep learning-based aberration compensation improves contrast and resolution in fluorescence microscopy},
	author       = {Guo, Min and Wu, Yicong and Hobson, Chad M. and Su, Yijun and Qian, Shuhao and Krueger, Eric and Christensen, Ryan and Kroeschell, Grant and Bui, Johnny and Chaw, Matthew and Zhang, Lixia and Liu, Jiamin and Hou, Xuekai and Han, Xiaofei and Lu, Zhiye and Ma, Xuefei and Zhovmer, Alexander and Combs, Christian and Moyle, Mark and Yemini, Eviatar and Liu, Huafeng and Liu, Zhiyi and Benedetto, Alexandre and La Riviere, Patrick and Colón-Ramos, Daniel and Shroff, Hari},
	year         = 2025,
	month        = 1,
	journal      = {Nature Communications},
	volume       = 16,
	number       = 1,
	pages        = 313,
	doi          = {10.1038/s41467-024-55267-x},
	issn         = {2041-1723},
	url          = {https://doi.org/10.1038/s41467-024-55267-x},
	abstract     = {Optical aberrations hinder fluorescence microscopy of thick samples, reducing image signal, contrast, and resolution. Here we introduce a deep learning-based strategy for aberration compensation, improving image quality without slowing image acquisition, applying additional dose, or introducing more optics. Our method (i) introduces synthetic aberrations to images acquired on the shallow side of image stacks, making them resemble those acquired deeper into the volume and (ii) trains neural networks to reverse the effect of these aberrations. We use simulations and experiments to show that applying the trained ‘de-aberration’ networks outperforms alternative methods, providing restoration on par with adaptive optics techniques; and subsequently apply the networks to diverse datasets captured with confocal, light-sheet, multi-photon, and super-resolution microscopy. In all cases, the improved quality of the restored data facilitates qualitative image inspection and improves downstream image quantitation, including orientational analysis of blood vessels in mouse tissue and improved membrane and nuclear segmentation in C. elegans embryos.}
}
@article{Wu2022,
	title        = {Multiscale fluorescence imaging of living samples},
	author       = {Wu, Yicong and Shroff, Hari},
	year         = 2022,
	month        = 10,
	day          = {01},
	journal      = {Histochemistry and Cell Biology},
	volume       = 158,
	number       = 4,
	pages        = {301--323},
	doi          = {10.1007/s00418-022-02147-4},
	issn         = {1432-119X},
	url          = {https://doi.org/10.1007/s00418-022-02147-4},
	abstract     = {Fluorescence microscopy is a highly effective tool for interrogating biological structure and function, particularly when imaging across multiple spatiotemporal scales. Here we survey recent innovations and applications in the relatively understudied area of multiscale fluorescence imaging of living samples. We discuss fundamental challenges in live multiscale imaging and describe successful examples that highlight the power of this approach. We attempt to synthesize general strategies from these test cases, aiming to help accelerate progress in this exciting area.}
}
@article{Schermelleh2010,
	title        = {A guide to super-resolution fluorescence microscopy},
	author       = {Schermelleh, Lothar and Heintzmann, Rainer and Leonhardt, Heinrich},
	year         = 2010,
	month        = {07},
	journal      = {Journal of Cell Biology},
	volume       = 190,
	number       = 2,
	pages        = {165--175},
	doi          = {10.1083/jcb.201002018},
	issn         = {0021-9525},
	url          = {https://doi.org/10.1083/jcb.201002018},
	abstract     = {For centuries, cell biology has been based on light microscopy and at the same time been limited by its optical resolution. However, several new technologies have been developed recently that bypass this limit. These new super-resolution technologies are either based on tailored illumination, nonlinear fluorophore responses, or the precise localization of single molecules. Overall, these new approaches have created unprecedented new possibilities to investigate the structure and function of cells.},
	eprint       = {https://rupress.org/jcb/article-pdf/190/2/165/1568325/jcb\_201002018.pdf}
}
@article{Archit2025,
	title        = {Segment Anything for Microscopy},
	author       = {Archit, Anwai and Freckmann, Luca and Nair, Sushmita and Khalid, Nabeel and Hilt, Paul and Rajashekar, Vikas and Freitag, Marei and Teuber, Carolin and Spitzner, Melanie and Tapia Contreras, Constanza and Buckley, Genevieve and von Haaren, Sebastian and Gupta, Sagnik and Grade, Marian and Wirth, Matthias and Schneider, G{\"u}nter and Dengel, Andreas and Ahmed, Sheraz and Pape, Constantin},
	year         = 2025,
	month        = 3,
	day          = {01},
	journal      = {Nature Methods},
	publisher    = "Springer Science and Business Media LLC",
	volume       = 22,
	number       = 3,
	pages        = {579--591},
	doi          = {10.1038/s41592-024-02580-4},
	issn         = {1548-7105},
	url          = {https://doi.org/10.1038/s41592-024-02580-4},
	urldate      = {2025-04-17},
	copyright    = {2025 The Author(s)},
	note         = {Publisher: Nature Publishing Group},
	abstract     = {Accurate segmentation of objects in microscopy images remains a bottleneck for many researchers despite the number of tools developed for this purpose. Here, we present Segment Anything for Microscopy ($\mu$SAM), a tool for segmentation and tracking in multidimensional microscopy data. It is based on Segment Anything, a vision foundation model for image segmentation. We extend it by fine-tuning generalist models for light and electron microscopy that clearly improve segmentation quality for a wide range of imaging conditions. We also implement interactive and automatic segmentation in a napari plugin that can speed up diverse segmentation tasks and provides a unified solution for microscopy annotation across different microscopy modalities. Our work constitutes the application of vision foundation models in microscopy, laying the groundwork for solving image analysis tasks in this domain with a small set of powerful deep learning models.},
	language     = "en",
	keywords     = {Image processing, Software}
}
@article{Sahl2017,
	title        = {Fluorescence nanoscopy in cell biology},
	author       = {Sahl, Steffen J. and Hell, Stefan W. and Jakobs, Stefan},
	year         = 2017,
	month        = 11,
	day          = {01},
	journal      = {Nature Reviews Molecular Cell Biology},
	volume       = 18,
	number       = 11,
	pages        = {685--701},
	doi          = {10.1038/nrm.2017.71},
	issn         = {1471-0080},
	url          = {https://doi.org/10.1038/nrm.2017.71},
	abstract     = {Fluorescence nanoscopy (also known as super-resolution microscopy) methods have expanded optical imaging to reach the nanometre resolution range, typically 20--50 nm and even down to the 1 nm level.Diffraction-unlimited nanoscopy methods, which neutralize the resolution-limiting role of diffraction, separate fluorophores by transiently transferring them between (at least) two discernible states, typically an 'on' and an 'off' state of fluorescence.The counting of molecules in nanoscale settings such as within organelles is a crucially important development, along with labelling strategies to reliably pinpoint the locations and spatial proximities of all the molecules investigated in an imaging experiment.Dynamic nanoscopy and extensions of nanoscopy imaging to tissue and in vivo contexts are further frontiers.Examples taken from mitochondrial biology and neurobiology illustrate the capabilities and discovery potential of nanoscale molecule-specific imaging with focused light.}
}
@article{Schermelleh2019,
	title        = {Super-resolution microscopy demystified},
	author       = {Schermelleh, Lothar  and Ferrand, Alexia and Huser, Thomas and Eggeling, Christian and Sauer, Markus and Biehlmaier, Oliver and Drummen, Gregor P. C.},
	year         = 2019,
	month        = 1,
	day          = {01},
	journal      = {Nature Cell Biology},
	volume       = 21,
	number       = 1,
	pages        = {72--84},
	doi          = {10.1038/s41556-018-0251-8},
	issn         = {1476-4679},
	url          = {https://doi.org/10.1038/s41556-018-0251-8},
	abstract     = {Super-resolution microscopy (SRM) bypasses the diffraction limit, a physical barrier that restricts the optical resolution to roughly 250 nm and was previously thought to be impenetrable. SRM techniques allow the visualization of subcellular organization with unprecedented detail, but also confront biologists with the challenge of selecting the best-suited approach for their particular research question. Here, we provide guidance on how to use SRM techniques advantageously for investigating cellular structures and dynamics to promote new discoveries.}
}
@article{Ji2017,
	title        = {Adaptive optical fluorescence microscopy},
	author       = {Ji, Na},
	year         = 2017,
	month        = 4,
	day          = {01},
	journal      = {Nature Methods},
	volume       = 14,
	number       = 4,
	pages        = {374--380},
	doi          = {10.1038/nmeth.4218},
	issn         = {1548-7105},
	url          = {https://doi.org/10.1038/nmeth.4218},
	abstract     = {This Perspective introduces the development and use of adaptive optics in correcting aberrations in deep optical imaging applications.}
}
@article{Hampson2021,
	title        = {Adaptive optics for high-resolution imaging},
	author       = {Hampson, Karen M. and Turcotte, Rapha{\"e}l and Miller, Donald T. and Kurokawa, Kazuhiro and Males, Jared R. and Ji, Na and Booth, Martin J.},
	year         = 2021,
	month        = 10,
	day          = 14,
	journal      = {Nature Reviews Methods Primers},
	volume       = 1,
	number       = 1,
	pages        = 68,
	doi          = {10.1038/s43586-021-00066-7},
	issn         = {2662-8449},
	url          = {https://doi.org/10.1038/s43586-021-00066-7},
	abstract     = {Adaptive optics (AO) is a technique that corrects for optical aberrations. It was originally proposed to correct for the blurring effect of atmospheric turbulence on images in ground-based telescopes and was instrumental in the work that resulted in the Nobel prize-winning discovery of a supermassive compact object at the centre of our galaxy. When AO is used to correct for the eye's imperfect optics, retinal changes at the cellular level can be detected, allowing us to study the operation of the visual system and to assess ocular health in the microscopic domain. By correcting for sample-induced blur in microscopy, AO has pushed the boundaries of imaging in thick tissue specimens, such as when observing neuronal processes in the brain. In this primer, we focus on the application of AO for high-resolution imaging in astronomy, vision science and microscopy. We begin with an overview of the general principles of AO and its main components, which include methods to measure the aberrations, devices for aberration correction, and how these components are linked in operation. We present results and applications from each field along with reproducibility considerations and limitations. Finally, we discuss future directions.}
}
@article{Shroff2024,
	title        = {Live-cell imaging powered by computation},
	author       = {Shroff, Hari and Testa, Ilaria and Jug, Florian and Manley, Suliana},
	year         = 2024,
	month        = 6,
	day          = {01},
	journal      = {Nature Reviews Molecular Cell Biology},
	volume       = 25,
	number       = 6,
	pages        = {443--463},
	doi          = {10.1038/s41580-024-00702-6},
	issn         = {1471-0080},
	url          = {https://doi.org/10.1038/s41580-024-00702-6},
	abstract     = {The proliferation of microscopy methods for live-cell imaging offers many new possibilities for users but can also be challenging to navigate. The prevailing challenge in live-cell fluorescence microscopy is capturing intra-cellular dynamics while preserving cell viability. Computational methods can help to address this challenge and are now shifting the boundaries of what is possible to capture in living systems. In this Review, we discuss these computational methods focusing on artificial intelligence-based approaches that can be layered on top of commonly used existing microscopies as well as hybrid methods that integrate computation and microscope hardware. We specifically discuss how computational approaches can improve the signal-to-noise ratio, spatial resolution, temporal resolution and multi-colour capacity of live-cell imaging.}
}
@article{Venkatesh2015,
	title        = {Directional bilateral filters for smoothing fluorescence microscopy images},
	author       = {Venkatesh, Manasij and Mohan, Kavya and Seelamantula, Chandra Sekhar},
	year         = 2015,
	month        = {08},
	journal      = {AIP Advances},
	volume       = 5,
	number       = 8,
	pages        = {084805},
	doi          = {10.1063/1.4930029},
	issn         = {2158-3226},
	url          = {https://doi.org/10.1063/1.4930029},
	abstract     = {Images obtained through fluorescence microscopy at low numerical aperture (NA) are noisy and have poor resolution. Images of specimens such as F-actin filaments obtained using confocal or widefield fluorescence microscopes contain directional information and it is important that an image smoothing or filtering technique preserve the directionality. F-actin filaments are widely studied in pathology because the abnormalities in actin dynamics play a key role in diagnosis of cancer, cardiac diseases, vascular diseases, myofibrillar myopathies, neurological disorders, etc. We develop the directional bilateral filter as a means of filtering out the noise in the image without significantly altering the directionality of the F-actin filaments. The bilateral filter is anisotropic to start with, but we add an additional degree of anisotropy by employing an oriented domain kernel for smoothing. The orientation is locally adapted using a structure tensor and the parameters of the bilateral filter are optimized for within the framework of statistical risk minimization. We show that the directional bilateral filter has better denoising performance than the traditional Gaussian bilateral filter and other denoising techniques such as SURE-LET, non-local means, and guided image filtering at various noise levels in terms of peak signal-to-noise ratio (PSNR). We also show quantitative improvements in low NA images of F-actin filaments.},
	eprint       = {https://pubs.aip.org/aip/adv/article-pdf/doi/10.1063/1.4930029/12895317/084805\_1\_online.pdf}
}
@article{Danielyan2014,
	title        = {Denoising of two-photon fluorescence images with Block-Matching 3D filtering},
	author       = {Aram Danielyan and Yu-Wei Wu and Pei-Yu Shih and Yulia Dembitskaya and Alexey Semyanov},
	year         = 2014,
	journal      = {Methods},
	volume       = 68,
	number       = 2,
	pages        = {308--316},
	doi          = {https://doi.org/10.1016/j.ymeth.2014.03.010},
	issn         = {1046-2023},
	url          = {https://www.sciencedirect.com/science/article/pii/S1046202314001030},
	note         = {Tools and Methods for Cellular Localization and Measurements},
	keywords     = {Noise modeling, Denoising, Two-photon fluorescent imaging, Ca dynamics},
	abstract     = {Two-photon florescence imaging is widely used to perform morphological analysis of subcellular structures such as neuronal dendrites and spines, astrocytic processes etc. This method is also indispensable for functional analysis of cellular activity such as Ca2+ dynamics. Although spatial resolution of laser scanning two-photon system is greater than that of confocal or wide field microscope, it is still diffraction limited. In practice, the resolution of the system is more affected by its signal-to-noise ratio (SNR) than the diffraction limit. Thus, various approaches aiming to increase the SNR in two-photon imaging are desirable and can potentially save on building costly super-resolution imaging system. Here we analyze the statistics of noise in the two-photon florescence images of hippocampal astrocytes expressing genetically encoded Ca2+ sensor GCaMP2 and show that it can be reasonably well approximated using the same models which are used for describing noise in images acquired with digital cameras. This allows to use denoising methods available for wide field imaging on two-photon images. Particularly we demonstrate that the Block-Matching 3D (BM3D) filter can significantly improve the quality of two-photon fluorescence images so small details such as astrocytic processes can be easier identified. Moreover, denoising of the images with BM3D yields less noisy Ca2+ signals in astrocytes when denoising of the images with Gaussian filter.}
}
@inproceedings{Zhang2019,
	title        = {A Poisson-Gaussian Denoising Dataset With Real Fluorescence Microscopy Images},
	author       = {Zhang, Yide and Zhu, Yinhao and Nichols, Evan and Wang, Qingfei and Zhang, Siyuan and Smith, Cody and Howard, Scott},
	year         = 2019,
	month        = 9,
	journal      = {Biomed. Opt. Express},
	booktitle    = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	publisher    = {Optica Publishing Group},
	pages        = {11702--11710},
	doi          = {10.1109/CVPR.2019.01198},
	url          = {https://ieeexplore.ieee.org/document/8953965},
	keywords     = {Datasets and Evaluation;Medical;Biological and Cell Microscopy},
	abstract     = {Because of the optical properties of medical fluorescence images (FIs) and hardware limitations, light scattering and diffraction constrain the image quality and resolution. In contrast to device-based approaches, we developed a post-processing method for FI resolution enhancement by employing improved generative adversarial networks. To overcome the drawback of fake texture generation, we proposed total gradient loss for network training. Fine-tuning training procedure was applied to further improve the network architecture. Finally, a more agreeable network for resolution enhancement was applied to actual FIs to produce sharper and clearer boundaries than in the original images.}
}
@inproceedings{Li2017,
	title        = {Pure-let deconvolution of 3D fluorescence microscopy images},
	author       = {Li, Jizhou and Luisier, Florian and Blu, Thierry},
	year         = 2017,
	booktitle    = {2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)},
	pages        = {723--727},
	doi          = {10.1109/ISBI.2017.7950621},
	url			 = {https://ieeexplore.ieee.org/document/7950621},
	keywords     = {Microscopy;Three-dimensional displays;Deconvolution;Noise measurement;Bars;Noise level;Wavelet transforms;3D deconvolution;fluorescence microscopy;Poisson noise;unbiased risk estimate}
}
@article{Makitalo2013,
	title        = {Optimal Inversion of the Generalized Anscombe Transformation for Poisson-Gaussian Noise},
	author       = {Makitalo, Markku and Foi, Alessandro},
	year         = 2013,
	journal      = {IEEE Transactions on Image Processing},
	volume       = 22,
	number       = 1,
	pages        = {91--103},
	doi          = {10.1109/TIP.2012.2202675},
	url			 = {https://ieeexplore.ieee.org/document/6212354},
	keywords     = {Noise reduction;Approximation methods;Gaussian noise;Standards;Accuracy;Photonics;Denoising;photon-limited imaging;Poisson-Gaussian noise;variance stabilization}
}
@article{Luisier2010,
	title        = {Fast interscale wavelet denoising of Poisson-corrupted images},
	author       = {Florian Luisier and Cédric Vonesch and Thierry Blu and Michael Unser},
	year         = 2010,
	journal      = {Signal Processing},
	volume       = 90,
	number       = 2,
	pages        = {415--427},
	doi          = {https://doi.org/10.1016/j.sigpro.2009.07.009},
	issn         = {0165-1684},
	url          = {https://www.sciencedirect.com/science/article/pii/S0165168409003016},
	keywords     = {Poisson, Interscale, Denoising, Wavelets, Risk estimation, Linear expansion of thresholds, Fluorescence microscopy},
	abstract     = {We present a fast algorithm for image restoration in the presence of Poisson noise. Our approach is based on (1) the minimization of an unbiased estimate of the MSE for Poisson noise, (2) a linear parametrization of the denoising process and (3) the preservation of Poisson statistics across scales within the Haar DWT. The minimization of the MSE estimate is performed independently in each wavelet subband, but this is equivalent to a global image-domain MSE minimization, thanks to the orthogonality of Haar wavelets. This is an important difference with standard Poisson noise-removal methods, in particular those that rely on a non-linear preprocessing of the data to stabilize the variance. Our non-redundant interscale wavelet thresholding outperforms standard variance-stabilizing schemes, even when the latter are applied in a translation-invariant setting (cycle-spinning). It also achieves a quality similar to a state-of-the-art multiscale method that was specially developed for Poisson data. Considering that the computational complexity of our method is orders of magnitude lower, it is a very competitive alternative. The proposed approach is particularly promising in the context of low signal intensities and/or large data sets. This is illustrated experimentally with the denoising of low-count fluorescence micrographs of a biological sample.}
}
@article{Van_der_Walt2014,
	title        = "scikit-image: image processing in Python",
	author       = "van der Walt, St{\'e}fan and Sch{\"o}nberger, Johannes L and
               Nunez-Iglesias, Juan and Boulogne, Fran{\c c}ois and Warner,
               Joshua D and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony
               and {scikit-image contributors}",
	year         = 2014,
	month        = 6,
	journal      = "PeerJ",
	publisher    = "PeerJ",
	volume       = 2,
	pages        = "e453",
	abstract     = "scikit-image is an image processing library that implements
               algorithms and utilities for use in research, education and
               industry applications. It is released under the liberal Modified
               BSD open source license, provides a well-documented API in the
               Python programming language, and is developed by an active,
               international team of collaborators. In this paper we highlight
               the advantages of open source to achieve the goals of the
               scikit-image library, and we showcase several real-world image
               processing applications that use scikit-image. More information
               can be found on the project homepage, http://scikit-image.org.",
	keywords     = "Education; Image processing; Open source; Python; Reproducible
               research; Scientific programming; Visualization",
	language     = "en",
	url			 = {https://peerj.com/articles/453/}
}
@book{Wiener1949,
	title        = {Extrapolation, Interpolation, and Smoothing of Stationary Time Series: With Engineering Applications},
	author       = {Wiener, Norbert},
	year         = 1949,
	month        = {08},
	publisher    = {The MIT Press},
	doi          = {10.7551/mitpress/2946.001.0001},
	isbn         = 9780262257190,
	url          = {https://doi.org/10.7551/mitpress/2946.001.0001},
	abstract     = {A book thatbecame the basis for modern communication theory, by a scientist considered one of the founders of the field of artifical intelligence.Some predict that Norbert Wiener will be remembered for his Extrapolation long after Cybernetics is forgotten. Indeed, few computer science students would know today what cybernetics is all about, while every communication student knows what Wiener's filter is. The original work was circulated as a classified memorandum in 1942, because it was connected with sensitive wartime efforts to improve radar communication. This book became the basis for modern communication theory, by a scientist considered one of the founders of the field of artifical intelligence. Combining ideas from statistics and time-series analysis, Wiener used Gauss's method of shaping the characteristic of a detector to allow for the maximal recognition of signals in the presence of noise. This method came to be known as the "Wiener filter."},
	eprint       = {https://direct.mit.edu/book-pdf/2313079/book\_9780262257190.pdf}
}
@article{Tikhonov1963,
	title        = {Solution of incorrectly formulated problems and the regularization method},
	author       = {Tikhonov, A. N.},
	year         = 1963,
	journal      = {Soviet Math. Dokl.},
	volume       = 4,
	pages        = {1035--1038},
	added-at     = {2008-10-07T16:03:39.000+0200},
	timestamp    = {2008-10-07T16:03:39.000+0200}
}
@article{Miller1970,
	title        = {Least Squares Methods for Ill-Posed Problems with a Prescribed Bound},
	author       = {Miller, Keith},
	year         = 1970,
	journal      = {SIAM Journal on Mathematical Analysis},
	volume       = 1,
	number       = 1,
	pages        = {52--74},
	doi          = {10.1137/0501006},
	url          = {https://doi.org/10.1137/0501006},
	eprint       = {https://doi.org/10.1137/0501006}
}
@article{Beck2009,
	title        = {A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems},
	author       = {Beck, Amir and Teboulle, Marc},
	year         = 2009,
	journal      = {SIAM Journal on Imaging Sciences},
	volume       = 2,
	number       = 1,
	pages        = {183--202},
	doi          = {10.1137/080716542},
	url          = {https://doi.org/10.1137/080716542},
	eprint       = {https://doi.org/10.1137/080716542},
	abstract     = {Abstract. We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.}
}
@article{Lucy1974,
	title        = "{An iterative technique for the rectification of observed distributions}",
	author       = {{Lucy}, L.~B.},
	year         = 1974,
	month        = 6,
	journal      = {Astronomical Journal},
	volume       = 79,
	pages        = 745,
	doi          = {10.1086/111605},
	adsurl       = {https://ui.adsabs.harvard.edu/abs/1974AJ.....79..745L},
	adsnote      = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{Richardson1972,
	title        = {Bayesian-Based Iterative Method of Image Restoration$\ast$},
	author       = {William Hadley Richardson},
	year         = 1972,
	month        = 1,
	journal      = {J. Opt. Soc. Am.},
	publisher    = {Optica Publishing Group},
	volume       = 62,
	number       = 1,
	pages        = {55--59},
	doi          = {10.1364/JOSA.62.000055},
	url          = {https://opg.optica.org/abstract.cfm?URI=josa-62-1-55},
	keywords     = {Crosstalk; Deconvolution; Image processing; Image restoration; Imaging techniques; Point spread function},
	abstract     = {An iterative method of restoring degraded images was developed by treating images, point spread functions, and degraded images as probability-frequency functions and by applying Bayes's theorem. The method functions effectively in the presence of noise and is adaptable to computer operation.}
}
@article{Sarder2006,
	title        = {Deconvolution methods for 3-D fluorescence microscopy images},
	author       = {Sarder, P. and Nehorai, A.},
	year         = 2006,
	journal      = {IEEE Signal Processing Magazine},
	volume       = 23,
	number       = 3,
	pages        = {32--45},
	doi          = {10.1109/MSP.2006.1628876},
	url			 = {https://ieeexplore.ieee.org/document/1628876},
	keywords     = {Deconvolution;Fluorescence;Microscopy;Optical distortion;Adaptive optics;Biomedical optical imaging;Optical sensors;Optical noise;Optical computing;Optical imaging}
}
@incollection{Goodwin2014,
	title        = {Chapter 10 - Quantitative deconvolution microscopy},
	author       = {Paul C. Goodwin},
	year         = 2014,
	booktitle    = {Quantitative Imaging in Cell Biology},
	publisher    = {Academic Press},
	series       = {Methods in Cell Biology},
	volume       = 123,
	pages        = {177--192},
	doi          = {https://doi.org/10.1016/B978-0-12-420138-5.00010-0},
	issn         = {0091-679X},
	url          = {https://www.sciencedirect.com/science/article/pii/B9780124201385000100},
	editor       = {Jennifer C. Waters and Torsten Wittman},
	keywords     = {Deconvolution, Wide-field microscopy, Point-spread function, Fourier transforms, Deblurring, Image restoration, Live-cell imaging, Contrast},
	abstract     = {The light microscope is an essential tool for the study of cells, organelles, biomolecules, and subcellular dynamics. A paradox exists in microscopy whereby the higher the needed lateral resolution, the more the image is degraded by out-of-focus information. This creates a significant need to generate axial contrast whenever high lateral resolution is required. One strategy for generating contrast is to measure or model the optical properties of the microscope and to use that model to algorithmically reverse some of the consequences of high-resolution imaging. Deconvolution microscopy implements model-based methods to enable the full diffraction-limited resolution of the microscope to be exploited even in complex and living specimens.}
}
@article{Guo2020,
	title        = {Rapid image deconvolution and multiview fusion for optical microscopy},
	author       = {Guo, Min and Li, Yue and Su, Yijun and Lambert, Talley and Nogare, Damian Dalle and Moyle, Mark W. and Duncan, Leighton H. and Ikegami, Richard and Santella, Anthony and Rey-Suarez, Ivan and Green, Daniel and Beiriger, Anastasia and Chen, Jiji and Vishwasrao, Harshad and Ganesan, Sundar and Prince, Victoria and Waters, Jennifer C. and Annunziata, Christina M. and Hafner, Markus and Mohler, William A. and Chitnis, Ajay B. and Upadhyaya, Arpita and Usdin, Ted B. and Bao, Zhirong and Col{\'o}n-Ramos, Daniel and La Riviere, Patrick and Liu, Huafeng and Wu, Yicong and Shroff, Hari},
	year         = 2020,
	month        = 11,
	day          = {01},
	journal      = {Nature Biotechnology},
	volume       = 38,
	number       = 11,
	pages        = {1337--1346},
	doi          = {10.1038/s41587-020-0560-x},
	issn         = {1546-1696},
	url          = {https://doi.org/10.1038/s41587-020-0560-x},
	abstract     = {The contrast and resolution of images obtained with optical microscopes can be improved by deconvolution and computational fusion of multiple views of the same sample, but these methods are computationally expensive for large datasets. Here we describe theoretical and practical advances in algorithm and software design that result in image processing times that are tenfold to several thousand fold faster than with previous methods. First, we show that an `unmatched back projector' accelerates deconvolution relative to the classic Richardson--Lucy algorithm by at least tenfold. Second, three-dimensional image-based registration with a graphics processing unit enhances processing speed 10- to 100-fold over CPU processing. Third, deep learning can provide further acceleration, particularly for deconvolution with spatially varying point spread functions. We illustrate our methods from the subcellular to millimeter spatial scale on diverse samples, including single cells, embryos and cleared tissue. Finally, we show performance enhancement on recently developed microscopes that have improved spatial resolution, including dual-view cleared-tissue light-sheet microscopes and reflective lattice light-sheet microscopes.}
}
@article{Schindelin2012,
	title        = {Fiji: an open-source platform for biological-image analysis},
	author       = {Schindelin, Johannes and Arganda-Carreras, Ignacio and Frise, Erwin and Kaynig, Verena and Longair, Mark and Pietzsch, Tobias and Preibisch, Stephan and Rueden, Curtis and Saalfeld, Stephan and Schmid, Benjamin and Tinevez, Jean-Yves and White, Daniel James and Hartenstein, Volker and Eliceiri, Kevin and Tomancak, Pavel and Cardona, Albert},
	year         = 2012,
	month        = 7,
	day          = {01},
	journal      = {Nature Methods},
	volume       = 9,
	number       = 7,
	pages        = {676--682},
	doi          = {10.1038/nmeth.2019},
	issn         = {1548-7105},
	url          = {https://doi.org/10.1038/nmeth.2019},
	note         = {ISBN: 1548-7105},
	abstract     = {Presented is an overview of the image-analysis software platform Fiji, a distribution of ImageJ that updates the underlying ImageJ architecture and adds modern software design elements to expand the capabilities of the platform and facilitate collaboration between biologists and computer scientists.},
	language     = "en"
}
@article{Sage2017,
	title        = {DeconvolutionLab2: An open-source software for deconvolution microscopy},
	author       = {Daniel Sage and Lauréne Donati and Ferréol Soulez and Denis Fortun and Guillaume Schmit and Arne Seitz and Romain Guiet and Cédric Vonesch and Michael Unser},
	year         = 2017,
	journal      = {Methods},
	volume       = 115,
	pages        = {28--41},
	doi          = {https://doi.org/10.1016/j.ymeth.2016.12.015},
	issn         = {1046-2023},
	url          = {https://www.sciencedirect.com/science/article/pii/S1046202316305096},
	note         = {Image Processing for Biologists},
	keywords     = {Deconvolution microscopy, Open-source software, Standard algorithms, Textbook approach, Reference datasets},
	abstract     = {Images in fluorescence microscopy are inherently blurred due to the limit of diffraction of light. The purpose of deconvolution microscopy is to compensate numerically for this degradation. Deconvolution is widely used to restore fine details of 3D biological samples. Unfortunately, dealing with deconvolution tools is not straightforward. Among others, end users have to select the appropriate algorithm, calibration and parametrization, while potentially facing demanding computational tasks. To make deconvolution more accessible, we have developed a practical platform for deconvolution microscopy called DeconvolutionLab. Freely distributed, DeconvolutionLab hosts standard algorithms for 3D microscopy deconvolution and drives them through a user-oriented interface. In this paper, we take advantage of the release of DeconvolutionLab2 to provide a complete description of the software package and its built-in deconvolution algorithms. We examine several standard algorithms used in deconvolution microscopy, notably: Regularized inverse filter, Tikhonov regularization, Landweber, Tikhonov–Miller, Richardson–Lucy, and fast iterative shrinkage-thresholding. We evaluate these methods over large 3D microscopy images using simulated datasets and real experimental images. We distinguish the algorithms in terms of image quality, performance, usability and computational requirements. Our presentation is completed with a discussion of recent trends in deconvolution, inspired by the results of the Grand Challenge on deconvolution microscopy that was recently organized.}
}
@article{Bazin2007,
	title        = {Volumetric neuroimage analysis extensions for the MIPAV software package},
	author       = {Pierre-Louis Bazin and Jennifer L. Cuzzocreo and Michael A. Yassa and William Gandler and Matthew J. McAuliffe and Susan S. Bassett and Dzung L. Pham},
	year         = 2007,
	journal      = {Journal of Neuroscience Methods},
	volume       = 165,
	number       = 1,
	pages        = {111--121},
	doi          = {https://doi.org/10.1016/j.jneumeth.2007.05.024},
	issn         = {0165-0270},
	url          = {https://www.sciencedirect.com/science/article/pii/S0165027007002270},
	keywords     = {Segmentation, Magnetic resonance imaging, Talairach atlas},
	abstract     = {We describe a new collection of publicly available software tools for performing quantitative neuroimage analysis. The tools perform semi-automatic brain extraction, tissue classification, Talairach alignment, and atlas-based measurements within a user-friendly graphical environment. They are implemented as plug-ins for MIPAV, a freely available medical image processing software package from the National Institutes of Health. Because the plug-ins and MIPAV are implemented in Java, both can be utilized on nearly any operating system platform. In addition to the software plug-ins, we have also released a digital version of the Talairach atlas that can be used to perform regional volumetric analyses. Several studies are conducted applying the new tools to simulated and real neuroimaging data sets.}
}
@article{Booth1861,
	title        = {Adaptive Optics in Microscopy},
	author       = {Martin J. Booth},
	year         = 2007,
	journal      = {Philosophical Transactions: Mathematical, Physical and Engineering Sciences},
	publisher    = {The Royal Society},
	volume       = 365,
	number       = 1861,
	pages        = {2829--2843},
	issn         = {1364503X},
	url          = {http://www.jstor.org/stable/25190627},
	urldate      = {2025-08-19},
	abstract     = {The imaging properties of optical microscopes are often compromised by aberrations that reduce image resolution and contrast. Adaptive optics technology has been employed in various systems to correct these aberrations and restore performance. This has required various departures from the traditional adaptive optics schemes that are used in astronomy. This review discusses the sources of aberrations, their effects and their correction with adaptive optics, particularly in confocal and two-photon microscopes. Different methods of wavefront sensing, indirect aberration measurement and aberration correction devices are discussed. Applications of adaptive optics in the related areas of optical data storage, optical tweezers and micro/nanofabrication are also reviewed.}
}
@article{Hell2007,
	title        = {Far-Field Optical Nanoscopy},
	author       = {Stefan W. Hell},
	year         = 2007,
	journal      = {Science},
	volume       = 316,
	number       = 5828,
	pages        = {1153--1158},
	doi          = {10.1126/science.1137395},
	url          = {https://www.science.org/doi/abs/10.1126/science.1137395},
	eprint       = {https://www.science.org/doi/pdf/10.1126/science.1137395},
	abstract     = {In 1873, Ernst Abbe discovered what was to become a well-known paradigm: the inability of a lens-based optical microscope to discern details that are closer together than half of the wavelength of light. However, for its most popular imaging mode, fluorescence microscopy, the diffraction barrier is crumbling. Here, I discuss the physical concepts that have pushed fluorescence microscopy to the nanoscale, once the prerogative of electron and scanning probe microscopes. Initial applications indicate that emergent far-field optical nanoscopy will have a strong impact in the life sciences and in other areas benefiting from nanoscale visualization.}
}
@article{Vicidomini2018,
	title        = {STED super-resolved microscopy},
	author       = {Vicidomini, Giuseppe and Bianchini, Paolo and Diaspro, Alberto},
	year         = 2018,
	month        = 3,
	day          = {01},
	journal      = {Nature Methods},
	volume       = 15,
	number       = 3,
	pages        = {173--182},
	doi          = {10.1038/nmeth.4593},
	issn         = {1548-7105},
	url          = {https://doi.org/10.1038/nmeth.4593},
	abstract     = {This Perspective reviews nanoscopy via stimulated emission depletion (STED), focusing on challenges for biologists and how technical advances are helping to meet these challenges.}
}
@article{Wu2018,
	title        = {Faster, sharper, and deeper: structured illumination microscopy for biological imaging},
	author       = {Wu, Yicong and Shroff, Hari},
	year         = 2018,
	month        = 12,
	day          = {01},
	journal      = {Nature Methods},
	volume       = 15,
	number       = 12,
	pages        = {1011--1019},
	doi          = {10.1038/s41592-018-0211-z},
	issn         = {1548-7105},
	url          = {https://doi.org/10.1038/s41592-018-0211-z},
	abstract     = {Structured illumination microscopy (SIM) allows rapid, super-resolution (SR) imaging in live specimens. We review recent technical advances in SR-SIM, with emphasis on imaging speed, resolution, and depth. Since its introduction decades ago, the technique has grown to offer myriad implementations, each with its own strengths and weaknesses. We discuss these, aiming to provide a practical guide for biologists and to highlight which approach is best suited to a given application.}
}
@article{Chen2015,
	title        = {Expansion microscopy},
	author       = {Fei Chen  and Paul W. Tillberg  and Edward S. Boyden},
	year         = 2015,
	journal      = {Science},
	volume       = 347,
	number       = 6221,
	pages        = {543--548},
	doi          = {10.1126/science.1260088},
	url          = {https://www.science.org/doi/abs/10.1126/science.1260088},
	eprint       = {https://www.science.org/doi/pdf/10.1126/science.1260088},
	abstract     = {The resolution of a light microscope is limited. Physicists have long since worked out what these limits are and which parameters determine the spatial resolution. Many groups have nevertheless made numerous attempts to overcome these resolution limits. Rather than improving the power and quality of the microscope, Chen et al. instead expanded the biological specimens under study (see the Perspective by Dodt). They introduced a polymer gel into fixed cells and tissues and chemically induced swelling of the polymer by almost two orders of magnitude. They could then produce much higher-resolution images of their samples, which included the mouse hippocampus. Science, this issue p. 543; see also p. 474 A new approach to image fixed biological samples by expanding the specimen under study reveals mouse brain substructures. [Also see Perspective by Dodt] In optical microscopy, fine structural details are resolved by using refraction to magnify images of a specimen. We discovered that by synthesizing a swellable polymer network within a specimen, it can be physically expanded, resulting in physical magnification. By covalently anchoring specific labels located within the specimen directly to the polymer network, labels spaced closer than the optical diffraction limit can be isotropically separated and optically resolved, a process we call expansion microscopy (ExM). Thus, this process can be used to perform scalable superresolution microscopy with diffraction-limited microscopes. We demonstrate ExM with apparent ~70-nanometer lateral resolution in both cultured cells and brain tissue, performing three-color superresolution imaging of ~107 cubic micrometers of the mouse hippocampus with a conventional confocal microscope.}
}
@rticle{Wassie2019,
	title        = {Expansion microscopy: principles and uses in biological research},
	author       = {Wassie, Asmamaw T. and Zhao, Yongxin and Boyden, Edward S.},
	year         = 2019,
	month        = 1,
	day          = {01},
	journal      = {Nature Methods},
	volume       = 16,
	number       = 1,
	pages        = {33--41},
	doi          = {10.1038/s41592-018-0219-4},
	issn         = {1548-7105},
	url          = {https://doi.org/10.1038/s41592-018-0219-4},
	abstract     = {Many biological investigations require 3D imaging of cells or tissues with nanoscale spatial resolution. We recently discovered that preserved biological specimens can be physically expanded in an isotropic fashion through a chemical process. Expansion microscopy (ExM) allows nanoscale imaging of biological specimens with conventional microscopes, decrowds biomolecules in support of signal amplification and multiplexed readout chemistries, and makes specimens transparent. We review the principles of how ExM works, advances in the technology made by our group and others, and its applications throughout biology and medicine.}
}
@article{Valli2021,
	title        = {Seeing beyond the limit: A guide to choosing the right super-resolution microscopy technique},
	author       = {Jessica Valli and Adrian Garcia-Burgos and Liam M. Rooney and Beatriz {Vale de Melo e Oliveira} and Rory R. Duncan and Colin Rickman},
	year         = 2021,
	journal      = {Journal of Biological Chemistry},
	volume       = 297,
	number       = 1,
	pages        = 100791,
	doi          = {https://doi.org/10.1016/j.jbc.2021.100791},
	issn         = {0021-9258},
	url          = {https://www.sciencedirect.com/science/article/pii/S0021925821005846},
	keywords     = {super resolution, microscopy, diffraction limit, fluorescence, localization, imaging, molecular imaging, molecular dynamics, protein–protein interactions},
	abstract     = {Super-resolution microscopy has become an increasingly popular and robust tool across the life sciences to study minute cellular structures and processes. However, with the increasing number of available super-resolution techniques has come an increased complexity and burden of choice in planning imaging experiments. Choosing the right super-resolution technique to answer a given biological question is vital for understanding and interpreting biological relevance. This is an often-neglected and complex task that should take into account well-defined criteria (e.g., sample type, structure size, imaging requirements). Trade-offs in different imaging capabilities are inevitable; thus, many researchers still find it challenging to select the most suitable technique that will best answer their biological question. This review aims to provide an overview and clarify the concepts underlying the most commonly available super-resolution techniques as well as guide researchers through all aspects that should be considered before opting for a given technique.}
}
@article{Chen2024,
	title        = {Advancements and Practical Considerations for Biophysical Research: Navigating the Challenges and Future of Super-resolution Microscopy},
	author       = {Chen, Huanhuan and Yan, Guangjie and Wen, Meng-Hsuan and Brooks, Kameron N. and Zhang, Yuteng and Huang, Pei-San and Chen, Tai-Yen},
	year         = 2024,
	month        = 5,
	day          = 27,
	journal      = {Chemical {\&} Biomedical Imaging},
	publisher    = {American Chemical Society},
	volume       = 2,
	number       = 5,
	pages        = {331--344},
	doi          = {10.1021/cbmi.4c00019},
	url          = {https://doi.org/10.1021/cbmi.4c00019}
}
@article{Hagen2021,
	title        = {Fluorescence microscopy datasets for training deep neural networks},
	author       = {Hagen, Guy M and Bendesky, Justin and Machado, Rosa and Nguyen, Tram-Anh and Kumar, Tanmay and Ventura, Jonathan},
	year         = 2021,
	month        = {05},
	journal      = {GigaScience},
	volume       = 10,
	number       = 5,
	pages        = {giab032},
	doi          = {10.1093/gigascience/giab032},
	issn         = {2047-217X},
	url          = {https://doi.org/10.1093/gigascience/giab032},
	abstract     = {Fluorescence microscopy is an important technique in many areas of biological research. Two factors that limit the usefulness and performance of fluorescence microscopy are photobleaching of fluorescent probes during imaging and, when imaging live cells, phototoxicity caused by light exposure. Recently developed methods in machine learning are able to greatly improve the signal-to-noise ratio of acquired images. This allows researchers to record images with much shorter exposure times, which in turn minimizes photobleaching and phototoxicity by reducing the dose of light reaching the sample.To use deep learning methods, a large amount of data is needed to train the underlying convolutional neural network. One way to do this involves use of pairs of fluorescence microscopy images acquired with long and short exposure times. We provide high-quality datasets that can be used to train and evaluate deep learning methods under development.The availability of high-quality data is vital for training convolutional neural networks that are used in current machine learning approaches.},
	eprint       = {https://academic.oup.com/gigascience/article-pdf/10/5/giab032/60688420/giab032.pdf}
}
@article{Weigert2018,
	title        = {Content-aware image restoration: pushing the limits of fluorescence microscopy},
	author       = {Weigert, Martin and Schmidt, Uwe and Boothe, Tobias and M{\"u}ller, Andreas and Dibrov, Alexandr and Jain, Akanksha and Wilhelm, Benjamin and Schmidt, Deborah and Broaddus, Coleman and Culley, Si{\^a}n and Rocha-Martins, Mauricio and Segovia-Miranda, Fabi{\'a}n and Norden, Caren and Henriques, Ricardo and Zerial, Marino and Solimena, Michele and Rink, Jochen and Tomancak, Pavel and Royer, Loic and Jug, Florian and Myers, Eugene W.},
	year         = 2018,
	month        = 12,
	day          = {01},
	journal      = {Nature Methods},
	volume       = 15,
	number       = 12,
	pages        = {1090--1097},
	doi          = {10.1038/s41592-018-0216-7},
	issn         = {1548-7105},
	url          = {https://doi.org/10.1038/s41592-018-0216-7},
	abstract     = {Fluorescence microscopy is a key driver of discoveries in the life sciences, with observable phenomena being limited by the optics of the microscope, the chemistry of the fluorophores, and the maximum photon exposure tolerated by the sample. These limits necessitate trade-offs between imaging speed, spatial resolution, light exposure, and imaging depth. In this work we show how content-aware image restoration based on deep learning extends the range of biological phenomena observable by microscopy. We demonstrate on eight concrete examples how microscopy images can be restored even if 60-fold fewer photons are used during acquisition, how near isotropic resolution can be achieved with up to tenfold under-sampling along the axial direction, and how tubular and granular structures smaller than the diffraction limit can be resolved at 20-times-higher frame rates compared to state-of-the-art methods. All developed image restoration methods are freely available as open source software in Python, FIJI, and KNIME.}
}
@article{Chen2021,
	title        = {Three-dimensional residual channel attention networks denoise and sharpen fluorescence microscopy image volumes},
	author       = {Chen, Jiji and Sasaki, Hideki and Lai, Hoyin and Su, Yijun and Liu, Jiamin and Wu, Yicong and Zhovmer, Alexander and Combs, Christian A. and Rey-Suarez, Ivan and Chang, Hung-Yu and Huang, Chi Chou and Li, Xuesong and Guo, Min and Nizambad, Srineil	and Upadhyaya, Arpita and Lee, Shih-Jong J. and Lucas, Luciano A. G. and Shroff, Hari},
	year         = 2021,
	month        = 6,
	day          = {01},
	journal      = {Nature Methods},
	volume       = 18,
	number       = 6,
	pages        = {678--687},
	doi          = {10.1038/s41592-021-01155-x},
	issn         = {1548-7105},
	url          = {https://doi.org/10.1038/s41592-021-01155-x},
	abstract     = {We demonstrate residual channel attention networks (RCAN) for the restoration and enhancement of volumetric time-lapse (four-dimensional) fluorescence microscopy data. First we modify RCAN to handle image volumes, showing that our network enables denoising competitive with three other state-of-the-art neural networks. We use RCAN to restore noisy four-dimensional super-resolution data, enabling image capture of over tens of thousands of images (thousands of volumes) without apparent photobleaching. Second, using simulations we show that RCAN enables resolution enhancement equivalent to, or better than, other networks. Third, we exploit RCAN for denoising and resolution improvement in confocal microscopy, enabling {\textasciitilde}2.5-fold lateral resolution enhancement using stimulated emission depletion microscopy ground truth. Fourth, we develop methods to improve spatial resolution in structured illumination microscopy using expansion microscopy data as ground truth, achieving improvements of {\textasciitilde}1.9-fold laterally and {\textasciitilde}3.6-fold axially. Finally, we characterize the limits of denoising and resolution enhancement, suggesting practical benchmarks for evaluation and further enhancement of network performance.}
}
@article{Qiao2021,
	title        = {Evaluation and development of deep neural networks for image super-resolution in optical microscopy},
	author       = {Qiao, Chang and Li, Di and Guo, Yuting and Liu, Chong and Jiang, Tao and Dai, Qionghai and Li, Dong},
	year         = 2021,
	month        = 2,
	day          = {01},
	journal      = {Nature Methods},
	volume       = 18,
	number       = 2,
	pages        = {194--202},
	doi          = {10.1038/s41592-020-01048-5},
	issn         = {1548-7105},
	url          = {https://doi.org/10.1038/s41592-020-01048-5},
	abstract     = {Deep neural networks have enabled astonishing transformations from low-resolution (LR) to super-resolved images. However, whether, and under what imaging conditions, such deep-learning models outperform super-resolution (SR) microscopy is poorly explored. Here, using multimodality structured illumination microscopy (SIM), we first provide an extensive dataset of LR--SR image pairs and evaluate the deep-learning SR models in terms of structural complexity, signal-to-noise ratio and upscaling factor. Second, we devise the deep Fourier channel attention network (DFCAN), which leverages the frequency content difference across distinct features to learn precise hierarchical representations of high-frequency information about diverse biological structures. Third, we show that DFCAN's Fourier domain focalization enables robust reconstruction of SIM images under low signal-to-noise ratio conditions. We demonstrate that DFCAN achieves comparable image quality to SIM over a tenfold longer duration in multicolor live-cell imaging experiments, which reveal the detailed structures of mitochondrial cristae and nucleoids and the interaction dynamics of organelles and cytoskeleton.}
}
@article{Hou2025,
	title        = {HD2Net: a deep learning framework for simultaneous denoising and deaberration in fluorescence microscopy},
	author       = {Xuekai Hou and Yue Li and Chad M. Hobson and Hari Shroff and Min Guo and Huafeng Liu},
	year         = 2025,
	month        = 6,
	journal      = {Opt. Express},
	publisher    = {Optica Publishing Group},
	volume       = 33,
	number       = 13,
	pages        = {27317--27333},
	doi          = {10.1364/OE.554927},
	url          = {https://opg.optica.org/oe/abstract.cfm?URI=oe-33-13-27317},
	keywords     = {Biomedical imaging; Deep learning; Fluorescence microscopy; Image enhancement; Image metrics; Optical aberration},
	abstract     = {Fluorescence microscopy is essential for biological research, offering high-contrast imaging of microscopic structures. However, the quality of these images is often compromised by optical aberrations and noise, particularly in low signal-to-noise ratio (SNR) conditions. While adaptive optics (AO) can correct aberrations, it requires costly hardware and slows down imaging, whereas current denoising approaches boost the SNR but leave out the aberration compensation. To address these limitations, we introduce HD2Net, a deep-learning framework that enhances image quality by simultaneously denoising and suppressing the effect of aberrations without the need for additional hardware. Building on our previous work, HD2Net incorporates noise estimation and aberration removal modules, effectively restoring images degraded by noise and aberrations. Through comprehensive evaluation of synthetic phantoms and biological data, we demonstrate that HD2Net outperforms existing methods, significantly improving image resolution and contrast. This framework offers a promising solution for enhancing biological imaging, particularly in challenging aberrating and low-light conditions.}
}
@article{Zhang2017,
	title        = {Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising},
	author       = {Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},
	year         = 2017,
	journal      = {IEEE Transactions on Image Processing},
	volume       = 26,
	number       = 7,
	pages        = {3142--3155},
	doi          = {10.1109/TIP.2017.2662206},
	url			 = {https://ieeexplore.ieee.org/document/7839189},
	keywords     = {Noise reduction;Image denoising;Training;Computational modeling;Noise level;Neural networks;Transform coding;Image denoising;convolutional neural networks;residual learning;batch normalization}
}
@article{Dabov2007,
	title        = {Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering},
	author       = {Dabov, Kostadin and Foi, Alessandro and Katkovnik, Vladimir and Egiazarian, Karen},
	year         = 2007,
	journal      = {IEEE Transactions on Image Processing},
	volume       = 16,
	number       = 8,
	pages        = {2080--2095},
	doi          = {10.1109/TIP.2007.901238},
	url			 = {https://ieeexplore.ieee.org/document/4271520},
	keywords     = {Image denoising;Collaboration;Filtering;Noise reduction;Signal processing algorithms;Signal processing;Energy resolution;Spatial resolution;Signal resolution;Discrete cosine transforms;Adaptive grouping;block matching;image denoising;sparsity;3-D transform shrinkage}
}
@inproceedings{Krull2019,
	title        = {Noise2Void - Learning Denoising From Single Noisy Images},
	author       = {Krull, Alexander and Buchholz, Tim-Oliver and Jug, Florian},
	year         = 2019,
	booktitle    = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {2124--2132},
	doi          = {10.1109/CVPR.2019.00223},
	url			 = {https://ieeexplore.ieee.org/document/8954066},
	keywords     = {Training;Photography;Deep learning;Noise reduction;Fluorescence;Pattern recognition;Noise measurement;Medical;Biological and Cell Microscopy;Deep Learning;Low-level Vision;Statistical Learning}
}
@inproceedings{Batson2019,
	title        = {{N}oise2{S}elf: Blind Denoising by Self-Supervision},
	author       = {Batson, Joshua and Royer, Loic},
	year         = 2019,
	month        = 6,
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 97,
	pages        = {524--533},
	url          = {https://proceedings.mlr.press/v97/batson19a.html},
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	pdf          = {http://proceedings.mlr.press/v97/batson19a/batson19a.pdf},
	abstract     = {We propose a general framework for denoising high-dimensional measurements which requires no prior on the signal, no estimate of the noise, and no clean training data. The only assumption is that the noise exhibits statistical independence across different dimensions of the measurement, while the true signal exhibits some correlation. For a broad class of functions (“$\mathcal{J}$-invariant”), it is then possible to estimate the performance of a denoiser from noisy data alone. This allows us to calibrate $\mathcal{J}$-invariant versions of any parameterised denoising algorithm, from the single hyperparameter of a median filter to the millions of weights of a deep neural network. We demonstrate this on natural image and microscopy data, where we exploit noise independence between pixels, and on single-cell gene expression data, where we exploit independence between detections of individual molecules. This framework generalizes recent work on training neural nets from noisy images and on cross-validation for matrix factorization.}
}
@inproceedings{Lehtinen2018,
	title        = {{N}oise2{N}oise: Learning Image Restoration without Clean Data},
	author       = {Lehtinen, Jaakko and Munkberg, Jacob and Hasselgren, Jon and Laine, Samuli and Karras, Tero and Aittala, Miika and Aila, Timo},
	year         = 2018,
	month        = 7,
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 80,
	pages        = {2965--2974},
	url          = {https://proceedings.mlr.press/v80/lehtinen18a.html},
	editor       = {Dy, Jennifer and Krause, Andreas},
	pdf          = {http://proceedings.mlr.press/v80/lehtinen18a/lehtinen18a.pdf},
	abstract     = {We apply basic statistical reasoning to signal reconstruction by machine learning - learning to map corrupted observations to clean signals - with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans - all corrupted by different processes - based on noisy data only.}
}
@article{Li2022,
	title        = {Incorporating the image formation process into deep learning improves network performance},
	author       = {Li, Yue and Su, Yijun and Guo, Min and Han, Xiaofei and Liu, Jiamin and Vishwasrao, Harshad D. and Li, Xuesong and Christensen, Ryan and Sengupta, Titas and Moyle, Mark W. and Rey-Suarez, Ivan and Chen, Jiji and Upadhyaya, Arpita and Usdin, Ted B. and Col{\'o}n-Ramos, Daniel Alfonso and Liu, Huafeng and Wu, Yicong and Shroff, Hari},
	year         = 2022,
	month        = 11,
	day          = {01},
	journal      = {Nature Methods},
	volume       = 19,
	number       = 11,
	pages        = {1427--1437},
	doi          = {10.1038/s41592-022-01652-7},
	issn         = {1548-7105},
	url          = {https://doi.org/10.1038/s41592-022-01652-7},
	abstract     = {We present Richardson--Lucy network (RLN), a fast and lightweight deep learning method for three-dimensional fluorescence microscopy deconvolution. RLN combines the traditional Richardson--Lucy iteration with a fully convolutional network structure, establishing a connection to the image formation process and thereby improving network performance. Containing only roughly 16,000 parameters, RLN enables four- to 50-fold faster processing than purely data-driven networks with many more parameters. By visual and quantitative analysis, we show that RLN provides better deconvolution, better generalizability and fewer artifacts than other networks, especially along the axial dimension. RLN outperforms classic Richardson--Lucy deconvolution on volumes contaminated with severe out of focus fluorescence or noise and provides four- to sixfold faster reconstructions of large, cleared-tissue datasets than classic multi-view pipelines. We demonstrate RLN's performance on cells, tissues and embryos imaged with widefield-, light-sheet-, confocal- and super-resolution microscopy.}
}
@article{Yanny2022,
	title        = {Deep learning for fast spatially varying deconvolution},
	author       = {Kyrollos Yanny and Kristina Monakhova and Richard W. Shuai and Laura Waller},
	year         = 2022,
	month        = 1,
	journal      = {Optica},
	publisher    = {Optica Publishing Group},
	volume       = 9,
	number       = 1,
	pages        = {96--99},
	doi          = {10.1364/OPTICA.442438},
	url          = {https://opg.optica.org/optica/abstract.cfm?URI=optica-9-1-96},
	keywords     = {Deep learning; Fourier transforms; Imaging systems; Inverse design; Neural networks; Three dimensional reconstruction},
	abstract     = {Deconvolution can be used to obtain sharp images or volumes from blurry or encoded measurements in imaging systems. Given knowledge of the system's point spread function (PSF) over the field of view, a reconstruction algorithm can be used to recover a clear image or volume. Most deconvolution algorithms assume shift-invariance; however, in realistic systems, the PSF varies laterally and axially across the field of view due to aberrations or design. Shift-varying models can be used, but are often slow and computationally intensive. In this work, we propose a deep-learning-based approach that leverages knowledge about the system's spatially varying PSFs for fast 2D and 3D reconstructions. Our approach, termed MultiWienerNet, uses multiple differentiable Wiener filters paired with a convolutional neural network to incorporate spatial variance. Trained using simulated data and tested on experimental data, our approach offers a 625{\textminus}1600{\texttimes} increase in speed compared to iterative methods with a spatially varying model, and outperforms existing deep-learning-based methods that assume shift invariance.}
}
@article{Saha2020,
	title        = {Practical sensorless aberration estimation for 3D microscopy with deep learning},
	author       = {Debayan Saha and Uwe Schmidt and Qinrong Zhang and Aurelien Barbotin and Qi Hu and Na Ji and Martin J. Booth and Martin Weigert and Eugene W. Myers},
	year         = 2020,
	month        = 9,
	journal      = {Opt. Express},
	publisher    = {Optica Publishing Group},
	volume       = 28,
	number       = 20,
	pages        = {29044--29053},
	doi          = {10.1364/OE.401933},
	url          = {https://opg.optica.org/oe/abstract.cfm?URI=oe-28-20-29044},
	keywords     = {Deep learning; Deformable mirrors; Neural networks; Optical aberration; Spatial light modulators; Three dimensional microscopy},
	abstract     = {Estimation of optical aberrations from volumetric intensity images is a key step in sensorless adaptive optics for 3D microscopy. Recent approaches based on deep learning promise accurate results at fast processing speeds. However, collecting ground truth microscopy data for training the network is typically very difficult or even impossible thereby limiting this approach in practice. Here, we demonstrate that neural networks trained only on simulated data yield accurate predictions for real experimental images. We validate our approach on simulated and experimental datasets acquired with two different microscopy modalities and also compare the results to non-learned methods. Additionally, we study the predictability of individual aberrations with respect to their data requirements and find that the symmetry of the wavefront plays a crucial role. Finally, we make our implementation freely available as open source software in Python.}
}
@article{Kang2024,
	title        = {Coordinate-based neural representations for computational adaptive optics in widefield microscopy},
	author       = {Kang, Iksung and Zhang, Qinrong and Yu, Stella X. and Ji, Na},
	year         = 2024,
	month        = 6,
	journal      = {Nature Machine Intelligence},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 6,
	number       = 6,
	pages        = {714--725},
	doi          = {10.1038/s42256-024-00853-3},
	issn         = {2522-5839},
	url          = {http://dx.doi.org/10.1038/s42256-024-00853-3}
}
@article{Kang2024.10.20.619284,
	title        = {Adaptive optical correction in in vivo two-photon fluorescence microscopy with neural fields},
	author       = {Kang, Iksung and Kim, Hyeonggeon and Natan, Ryan and Zhang, Qinrong and Yu, Stella X. and Ji, Na},
	year         = 2024,
	journal      = {bioRxiv},
	publisher    = {Cold Spring Harbor Laboratory},
	doi          = {10.1101/2024.10.20.619284},
	url          = {https://www.biorxiv.org/content/early/2024/10/22/2024.10.20.619284},
	elocation-id = {2024.10.20.619284},
	abstract     = {Adaptive optics (AO) techniques are designed to restore ideal imaging performance by measuring and correcting aberrations originating from both the microscope system and the sample itself. Conventional AO methods require additional hardware, such as wavefront sensors and corrective devices, for aberration measurement and correction, respectively. These methods often necessitate microscopes to adhere to strict design parameters, like perfect optical conjugation, to ensure the accurate delivery of corrective patterns for wavefront correction using corrective devices. However, in general microscope systems, including commercially available ones, conjugation errors are more prone to arise due to incomplete conjugation among optical components by design and misalignment of the components, coupled with their limited access and adjustability, which hinders the rigorous integration of AO hardware. Here, we describe a general-purpose AO framework using neural fields, NeAT, that is applicable to both custom-built and commercial two-photon fluorescence microscopes and demonstrate its performance in various in vivo imaging settings. This framework estimates wavefront aberration from a single 3D two-photon fluorescence image stack, without requiring external datasets for training. Additionally, it addresses the issue of incomplete optical conjugation by estimating and correcting any conjugation errors, which enables more accurate aberration correction by the corrective device. Finally, it jointly recovers the sample{\textquoteright}s 3D structural information during the learning process, potentially eliminating the need for hardware-based AO correction. We first carefully assess its aberration estimation performance using a custom-built two-photon fluorescence microscope equipped with a wavefront sensor which provides the ground truth aberration for comparison. We further characterize and assess the robustness of the aberration estimation to image stacks with low signal-to-noise ratios, strong aberration, and motion artifacts. As practical applications, using a commercial microscope with a spatial light modulator, we first demonstrate NeAT{\textquoteright}s real-time aberration correction performance in in vivo morphological imaging of the mouse brain. We further show its performance in in vivo functional activity imaging of glutamate and calcium dynamics within the mouse brain.Competing Interest StatementI.K. and N.J. are listed as inventors on a patent related to the technology described in this study (U.S. patent application No. 63/707.628). No other authors declare competing interests.},
	eprint       = {https://www.biorxiv.org/content/early/2024/10/22/2024.10.20.619284.full.pdf}
}
@article{Fersini2025,
	title        = {Wavefront estimation through structured detection in laser scanning microscopy},
	author       = {Francesco Fersini and Alessandro Zunino and Pietro Morerio and Francesca Baldini and Alberto Diaspro and Martin J. Booth and Alessio Del Bue and Giuseppe Vicidomini},
	year         = 2025,
	month        = 5,
	journal      = {Biomed. Opt. Express},
	publisher    = {Optica Publishing Group},
	volume       = 16,
	number       = 5,
	pages        = {2135--2155},
	doi          = {10.1364/BOE.559899},
	url          = {https://opg.optica.org/boe/abstract.cfm?URI=boe-16-5-2135},
	keywords     = {Beam shaping; Image metrics; Imaging techniques; Optical systems; Spatial light modulators; Spatial resolution},
	abstract     = {Laser scanning microscopy (LSM) is the base of numerous advanced imaging techniques, including confocal laser scanning microscopy (CLSM), a widely used tool in life sciences research. However, its effective resolution is often compromised by optical aberrations, a common challenge in all optical systems. While adaptive optics (AO) can correct these aberrations, current methods face significant limitations: aberration estimation, which is central to any AO approach, typically requires specialized hardware or prolonged sample exposure, rendering these methods sample-invasive, and less user-friendly. In this study, we propose a simple and efficient AO strategy for CLSM systems equipped with a detector array -- image-scanning microscopy -- and an AO element for beam shaping. We demonstrate, for the first time, that datasets acquired with a detector array inherently encode aberration information. As a proof-of-concept of this important property, we designed a custom convolutional neural network capable of decoding aberrations up to the 11th Zernike coefficient, directly from a single acquisition. While this data-driven approach represents an initial exploration of the aberration content, it opens the door to more advanced decoding strategies -- including model-based methods. This work establishes a new paradigm for aberration sensing in LSM and is designed to work synergistically with conventional AO approaches such as phase diversity, enabling faster, less invasive, and more accessible high-resolution imaging.}
}
@article{Zhou2023,
	title        = {Aberration Modeling in Deep Learning for Volumetric Reconstruction of Light-Field Microscopy},
	author       = {Zhou, You and Jin, Zhouyu and Zhao, Qianhui and Xiong, Bo and Cao, Xun},
	year         = 2023,
	journal      = {Laser \& Photonics Reviews},
	volume       = 17,
	number       = 10,
	pages        = 2300154,
	doi          = {https://doi.org/10.1002/lpor.202300154},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/lpor.202300154},
	keywords     = {deep learning, light-field microscopy, spherical aberration modeling, volumetric imaging},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/lpor.202300154},
	abstract     = {Abstract Optical aberration is a crucial issue in optical microscopes, fundamentally constraining the achievable imaging performance. As a commonly encountered one, spherical aberration is introduced by the refractive index mismatches between samples and their surrounding environments, leading to problems like low contrast, blurring, and distortion in imaging. Light-field microscopy (LFM) has recently emerged as a powerful tool for rapid volumetric imaging. The presence of spherical aberration in LFM will cause substantial changes of the point spread function (PSF) and thus greatly affects the imaging performance. Here, the aberration-modeling view-channel-depth (AM-VCD) network is proposed for LFM reconstruction, effectively mitigating the influence of severe spherical aberration. By quantitatively estimating the spherical aberration in advance and incorporating it into the network training, the AM-VCD achieves aberration-corrected high-speed visualization of 3D processes with uniform spatial resolution and real-time reconstruction speed. Without necessitating hardware modifications, this method provides a convenient way for directly observing the 3D dynamics of samples in solution. The capability of AM-VCD under a large refractive index mismatch is demonstrated through volumetric imaging of a large-scale fishbone of a largemouth bass. Furthermore, the capability of AM-VCD in nearly 100 Hz volumetric imaging of neutrophil migration and a beating heart in living zebrafish is investigated.}
}
@article{Qiao2024,
	title        = {Deep learning-based optical aberration estimation enables offline digital adaptive optics and super-resolution imaging},
	author       = {Chang Qiao and Haoyu Chen and Run Wang and Tao Jiang and Yuwang Wang and Dong Li},
	year         = 2024,
	month        = 3,
	journal      = {Photon. Res.},
	publisher    = {Optica Publishing Group},
	volume       = 12,
	number       = 3,
	pages        = {474--484},
	doi          = {10.1364/PRJ.506778},
	url          = {https://opg.optica.org/prj/abstract.cfm?URI=prj-12-3-474},
	keywords     = {Deformable mirrors; Imaging systems; Optical aberration; Spatial light modulators; Spatial resolution; Spherical aberration},
	abstract     = {Optical aberrations degrade the performance of fluorescence microscopy. Conventional adaptive optics (AO) leverages specific devices, such as the Shack\&\#x2013;Hartmann wavefront sensor and deformable mirror, to measure and correct optical aberrations. However, conventional AO requires either additional hardware or a more complicated imaging procedure, resulting in higher cost or a lower acquisition speed. In this study, we proposed a novel space-frequency encoding network (SFE-Net) that can directly estimate the aberrated point spread functions (PSFs) from biological images, enabling fast optical aberration estimation with high accuracy without engaging extra optics and image acquisition. We showed that with the estimated PSFs, the optical aberration can be computationally removed by the deconvolution algorithm. Furthermore, to fully exploit the benefits of SFE-Net, we incorporated the estimated PSF with neural network architecture design to devise an aberration-aware deep-learning super-resolution model, dubbed SFT-DFCAN. We demonstrated that the combination of SFE-Net and SFT-DFCAN enables instant digital AO and optical aberration-aware super-resolution reconstruction for live-cell imaging.}
}
@article{Hu2021,
	title        = {Image enhancement for fluorescence microscopy based on deep learning with prior knowledge of aberration},
	author       = {Lejia Hu and Shuwen Hu and Wei Gong and Ke Si},
	year         = 2021,
	month        = 5,
	journal      = {Opt. Lett.},
	publisher    = {Optica Publishing Group},
	volume       = 46,
	number       = 9,
	pages        = {2055--2058},
	doi          = {10.1364/OL.418997},
	url          = {https://opg.optica.org/ol/abstract.cfm?URI=ol-46-9-2055},
	keywords     = {Deep learning; Fluorescence microscopy; High speed imaging; Image enhancement; Multiphoton microscopy; Spatial light modulators},
	abstract     = {In this Letter, we propose a deep learning method with prior knowledge of potential aberration to enhance the fluorescence microscopy without additional hardware. The proposed method could effectively reduce noise and improve the peak signal-to-noise ratio of the acquired images at high speed. The enhancement performance and generalization of this method is demonstrated on three commercial fluorescence microscopes. This work provides a computational alternative to overcome the degradation induced by the biological specimen, and it has the potential to be further applied in biological applications.}
}
@article{Wang2019,
	title        = {Deep learning enables cross-modality super-resolution in fluorescence microscopy},
	author       = {Wang, Hongda and Rivenson, Yair and Jin, Yiyin and Wei, Zhensong and Gao, Ronald and G{\"u}nayd{\i}n, Harun and Bentolila, Laurent A. and Kural, Comert and Ozcan, Aydogan},
	year         = 2019,
	month        = 1,
	day          = {01},
	journal      = {Nature Methods},
	volume       = 16,
	number       = 1,
	pages        = {103--110},
	doi          = {10.1038/s41592-018-0239-0},
	issn         = {1548-7105},
	url          = {https://doi.org/10.1038/s41592-018-0239-0},
	abstract     = {We present deep-learning-enabled super-resolution across different fluorescence microscopy modalities. This data-driven approach does not require numerical modeling of the imaging process or the estimation of a point-spread-function, and is based on training a generative adversarial network (GAN) to transform diffraction-limited input images into super-resolved ones. Using this framework, we improve the resolution of wide-field images acquired with low-numerical-aperture objectives, matching the resolution that is acquired using high-numerical-aperture objectives. We also demonstrate cross-modality super-resolution, transforming confocal microscopy images to match the resolution acquired with a stimulated emission depletion (STED) microscope. We further demonstrate that total internal reflection fluorescence (TIRF) microscopy images of subcellular structures within cells and tissues can be transformed to match the results obtained with a TIRF-based structured illumination microscope. The deep network rapidly outputs these super-resolved images, without any iterations or parameter search, and could serve to democratize super-resolution imaging.}
}
@article{Park2022,
	title        = {Deep learning enables reference-free isotropic super-resolution for volumetric fluorescence microscopy},
	author       = {Park, Hyoungjun and Na, Myeongsu and Kim, Bumju and Park, Soohyun and Kim, Ki Hean and Chang, Sunghoe and Ye, Jong Chul},
	year         = 2022,
	month        = 6,
	day          = {08},
	journal      = {Nature Communications},
	volume       = 13,
	number       = 1,
	pages        = 3297,
	doi          = {10.1038/s41467-022-30949-6},
	issn         = {2041-1723},
	url          = {https://doi.org/10.1038/s41467-022-30949-6},
	abstract     = {Volumetric imaging by fluorescence microscopy is often limited by anisotropic spatial resolution, in which the axial resolution is inferior to the lateral resolution. To address this problem, we present a deep-learning-enabled unsupervised super-resolution technique that enhances anisotropic images in volumetric fluorescence microscopy. In contrast to the existing deep learning approaches that require matched high-resolution target images, our method greatly reduces the effort to be put into practice as the training of a network requires only a single 3D image stack, without a priori knowledge of the image formation process, registration of training data, or separate acquisition of target data. This is achieved based on the optimal transport-driven cycle-consistent generative adversarial network that learns from an unpaired matching between high-resolution 2D images in the lateral image plane and low-resolution 2D images in other planes. Using fluorescence confocal microscopy and light-sheet microscopy, we demonstrate that the trained network not only enhances axial resolution but also restores suppressed visual details between the imaging planes and removes imaging artifacts.}
}
@article{Ning2023,
	title        = {Deep self-learning enables fast, high-fidelity isotropic resolution restoration for volumetric fluorescence microscopy},
	author       = {Ning, Kefu and Lu, Bolin and Wang, Xiaojun and Zhang, Xiaoyu and Nie, Shuo and Jiang, Tao and Li, Anan and Fan, Guoqing and Wang, Xiaofeng and Luo, Qingming and Gong, Hui and Yuan, Jing},
	year         = 2023,
	month        = 8,
	day          = 28,
	journal      = {Light: Science {\&} Applications},
	volume       = 12,
	number       = 1,
	pages        = 204,
	doi          = {10.1038/s41377-023-01230-2},
	issn         = {2047-7538},
	url          = {https://doi.org/10.1038/s41377-023-01230-2},
	abstract     = {One intrinsic yet critical issue that troubles the field of fluorescence microscopy ever since its introduction is the unmatched resolution in the lateral and axial directions (i.e., resolution anisotropy), which severely deteriorates the quality, reconstruction, and analysis of 3D volume images. By leveraging the natural anisotropy, we present a deep self-learning method termed Self-Net that significantly improves the resolution of axial images by using the lateral images from the same raw dataset as rational targets. By incorporating unsupervised learning for realistic anisotropic degradation and supervised learning for high-fidelity isotropic recovery, our method can effectively suppress the hallucination with substantially enhanced image quality compared to previously reported methods. In the experiments, we show that Self-Net can reconstruct high-fidelity isotropic 3D images from organelle to tissue levels via raw images from various microscopy platforms, e.g., wide-field, laser-scanning, or super-resolution microscopy. For the first time, Self-Net enables isotropic whole-brain imaging at a voxel resolution of 0.2{\thinspace}{\texttimes}{\thinspace}0.2{\thinspace}{\texttimes}{\thinspace}0.2{\thinspace}$\mu$m3, which addresses the last-mile problem of data quality in single-neuron morphology visualization and reconstruction with minimal effort and cost. Overall, Self-Net is a promising approach to overcoming the inherent resolution anisotropy for all classes of 3D fluorescence microscopy.}
}
@inproceedings{Ronneberger2015,
	title        = "U-Net: Convolutional Networks for Biomedical Image Segmentation",
	shorttitle   = {U-{Net}},
	author       = "Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas",
	year         = 2015,
	booktitle    = "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
	publisher    = "Springer International Publishing",
	address      = "Cham",
	series       = {Lecture {Notes} in {Computer} {Science}},
	pages        = "234--241",
	doi          = {10.1007/978-3-319-24574-4_28},
	isbn         = "978-3-319-24574-4",
	editor       = "Navab, Nassir
	and Hornegger, Joachim
	and Wells, William M.
	and Frangi, Alejandro F.",
	abstract     = "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
	language     = {en},
	keywords     = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
	url			 = {https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28}
}
@inproceedings{Ji2021,
	title        = "Early-stopped neural networks are consistent",
	author       = "Ziwei Ji and Li, Justin D. and Matus Telgarsky",
	year         = 2021,
	booktitle    = "Advances in Neural Information Processing Systems 34 - 35th Conference on Neural Information Processing Systems, NeurIPS 2021",
	publisher    = "Neural information processing systems foundation",
	series       = "Advances in Neural Information Processing Systems",
	pages        = "1805--1817",
	note         = "Publisher Copyright: {\textcopyright} 2021 Neural information processing systems foundation. All rights reserved.; 35th Conference on Neural Information Processing Systems, NeurIPS 2021 ; Conference date: 06-12-2021 Through 14-12-2021",
	abstract     = "This work studies the behavior of shallow ReLU networks trained with the logistic loss via gradient descent on binary classification data where the underlying data distribution is general, and the (optimal) Bayes risk is not necessarily zero. In this setting, it is shown that gradient descent with early stopping achieves population risk arbitrarily close to optimal in terms of not just logistic and misclassification losses, but also in terms of calibration, meaning the sigmoid mapping of its outputs approximates the true underlying conditional distribution arbitrarily finely. Moreover, the necessary iteration, sample, and architectural complexities of this analysis all scale naturally with a certain complexity measure of the true conditional model. Lastly, while it is not shown that early stopping is necessary, it is shown that any univariate classifier satisfying a local interpolation property is inconsistent.",
	language     = "English (US)",
	editor       = "Marc'Aurelio Ranzato and Alina Beygelzimer and Yann Dauphin and Liang, \{Percy S.\} and \{Wortman Vaughan\}, Jenn",
	url			 = {https://dl.acm.org/doi/10.5555/3540261.3540400}
}
@article{Santos2022,
	title        = {Avoiding Overfitting: A Survey on Regularization Methods for Convolutional Neural Networks},
	author       = {Santos, Claudio Filipi Gon\c{c}alves Dos and Papa, Jo\~{a}o Paulo},
	year         = 2022,
	month        = 9,
	journal      = {ACM Comput. Surv.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 54,
	number       = {10s},
	doi          = {10.1145/3510413},
	issn         = {0360-0300},
	url          = {https://doi.org/10.1145/3510413},
	issue_date   = {January 2022},
	abstract     = {Several image processing tasks, such as image classification and object detection, have been significantly improved using Convolutional Neural Networks (CNN). Like ResNet and EfficientNet, many architectures have achieved outstanding results in at least one dataset by the time of their creation. A critical factor in training concerns the network’s regularization, which prevents the structure from overfitting. This work analyzes several regularization methods developed in the past few years, showing significant improvements for different CNN models. The works are classified into three main areas: the first one is called “data augmentation,” where all the techniques focus on performing changes in the input data. The second, named “internal changes,” aims to describe procedures to modify the feature maps generated by the neural network or the kernels. The last one, called “label,” concerns transforming the labels of a given input. This work presents two main differences comparing to other available surveys about regularization: (i) the first concerns the papers gathered in the manuscript, which are not older than five years, and (ii) the second distinction is about reproducibility, i.e., all works referred here have their code available in public repositories or they have been directly implemented in some framework, such as TensorFlow or Torch.},
	articleno    = 213,
	numpages     = 25,
	keywords     = {Regularization, convolutional neural networks}
}
@article{Miseta2024,
	title        = {Surpassing early stopping: A novel correlation-based stopping criterion for neural networks},
	author       = {Tamás Miseta and Attila Fodor and Ágnes Vathy-Fogarassy},
	year         = 2024,
	journal      = {Neurocomputing},
	volume       = 567,
	pages        = 127028,
	doi          = {https://doi.org/10.1016/j.neucom.2023.127028},
	issn         = {0925-2312},
	url          = {https://www.sciencedirect.com/science/article/pii/S0925231223011517},
	keywords     = {Neural network, Stopping criterion, Early stopping, Correlation-Driven Stopping Criterion, Regularization, Pearson correlation},
	abstract     = {During the training of neural networks, selecting the right stopping criterion is crucial to prevent overfitting and conserve computing power. While the early stopping and the maximum number of epochs stopping methods are simple to implement, they have limitations in identifying the point during training where the training and the validation loss start to diverge. To overcome these limitations, we propose a general correlation-based stopping criterion called the Correlation-Driven Stopping Criterion (CDSC). The CDSC stops the training process when the rolling Pearson correlation of the loss metrics between the training and validation datasets decreases below a pre-defined threshold. To show the effectiveness of the newly proposed Correlation-Driven Stopping Criterion, its effectiveness was compared with the effectiveness of the early stopping and the maximum number of epochs stopping methods across multiple common machine learning problems and neural network models. Our study shows that the proposed Correlation-Driven Stopping Criterion can enhance the out-of-sample performance of all tested neural network models while conserving computing power.}
}
@article{Shah2024,
	title        = {Image restoration in frequency space using complex-valued CNNs},
	author       = {Shah, Zafran Hussain  and Müller, Marcel  and Hübner, Wolfgang  and Ortkrass, Henning  and Hammer, Barbara  and Huser, Thomas  and Schenck, Wolfram},
	year         = 2024,
	journal      = {Frontiers in Artificial Intelligence},
	volume       = {Volume 7 - 2024},
	doi          = {10.3389/frai.2024.1353873},
	issn         = {2624-8212},
	url          = {https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1353873},
	abstract     = {Real-valued convolutional neural networks (RV-CNNs) in the spatial domain have outperformed classical approaches in many image restoration tasks such as image denoising and super-resolution. Fourier analysis of the results produced by these spatial domain models reveals the limitations of these models in properly processing the full frequency spectrum. This lack of complete spectral information can result in missing textural and structural elements. To address this limitation, we explore the potential of complex-valued convolutional neural networks (CV-CNNs) for image restoration tasks. CV-CNNs have shown remarkable performance in tasks such as image classification and segmentation. However, CV-CNNs for image restoration problems in the frequency domain have not been fully investigated to address the aforementioned issues. Here, we propose several novel CV-CNN-based models equipped with complex-valued attention gates for image denoising and super-resolution in the frequency domains. We also show that our CV-CNN-based models outperform their real-valued counterparts for denoising super-resolution structured illumination microscopy (SR-SIM) and conventional image datasets. Furthermore, the experimental results show that our proposed CV-CNN-based models preserve the frequency spectrum better than their real-valued counterparts in the denoising task. Based on these findings, we conclude that CV-CNN-based methods provide a plausible and beneficial deep learning approach for image restoration in the frequency domain.}
}
@article{Liu2024,
	title        = {A Saturation Artifacts Inpainting Method Based on Two-Stage GAN for Fluorescence Microscope Images},
	author       = {Liu, Jihong and Gao, Fei and Zhang, Lvheng and Yang, Haixu},
	year         = 2024,
	journal      = {Micromachines},
	volume       = 15,
	number       = 7,
	doi          = {10.3390/mi15070928},
	issn         = {2072-666X},
	url          = {https://www.mdpi.com/2072-666X/15/7/928},
	article-number = 928,
	pubmedid     = 39064439,
	abstract     = {Fluorescence microscopic images of cells contain a large number of morphological features that are used as an unbiased source of quantitative information about cell status, through which researchers can extract quantitative information about cells and study the biological phenomena of cells through statistical and analytical analysis. As an important research object of phenotypic analysis, images have a great influence on the research results. Saturation artifacts present in the image result in a loss of grayscale information that does not reveal the true value of fluorescence intensity. From the perspective of data post-processing, we propose a two-stage cell image recovery model based on a generative adversarial network to solve the problem of phenotypic feature loss caused by saturation artifacts. The model is capable of restoring large areas of missing phenotypic features. In the experiment, we adopt the strategy of progressive restoration to improve the robustness of the training effect and add the contextual attention structure to enhance the stability of the restoration effect. We hope to use deep learning methods to mitigate the effects of saturation artifacts to reveal how chemical, genetic, and environmental factors affect cell state, providing an effective tool for studying the field of biological variability and improving image quality in analysis.}
}
@article{Bouchard2023,
	title        = {Resolution enhancement with a task-assisted GAN to guide optical nanoscopy image analysis and acquisition},
	author       = {Bouchard, Catherine and Wiesner, Theresa and Desch{\^e}nes, Andr{\'e}anne and Bilodeau, Anthony and Turcotte, Beno{\^i}t and Gagn{\'e}, Christian and Lavoie-Cardinal, Flavie},
	year         = 2023,
	month        = 8,
	day          = {01},
	journal      = {Nature Machine Intelligence},
	volume       = 5,
	number       = 8,
	pages        = {830--844},
	doi          = {10.1038/s42256-023-00689-3},
	issn         = {2522-5839},
	url          = {https://doi.org/10.1038/s42256-023-00689-3},
	abstract     = {Super-resolution fluorescence microscopy methods enable the characterization of nanostructures in living and fixed biological tissues. However, they require the adjustment of multiple imaging parameters while attempting to satisfy conflicting objectives, such as maximizing spatial and temporal resolution while minimizing light exposure. To overcome the limitations imposed by these trade-offs, post-acquisition algorithmic approaches have been proposed for resolution enhancement and image-quality improvement. Here we introduce the task-assisted generative adversarial network (TA-GAN), which incorporates an auxiliary task (for example, segmentation, localization) closely related to the observed biological nanostructure characterization. We evaluate how the TA-GAN improves generative accuracy over unassisted methods, using images acquired with different modalities such as confocal, bright-field, stimulated emission depletion and structured illumination microscopy. The TA-GAN is incorporated directly into the acquisition pipeline of the microscope to predict the nanometric content of the field of view without requiring the acquisition of a super-resolved image. This information is used to automatically select the imaging modality and regions of interest, optimizing the acquisition sequence by reducing light exposure. Data-driven microscopy methods like the TA-GAN will enable the observation of dynamic molecular processes with spatial and temporal resolutions that surpass the limits currently imposed by the trade-offs constraining super-resolution microscopy.}
}
@article{Qiao2023,
	title        = {Rationalized deep learning super-resolution microscopy for sustained live imaging of rapid subcellular processes},
	author       = {Qiao, Chang and Li, Di and Liu, Yong and Zhang, Siwei and Liu, Kan and Liu, Chong and Guo, Yuting and Jiang, Tao and Fang, Chuyu and Li, Nan and Zeng, Yunmin and He, Kangmin and Zhu, Xueliang and Lippincott-Schwartz, Jennifer and Dai, Qionghai and Li, Dong},
	year         = 2023,
	month        = 3,
	day          = {01},
	journal      = {Nature Biotechnology},
	volume       = 41,
	number       = 3,
	pages        = {367--377},
	doi          = {10.1038/s41587-022-01471-3},
	issn         = {1546-1696},
	url          = {https://doi.org/10.1038/s41587-022-01471-3},
	abstract     = {The goal when imaging bioprocesses with optical microscopy is to acquire the most spatiotemporal information with the least invasiveness. Deep neural networks have substantially improved optical microscopy, including image super-resolution and restoration, but still have substantial potential for artifacts. In this study, we developed rationalized deep learning (rDL) for structured illumination microscopy and lattice light sheet microscopy (LLSM) by incorporating prior knowledge of illumination patterns and, thereby, rationally guiding the network to denoise raw images. Here we demonstrate that rDL structured illumination microscopy eliminates spectral bias-induced resolution degradation and reduces model uncertainty by five-fold, improving the super-resolution information by more than ten-fold over other computational approaches. Moreover, rDL applied to LLSM enables self-supervised training by using the spatial or temporal continuity of noisy data itself, yielding results similar to those of supervised methods. We demonstrate the utility of rDL by imaging the rapid kinetics of motile cilia, nucleolar protein condensation during light-sensitive mitosis and long-term interactions between membranous and membrane-less organelles.}
}
@inproceedings{Zhong2021,
	title        = {Blind Denoising of Fluorescence Microscopy Images Using GAN-Based Global Noise Modeling},
	author       = {Liqun Zhong and Guole Liu and Ge Yang},
	year         = 2021,
	booktitle    = {ISBI},
	pages        = {863--867},
	url          = {https://doi.org/10.1109/ISBI48211.2021.9434150},
	cdate        = 1609459200000
}
@article{Park2024,
	title        = {Unsupervised inter-domain transformation for virtually stained high-resolution mid-infrared photoacoustic microscopy using explainable deep learning},
	author       = {Park, Eunwoo and Misra, Sampa and Hwang, Dong Gyu and Yoon, Chiho and Ahn, Joongho and Kim, Donggyu and Jang, Jinah and Kim, Chulhong},
	year         = 2024,
	month        = 12,
	day          = 30,
	journal      = {Nature Communications},
	volume       = 15,
	number       = 1,
	pages        = 10892,
	doi          = {10.1038/s41467-024-55262-2},
	issn         = {2041-1723},
	url          = {https://doi.org/10.1038/s41467-024-55262-2},
	abstract     = {Mid-infrared photoacoustic microscopy can capture biochemical information without staining. However, the long mid-infrared optical wavelengths make the spatial resolution of photoacoustic microscopy significantly poorer than that of conventional confocal fluorescence microscopy. Here, we demonstrate an explainable deep learning-based unsupervised inter-domain transformation of low-resolution unlabeled mid-infrared photoacoustic microscopy images into confocal-like virtually fluorescence-stained high-resolution images. The explainable deep learning-based framework is proposed for this transformation, wherein an unsupervised generative adversarial network is primarily employed and then a saliency constraint is added for better explainability. We validate the performance of explainable deep learning-based mid-infrared photoacoustic microscopy by identifying cell nuclei and filamentous actins in cultured human cardiac fibroblasts and matching them with the corresponding CFM images. The XDL ensures similar saliency between the two domains, making the transformation process more stable and more reliable than existing networks. Our XDL-MIR-PAM enables label-free high-resolution duplexed cellular imaging, which can significantly benefit many research avenues in cell biology.}
}
@inproceedings{Osuna-Vargas2025,
	title        = {Denoising Diffusion Models for High-Resolution Microscopy Image Restoration},
	author       = {Osuna-Vargas, Pamela and Wehrheim, Maren H. and Zinz, Lucas and Rahm, Johanna and Balakrishnan, Ashwin and Kaminer, Alexandra and Heilemann, Mike and Kaschube, Matthias},
	year         = 2025,
	booktitle    = {2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
	pages        = {4320--4330},
	doi          = {10.1109/WACV61041.2025.00424},
	url			 = {https://ieeexplore.ieee.org/document/10943369},
	keywords     = {Training;Image resolution;Microscopy;Noise reduction;Stochastic processes;Diffusion models;Probabilistic logic;Reliability;Signal resolution;Signal to noise ratio;image denoising;image restoration;fluorescence microscopy;generative models;denoising diffusion probabilistic models}
}
@misc{Zhang2018,
	title        = {Image Super-Resolution Using Very Deep Residual Channel Attention Networks},
	author       = {Yulun Zhang and Kunpeng Li and Kai Li and Lichen Wang and Bineng Zhong and Yun Fu},
	year         = 2018,
	url          = {https://arxiv.org/abs/1807.02758},
	eprint       = {1807.02758},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@article{Wang2004,
	title        = {Image quality assessment: from error visibility to structural similarity},
	author       = {Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
	year         = 2004,
	journal      = {IEEE Transactions on Image Processing},
	volume       = 13,
	number       = 4,
	pages        = {600--612},
	doi          = {10.1109/TIP.2003.819861},
	url			 = {https://ieeexplore.ieee.org/document/1284395},
	keywords     = {Image quality;Humans;Transform coding;Visual system;Visual perception;Data mining;Layout;Quality assessment;Degradation;Indexes}
}
@book{Gonzalez2008,
	title        = {Digital Image Processing},
	author       = {Gonzalez, R.C. and Woods, R.E.},
	year         = 2008,
	publisher    = {Prentice Hall},
	isbn         = 9780131687288,
	url          = {https://books.google.com/books?id=8uGOnjRGEzoC},
	lccn         = 2009289249
}
@article{Haase2024,
	title        = "A call for {FAIR} and open-access training materials to advance
              {BioImage} Analysis",
	author       = "Haase, Robert and Tischer, Christian and Bankhead, Peter and
              Miura, Kota and Cimini, Beth",
	year         = 2024,
	month        = 3,
	url			 = {https://osf.io/preprints/osf/2zgmc_v1},
	abstract     = "Interdisciplinary communities, such as the life-sciences, have a
              strong need for efficient knowledge-transfer. In our community,
              computer scientists, bioimage analysts and biologists frequently
              come together to train each other in quantitative microscopy
              bioimage data analysis. For these trainings, re-usable
              high-quality training materials can be key. We advocate for
              publishing training materials according to the FAIR principles:
              Materials must be findable, openly accessible, stored in
              interoperable file formats, and most importantly made reusable by
              attaching open-access licenses. We are convinced that the path
              towards FAIR training materials leads us to more advanced and
              higher quality training, facilitating the advance of BioImage
              Analysis as a whole."
}
@article{Ljosa2012,
	title        = "Annotated high-throughput microscopy image sets for validation",
	author       = "Ljosa, Vebjorn and Sokolnicki, Katherine L and Carpenter, Anne E",
	year         = 2012,
	month        = 6,
	journal      = "Nat. Methods",
	volume       = 9,
	number       = 7,
	pages        = 637,
	language     = "en",
	url			 = {https://doi.org/10.1038/nmeth.2083}
}
@misc{Cimini2019,
	title        = "When to say 'good enough'",
	author       = "Cimini, Beth A",
	year         = 2019,
	month        = 10,
	publisher    = "Zenodo",
	abstract     = "Blog post from the ``Measure Everything... Ask Questions Later''
               blog of the Broad Institute Imaging Platform",
	url			 = {https://carpenter-singh-lab.broadinstitute.org/blog/when-to-say-good-enough}
}
@article{Jamali2021,
	title        = "2020 {BioImage} Analysis Survey: Community experiences and needs
               for the future",
	author       = "Jamali, Nasim and Dobson, Ellen T A and Eliceiri, Kevin W and
               Carpenter, Anne E and Cimini, Beth A",
	year         = 2021,
	month        = 1,
	journal      = "Biological Imaging",
	publisher    = "Cambridge University Press",
	volume       = 1,
	pages        = "e4",
	abstract     = "In this paper, we summarize a global survey of 484 participants
               of the imaging community, conducted in 2020 through the
               NIH-funded Center for Open Bioimage Analysis (COBA). This
               23-question survey covered experience with image analysis,
               scientific background and demographics, and views and requests
               from different members of the imaging community. Through
               open-ended questions, we asked the community to provide feedback
               for the open-source tool developers and tool user groups. The
               community’s requests for tool developers include general
               improvement of tool documentation and easy-to-follow tutorials.
               Respondents encourage tool users to follow the best practice
               guidelines for imaging and ask their image analysis questions on
               the Scientific Community Image Forum (forum.image.sc). We
               analyzed the community’s preferred method of learning based on
               level of computational proficiency and work description. In
               general, written step-by-step and video tutorials are preferred
               methods of learning by the community, followed by interactive
               webinars and office hours with an expert. There is also
               enthusiasm for a centralized online location for existing
               educational resources. The survey results will help the
               community, especially developers, trainers, and organizations
               like COBA, decide how to structure and prioritize their efforts.",
	keywords     = "Bioimage analysis; community; open-source software; survey;
               training",
	doi			 = {10.1017/S2633903X21000039},
	url			 = {https://www.cambridge.org/core/journals/biological-imaging/article/2020-bioimage-analysis-survey-community-experiences-and-needs-for-the-future/9E824DC0C27568FE5B9D12FB59B1BB90}
}
@article{Sivagurunathan2023,
	title        = "Bridging imaging users to imaging analysis - A community survey",
	author       = "Sivagurunathan, Suganya and Marcotti, Stefania and Nelson, Carl J
              and Jones, Martin L and Barry, David J and Slater, Thomas J A and
              Eliceiri, Kevin W and Cimini, Beth A",
	year         = 2023,
	month        = 9,
	journal      = "J. Microsc.",
	abstract     = "The 'Bridging Imaging Users to Imaging Analysis' survey was
              conducted in 2022 by the Center for Open Bioimage Analysis (COBA),
              BioImaging North America (BINA) and the Royal Microscopical
              Society Data Analysis in Imaging Section (RMS DAIM) to understand
              the needs of the imaging community. Through multichoice and
              open-ended questions, the survey inquired about demographics,
              image analysis experiences, future needs and suggestions on the
              role of tool developers and users. Participants of the survey were
              from diverse roles and domains of the life and physical sciences.
              To our knowledge, this is the first attempt to survey
              cross-community to bridge knowledge gaps between physical and life
              sciences imaging. Survey results indicate that respondents'
              overarching needs are documentation, detailed tutorials on the
              usage of image analysis tools, user-friendly intuitive software,
              and better solutions for segmentation, ideally in a format
              tailored to their specific use cases. The tool creators suggested
              the users familiarise themselves with the fundamentals of image
              analysis, provide constant feedback and report the issues faced
              during image analysis while the users would like more
              documentation and an emphasis on tool friendliness. Regardless of
              the computational experience, there is a strong preference for
              'written tutorials' to acquire knowledge on image analysis. We
              also observed that the interest in having 'office hours' to get an
              expert opinion on their image analysis methods has increased over
              the years. The results also showed less-than-expected usage of
              online discussion forums in the imaging community for solving
              image analysis problems. Surprisingly, we also observed a
              decreased interest among the survey respondents in deep/machine
              learning despite the increasing adoption of artificial
              intelligence in biology. In addition, the community suggests the
              need for a common repository for the available image analysis
              tools and their applications. The opinions and suggestions of the
              community, released here in full, will help the image analysis
              tool creation and education communities to design and deliver the
              resources accordingly.",
	keywords     = "deep learning; image analysis; life science; physical science;
              survey",
	language     = "en",
	url			 = {https://doi.org/10.1111/jmi.13229}
}
@article{Schindelin2015,
	title        = "The {ImageJ} ecosystem: An open platform for biomedical image
               analysis",
	author       = "Schindelin, Johannes and Rueden, Curtis T and Hiner, Mark C and
               Eliceiri, Kevin W",
	year         = 2015,
	month        = 7,
	journal      = "Mol. Reprod. Dev.",
	publisher    = "Wiley",
	volume       = 82,
	number       = "7-8",
	pages        = "518--529",
	abstract     = "Technology in microscopy advances rapidly, enabling increasingly
               affordable, faster, and more precise quantitative biomedical
               imaging, which necessitates correspondingly more-advanced image
               processing and analysis techniques. A wide range of software is
               available-from commercial to academic, special-purpose to Swiss
               army knife, small to large-but a key characteristic of software
               that is suitable for scientific inquiry is its accessibility.
               Open-source software is ideal for scientific endeavors because it
               can be freely inspected, modified, and redistributed; in
               particular, the open-software platform ImageJ has had a huge
               impact on the life sciences, and continues to do so. From its
               inception, ImageJ has grown significantly due largely to being
               freely available and its vibrant and helpful user community.
               Scientists as diverse as interested hobbyists, technical
               assistants, students, scientific staff, and advanced biology
               researchers use ImageJ on a daily basis, and exchange knowledge
               via its dedicated mailing list. Uses of ImageJ range from data
               visualization and teaching to advanced image processing and
               statistical analysis. The software's extensibility continues to
               attract biologists at all career stages as well as computer
               scientists who wish to effectively implement specific
               image-processing algorithms. In this review, we use the ImageJ
               project as a case study of how open-source software fosters its
               suites of software tools, making multitudes of image-analysis
               technology easily accessible to the scientific community. We
               specifically explore what makes ImageJ so popular, how it impacts
               the life sciences, how it inspires other projects, and how it is
               self-influenced by coevolving projects within the ImageJ
               ecosystem.",
	language     = "en",
	url			 = {https://doi.org/10.1002/mrd.22489}
}
@misc{napari,
	title        = "napari: a multi-dimensional image viewer for Python",
	doi          = {10.5281/zenodo.3555620},
	url			 = {https://zenodo.org/records/16883660}
}
@article{Stirling2021,
	title        = "{CellProfiler} 4: improvements in speed, utility and usability",
	author       = "Stirling, David R and Swain-Bowden, Madison J and Lucas, Alice M
              and Carpenter, Anne E and Cimini, Beth A and Goodman, Allen",
	year         = 2021,
	month        = 9,
	journal      = "BMC Bioinformatics",
	volume       = 22,
	number       = 1,
	pages        = 433,
	abstract     = "BACKGROUND: Imaging data contains a substantial amount of
              information which can be difficult to evaluate by eye. With the
              expansion of high throughput microscopy methodologies producing
              increasingly large datasets, automated and objective analysis of
              the resulting images is essential to effectively extract
              biological information from this data. CellProfiler is a free,
              open source image analysis program which enables researchers to
              generate modular pipelines with which to process microscopy images
              into interpretable measurements. RESULTS: Herein we describe
              CellProfiler 4, a new version of this software with expanded
              functionality. Based on user feedback, we have made several user
              interface refinements to improve the usability of the software. We
              introduced new modules to expand the capabilities of the software.
              We also evaluated performance and made targeted optimizations to
              reduce the time and cost associated with running common
              large-scale analysis pipelines. CONCLUSIONS: CellProfiler 4
              provides significantly improved performance in complex workflows
              compared to previous versions. This release will ensure that
              researchers will have continued access to CellProfiler's powerful
              computational tools in the coming years.",
	keywords     = "Bioimaging; Image analysis; Image quantitation; Image
              segmentation; Microscopy",
	language     = "en",
	url			 = {https://doi.org/10.1186/s12859-021-04344-9}
}
@article{Bankhead2017,
	title        = "{QuPath}: Open source software for digital pathology image
              analysis",
	shorttitle   = {{QuPath}},
	author       = "Bankhead, Peter and Loughrey, Maurice B and Fernández, José A and
              Dombrowski, Yvonne and McArt, Darragh G and Dunne, Philip D and
              McQuaid, Stephen and Gray, Ronan T and Murray, Liam J and Coleman,
              Helen G and James, Jacqueline A and Salto-Tellez, Manuel and
              Hamilton, Peter W",
	year         = 2017,
	month        = 12,
	journal      = "Sci. Rep.",
	volume       = 7,
	number       = 1,
	pages        = 16878,
	doi          = {10.1038/s41598-017-17204-5},
	issn         = {2045-2322},
	url          = {https://www.nature.com/articles/s41598-017-17204-5},
	urldate      = {2023-10-11},
	copyright    = {2017 The Author(s)},
	note         = {Number: 1 Publisher: Nature Publishing Group},
	abstract     = "QuPath is new bioimage analysis software designed to meet the
              growing need for a user-friendly, extensible, open-source solution
              for digital pathology and whole slide image analysis. In addition
              to offering a comprehensive panel of tumor identification and
              high-throughput biomarker evaluation tools, QuPath provides
              researchers with powerful batch-processing and scripting
              functionality, and an extensible platform with which to develop
              and share new algorithms to analyze complex tissue images.
              Furthermore, QuPath's flexible design makes it suitable for a wide
              range of additional image analysis applications across biomedical
              research.",
	language     = "en",
	keywords     = {Biomarkers, Cancer imaging, Colon cancer, Image processing, Software}
}
@article{de-Chaumont2012,
	title        = "Icy: an open bioimage informatics platform for extended
               reproducible research",
	author       = "de Chaumont, Fabrice and Dallongeville, Stéphane and Chenouard,
               Nicolas and Hervé, Nicolas and Pop, Sorin and Provoost, Thomas
               and Meas-Yedid, Vannary and Pankajakshan, Praveen and Lecomte,
               Timothée and Le Montagner, Yoann and Lagache, Thibault and
               Dufour, Alexandre and Olivo-Marin, Jean-Christophe",
	year         = 2012,
	month        = 6,
	journal      = "Nat. Methods",
	publisher    = "Springer Science and Business Media LLC",
	volume       = 9,
	number       = 7,
	pages        = "690--696",
	abstract     = "Current research in biology uses evermore complex computational
               and imaging tools. Here we describe Icy, a collaborative bioimage
               informatics platform that combines a community website for
               contributing and sharing tools and material, and software with a
               high-end visual programming framework for seamless development of
               sophisticated imaging workflows. Icy extends the reproducible
               research principles, by encouraging and facilitating the
               reusability, modularity, standardization and management of
               algorithms and protocols. Icy is free, open-source and available
               at http://icy.bioimageanalysis.org/.",
	language     = "en",
	url			 = {https://doi.org/10.1038/nmeth.2075}
}
@article{Berg2019,
	title        = "Ilastik: Interactive machine learning for (bio)image analysis",
	shorttitle   = {ilastik},
	author       = "Berg, Stuart and Kutra, Dominik and Kroeger, Thorben and
               Straehle, Christoph N and Kausler, Bernhard X and Haubold,
               Carsten and Schiegg, Martin and Ales, Janez and Beier, Thorsten
               and Rudy, Markus and Eren, Kemal and Cervantes, Jaime I and Xu,
               Buote and Beuttenmueller, Fynn and Wolny, Adrian and Zhang, Chong
               and Koethe, Ullrich and Hamprecht, Fred A and Kreshuk, Anna",
	year         = 2019,
	month        = 12,
	journal      = "Nat. Methods",
	publisher    = "Springer Science and Business Media LLC",
	volume       = 16,
	number       = 12,
	pages        = "1226--1232",
	doi          = {10.1038/s41592-019-0582-9},
	issn         = {1548-7105},
	url          = {https://www.nature.com/articles/s41592-019-0582-9},
	urldate      = {2023-08-22},
	copyright    = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	note         = {Number: 12 Publisher: Nature Publishing Group},
	abstract     = "We present ilastik, an easy-to-use interactive tool that brings
               machine-learning-based (bio)image analysis to end users without
               substantial computational expertise. It contains pre-defined
               workflows for image segmentation, object classification, counting
               and tracking. Users adapt the workflows to the problem at hand by
               interactively providing sparse training annotations for a
               nonlinear classifier. ilastik can process data in up to five
               dimensions (3D, time and number of channels). Its computational
               back end runs operations on-demand wherever possible, allowing
               for interactive prediction on data larger than RAM. Once the
               classifiers are trained, ilastik workflows can be applied to new
               data from the command line without further user interaction. We
               describe all ilastik workflows in detail, including three case
               studies and a discussion on the expected performance.",
	language     = "en",
	keywords     = {Image processing, Machine learning, Software}
}
@article{Arzt2022,
	title        = "{LABKIT}: Labeling and segmentation toolkit for big image data",
	shorttitle   = {{LABKIT}},
	author       = "Arzt, Matthias and Deschamps, Joran and Schmied, Christopher and
               Pietzsch, Tobias and Schmidt, Deborah and Tomancak, Pavel and
               Haase, Robert and Jug, Florian",
	year         = 2022,
	month        = 2,
	journal      = "Front. Comput. Sci.",
	publisher    = "Frontiers Media SA",
	volume       = 4,
	issn         = {2624-9898},
	url          = {https://www.frontiersin.org/articles/10.3389/fcomp.2022.777728},
	urldate      = {2022-08-30},
	abstract     = "We present LABKIT, a user-friendly Fiji plugin for the
               segmentation of microscopy image data. It offers easy to use
               manual and automated image segmentation routines that can be
               rapidly applied to single- and multi-channel images as well as to
               timelapse movies in 2D or 3D. LABKIT is specifically designed to
               work efficiently on big image data and enables users of consumer
               laptops to conveniently work with multiple-terabyte images. This
               efficiency is achieved by using ImgLib2 and BigDataViewer as well
               as a memory efficient and fast implementation of the random
               forest based pixel classification algorithm as the foundation of
               our software. Optionally we harness the power of graphics
               processing units (GPU) to gain additional runtime performance.
               LABKIT is easy to install on virtually all laptops and
               workstations. Additionally, LABKIT is compatible with high
               performance computing (HPC) clusters for distributed processing
               of big image data. The ability to use pixel classifiers trained
               in LABKIT via the ImageJ macro language enables our users to
               integrate this functionality as a processing step in automated
               image processing workflows. Finally, LABKIT comes with rich
               online resources such as tutorials and examples that will help
               users to familiarize themselves with available features and how
               to best use LABKIT in a number of practical real-world use-cases."
}
@inproceedings{Schmidt2018,
	title        = "Cell Detection with Star-Convex Polygons",
	author       = "Schmidt, Uwe and Weigert, Martin and Broaddus, Coleman and Myers,
               Gene",
	year         = 2018,
	booktitle    = "Medical Image Computing and Computer Assisted Intervention –
               MICCAI 2018",
	publisher    = "Springer International Publishing",
	address      = {Cham},
	series       = {Lecture {Notes} in {Computer} {Science}},
	pages        = "265--273",
	doi          = {10.1007/978-3-030-00934-2_30},
	isbn         = {978-3-030-00934-2},
	abstract     = "Automatic detection and segmentation of cells and nuclei in
               microscopy images is important for many biological applications.
               Recent successful learning-based approaches include per-pixel
               cell segmentation with subsequent pixel grouping, or localization
               of bounding boxes with subsequent shape refinement. In situations
               of crowded cells, these can be prone to segmentation errors, such
               as falsely merging bordering cells or suppressing valid cell
               instances due to the poor approximation with bounding boxes. To
               overcome these issues, we propose to localize cell nuclei via
               star-convex polygons, which are a much better shape
               representation as compared to bounding boxes and thus do not need
               shape refinement. To that end, we train a convolutional neural
               network that predicts for every pixel a polygon for the cell
               instance at that position. We demonstrate the merits of our
               approach on two synthetic datasets and one challenging dataset of
               diverse fluorescence microscopy images.",
	language     = {en},
	url			 = {https://link.springer.com/chapter/10.1007/978-3-030-00934-2_30},
	editor       = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and Alberola-López, Carlos and Fichtinger, Gabor}
}
@article{Goldsborough2024,
	title        = "{InstanSeg}: an embedding-based instance segmentation
                   algorithm optimized for accurate, efficient and portable cell
                   segmentation",
	author       = "Goldsborough, Thibaut and Philps, Ben and O'Callaghan, Alan
                   and Inglis, Fiona and Leplat, Leo and Filby, Andrew and
                   Bilen, Hakan and Bankhead, Peter",
	year         = 2024,
	month        = 8,
	journal      = "arXiv [cs.CV]",
	abstract     = "Cell and nucleus segmentation are fundamental tasks for
                   quantitative bioimage analysis. Despite progress in recent
                   years, biologists and other domain experts still require
                   novel algorithms to handle increasingly large and complex
                   real-world datasets. These algorithms must not only achieve
                   state-of-the-art accuracy, but also be optimized for
                   efficiency, portability and user-friendliness. Here, we
                   introduce InstanSeg: a novel embedding-based instance
                   segmentation pipeline designed to identify cells and nuclei
                   in microscopy images. Using six public cell segmentation
                   datasets, we demonstrate that InstanSeg can significantly
                   improve accuracy when compared to the most widely used
                   alternative methods, while reducing the processing time by at
                   least 60\%. Furthermore, InstanSeg is designed to be fully
                   serializable as TorchScript and supports GPU acceleration on
                   a range of hardware. We provide an open-source implementation
                   of InstanSeg in Python, in addition to a user-friendly,
                   interactive QuPath extension for inference written in Java.
                   Our code and pre-trained models are available at
                   https://github.com/instanseg/instanseg .",
	archiveprefix = "arXiv",
	primaryclass = "cs.CV",
	url			 = {https://doi.org/10.48550/arXiv.2408.15954}
}
@article{Pachitariu2022,
	title        = "Cellpose 2.0: how to train your own model",
	shorttitle   = {Cellpose 2.0},
	author       = "Pachitariu, Marius and Stringer, Carsen",
	year         = 2022,
	month        = 12,
	journal      = "Nat. Methods",
	publisher    = "Springer Science and Business Media LLC",
	volume       = 19,
	number       = 12,
	pages        = "1634--1641",
	doi          = {10.1038/s41592-022-01663-4},
	issn         = {1548-7105},
	url          = {http://www.nature.com/articles/s41592-022-01663-4},
	urldate      = {2022-11-09},
	copyright    = {2022 The Author(s)},
	note         = {Publisher: Nature Publishing Group},
	abstract     = "Pretrained neural network models for biological segmentation can
               provide good out-of-the-box results for many image types.
               However, such models do not allow users to adapt the segmentation
               style to their specific needs and can perform suboptimally for
               test images that are very different from the training images.
               Here we introduce Cellpose 2.0, a new package that includes an
               ensemble of diverse pretrained models as well as a
               human-in-the-loop pipeline for rapid prototyping of new custom
               models. We show that models pretrained on the Cellpose dataset
               can be fine-tuned with only 500-1,000 user-annotated regions of
               interest (ROI) to perform nearly as well as models trained on
               entire datasets with up to 200,000 ROI. A human-in-the-loop
               approach further reduced the required user annotation to 100-200
               ROI, while maintaining high-quality segmentations. We provide
               software tools such as an annotation graphical user interface, a
               model zoo and a human-in-the-loop pipeline to facilitate the
               adoption of Cellpose 2.0.",
	language     = "en",
	keywords     = {Computational platforms and environments, Image processing}
}
@article{Zhang2023,
	title        = "Bio-Image Informatics Index {BIII}: A unique database of
                   image analysis tools and workflows for and by the bioimaging
                   community",
	author       = "Zhang, Chong and Gaignard, Alban and Kalas, Matus and Levet,
                   Florian and Delestro, Felipe and Lindblad, Joakim and
                   Sladoje, Natasa and Plantard, Laure and Latour, Alain and
                   Haase, Robert and Martins, Gabriel and Sampaio, Paula and
                   Scholz, Leandro and Taggers, Neubias and Tosi, Sébastien and
                   Miura, Kota and Colombelli, Julien and Paul-Gilloteaux,
                   Perrine",
	year         = 2023,
	month        = 12,
	journal      = "arXiv [q-bio.QM]",
	abstract     = "Bio image analysis has recently become one keystone of
                   biological research but biologists tend to get lost in a
                   plethora of available software and the way to adjust
                   available tools to their own image analysis problem. We
                   present BIII, BioImage Informatic Index (www.biii.eu), the
                   result of the first large community effort to bridge the
                   communities of algorithm and software developers, bioimage
                   analysts and biologists, under the form of a web-based
                   knowledge database crowdsourced by these communities.
                   Software tools (> 1300), image databases for benchmarking
                   (>20) and training materials (>70) for bio image analysis are
                   referenced and curated following standards constructed by the
                   community and then reaching a broader audience. Software
                   tools are organized as full protocol of analysis (workflow),
                   specific brick (component) to construct a workflow, or
                   software platform or library (collection). They are described
                   using Edam Bio Imaging, which is iteratively defined using
                   this website. All entries are exposed following FAIR
                   principles and accessible for other usage.",
	archiveprefix = "arXiv",
	primaryclass = "q-bio.QM",
	url			  = {https://doi.org/10.48550/arXiv.2312.11256}
}
@article{Ouyang2022,
	title        = "{BioImage} Model Zoo: A Community-Driven Resource for Accessible
              Deep Learning in {BioImage} Analysis",
	shorttitle   = {{BioImage} {Model} {Zoo}},
	author       = "Ouyang, Wei and Beuttenmueller, Fynn and Gómez-de-Mariscal,
              Estibaliz and Pape, Constantin and Burke, Tom and
              Garcia-López-de-Haro, Carlos and Russell, Craig and Moya-Sans,
              Lucía and de-la-Torre-Gutiérrez, Cristina and Schmidt, Deborah and
              Kutra, Dominik and Novikov, Maksim and Weigert, Martin and
              Schmidt, Uwe and Bankhead, Peter and Jacquemet, Guillaume and
              Sage, Daniel and Henriques, Ricardo and Muñoz-Barrutia, Arrate and
              Lundberg, Emma and Jug, Florian and Kreshuk, Anna",
	year         = 2022,
	month        = 6,
	journal      = "bioRxiv",
	publisher    = {bioRxiv},
	pages        = "2022.06.07.495102",
	doi          = {10.1101/2022.06.07.495102},
	url          = {https://www.biorxiv.org/content/10.1101/2022.06.07.495102v1},
	urldate      = {2023-10-05},
	copyright    = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	note         = {Pages: 2022.06.07.495102 Section: New Results},
	abstract     = "Deep learning-based approaches are revolutionizing imaging-driven
              scientific research. However, the accessibility and
              reproducibility of deep learning-based workflows for imaging
              scientists remain far from sufficient. Several tools have recently
              risen to the challenge of democratizing deep learning by providing
              user-friendly interfaces to analyze new data with pre-trained or
              fine-tuned models. Still, few of the existing pre-trained models
              are interoperable between these tools, critically restricting a
              model’s overall utility and the possibility of validating and
              reproducing scientific analyses. Here, we present the BioImage
              Model Zoo (): a community-driven, fully open resource where
              standardized pre-trained models can be shared, explored, tested,
              and downloaded for further adaptation or direct deployment in
              multiple end user-facing tools (e.g., ilastik, deepImageJ, QuPath,
              StarDist, ImJoy, ZeroCostDL4Mic, CSBDeep). To enable everyone to
              contribute and consume the Zoo resources, we provide a model
              standard to enable cross-compatibility, a rich list of example
              models and practical use-cases, developer tools, documentation,
              and the accompanying infrastructure for model upload, download and
              testing. Our contribution aims to lay the groundwork to make deep
              learning methods for microscopy imaging findable, accessible,
              interoperable, and reusable (FAIR) across software tools and
              platforms. \#\#\# Competing Interest Statement The authors have
              declared no competing interest.",
	language     = "en"
}
@article{Rueden2019,
	title        = "Scientific Community Image Forum: A discussion forum for
              scientific image software",
	author       = "Rueden, Curtis T and Ackerman, Jeanelle and Arena, Ellen T and
              Eglinger, Jan and Cimini, Beth A and Goodman, Allen and Carpenter,
              Anne E and Eliceiri, Kevin W",
	year         = 2019,
	month        = 6,
	journal      = "PLoS Biol.",
	volume       = 17,
	number       = 6,
	pages        = "e3000340",
	abstract     = "Forums and email lists play a major role in assisting scientists
              in using software. Previously, each open-source bioimaging
              software package had its own distinct forum or email list.
              Although each provided access to experts from various software
              teams, this fragmentation resulted in many scientists not knowing
              where to begin with their projects. Thus, the scientific imaging
              community lacked a central platform where solutions could be
              discussed in an open, software-independent manner. In response, we
              introduce the Scientific Community Image Forum, where users can
              pose software-related questions about digital image analysis,
              acquisition, and data management.",
	language     = "en",
	url			 = {https://doi.org/10.1371/journal.pbio.3000340}
}
@article{Gomez-de-Mariscal2021,
	title        = "{DeepImageJ}: A user-friendly environment to run deep learning
               models in {ImageJ}",
	author       = "Gómez-de-Mariscal, Estibaliz and García-López-de-Haro, Carlos and
               Ouyang, Wei and Donati, Laurène and Lundberg, Emma and Unser,
               Michael and Muñoz-Barrutia, Arrate and Sage, Daniel",
	year         = 2021,
	month        = 10,
	journal      = "Nat. Methods",
	publisher    = "Springer Science and Business Media LLC",
	volume       = 18,
	number       = 10,
	pages        = "1192--1195",
	abstract     = "DeepImageJ is a user-friendly solution that enables the generic
               use of pre-trained deep learning models for biomedical image
               analysis in ImageJ. The deepImageJ environment gives access to
               the largest bioimage repository of pre-trained deep learning
               models (BioImage Model Zoo). Hence, nonexperts can easily perform
               common image processing tasks in life-science research with deep
               learning-based tools including pixel and object classification,
               instance segmentation, denoising or virtual staining. DeepImageJ
               is compatible with existing state of the art solutions and it is
               equipped with utility tools for developers to include new models.
               Very recently, several training frameworks have adopted the
               deepImageJ format to deploy their work in one of the most used
               softwares in the field (ImageJ). Beyond its direct use, we expect
               deepImageJ to contribute to the broader dissemination and reuse
               of deep learning models in life sciences applications and
               bioimage informatics.",
	language     = "en",
	url			 = {https://doi.org/10.1038/s41592-021-01262-9}
}
@article{Ouyang2019,
	title        = "{ImJoy}: an open-source computational platform for the deep
               learning era",
	author       = "Ouyang, Wei and Mueller, Florian and Hjelmare, Martin and
               Lundberg, Emma and Zimmer, Christophe",
	year         = 2019,
	month        = 12,
	journal      = "Nat. Methods",
	publisher    = "Springer Science and Business Media LLC",
	volume       = 16,
	number       = 12,
	pages        = "1199--1200",
	language     = "en",
	url			 = {https://doi.org/10.1038/s41592-019-0627-0}
}
@misc{Shah,
	title        = "Bilayers",
	author       = "Shah, Rajavi and Gogoberidze, Nodar and Cimini, Beth",
	url          = "https://github.com/bilayer-containers/bilayers",
	abstract     = "A Container Specification and CI/CD built for whole-community
              support"
}
@article{Hidalgo-Cenalmor2024,
	title        = "{DL4MicEverywhere}: deep learning for microscopy made flexible,
               shareable and reproducible",
	shorttitle   = {{DL4MicEverywhere}},
	author       = "Hidalgo-Cenalmor, Iván and Pylvänäinen, Joanna W and G Ferreira,
               Mariana and Russell, Craig T and Saguy, Alon and
               Arganda-Carreras, Ignacio and Shechtman, Yoav and {AI4Life
               Horizon Europe Program Consortium} and Jacquemet, Guillaume and
               Henriques, Ricardo and Gómez-de-Mariscal, Estibaliz",
	year         = 2024,
	month        = 6,
	journal      = "Nat. Methods",
	publisher    = "Springer Science and Business Media LLC",
	volume       = 21,
	number       = 6,
	pages        = "925--927",
	doi          = {10.1038/s41592-024-02295-6},
	issn         = {1548-7091, 1548-7105},
	url          = {https://www.nature.com/articles/s41592-024-02295-6},
	urldate      = {2024-05-20},
	language     = "en"
}
@inproceedings{jupyter,
	title        = {Jupyter Notebooks - a publishing format for reproducible computational workflows},
	author       = {Thomas Kluyver and Benjamin Ragan-Kelley and Fernando P{\'e}rez and Brian Granger and Matthias Bussonnier and Jonathan Frederic and Kyle Kelley and Jessica Hamrick and Jason Grout and Sylvain Corlay and Paul Ivanov and Dami{\'a}n Avila and Safia Abdalla and Carol Willing and  Jupyter development team},
	year         = 2016,
	booktitle    = {Positioning and Power in Academic Publishing: Players, Agents and Agendas},
	publisher    = {IOS Press},
	address      = {Netherlands},
	pages        = {87--90},
	url          = {https://eprints.soton.ac.uk/403913/},
	editor       = {Fernando Loizides and Birgit Scmidt},
	abstract     = {It is increasingly necessary for researchers in all fields to write computer code, and in order to reproduce research results, it is important that this code is published. We present Jupyter notebooks, a document format for publishing code, results and explanations in a form that is both readable and executable. We discuss various tools and use cases for notebook documents.}
}
@article{Kreshuk2019,
	title        = "Machine Learning: Advanced Image Segmentation Using ilastik",
	author       = "Kreshuk, Anna and Zhang, Chong",
	year         = 2019,
	journal      = "Methods Mol. Biol.",
	volume       = 2040,
	pages        = "449--463",
	abstract     = "Segmentation is one of the most ubiquitous problems in biological
              image analysis. Here we present a machine learning-based solution
              to it as implemented in the open source ilastik toolkit. We give a
              broad description of the underlying theory and demonstrate two
              workflows: Pixel Classification and Autocontext. We illustrate
              their use on a challenging problem in electron microscopy image
              segmentation. After following this walk-through, we expect the
              readers to be able to apply the necessary steps to their own data
              and segment their images by either workflow.",
	keywords     = "Machine learning; Random forest; Semantic segmentation; ilastik",
	language     = "en",
	url			 = {https://link.springer.com/protocol/10.1007/978-1-4939-9686-5_21}
}
@article{arganda-carreras2017,
	title        = {Trainable {Weka} {Segmentation}: a machine learning tool for microscopy pixel classification},
	shorttitle   = {Trainable {Weka} {Segmentation}},
	author       = {Arganda-Carreras, Ignacio and Kaynig, Verena and Rueden, Curtis and Eliceiri, Kevin W and Schindelin, Johannes and Cardona, Albert and Sebastian Seung, H},
	year         = 2017,
	month        = 8,
	journal      = {Bioinformatics},
	volume       = 33,
	number       = 15,
	pages        = {2424--2426},
	doi          = {10.1093/bioinformatics/btx180},
	issn         = {1367-4803},
	url          = {https://doi.org/10.1093/bioinformatics/btx180},
	urldate      = {2023-08-22},
	abstract     = {State-of-the-art light and electron microscopes are capable of acquiring large image datasets, but quantitatively evaluating the data often involves manually annotating structures of interest. This process is time-consuming and often a major bottleneck in the evaluation pipeline. To overcome this problem, we have introduced the Trainable Weka Segmentation (TWS), a machine learning tool that leverages a limited number of manual annotations in order to train a classifier and segment the remaining data automatically. In addition, TWS can provide unsupervised segmentation learning schemes (clustering) and can be customized to employ user-designed image features or classifiers.TWS is distributed as open-source software as part of the Fiji image processing distribution of ImageJ at http://imagej.net/Trainable\_Weka\_Segmentation.Supplementary data are available at Bioinformatics online.}
}
@article{fazeli2020,
	title        = {Automated cell tracking using {StarDist} and {TrackMate}},
	author       = {Fazeli, Elnaz and Roy, Nathan H. and Follain, Gautier and Laine, Romain F. and von Chamier, Lucas and Hänninen, Pekka E. and Eriksson, John E. and Tinevez, Jean-Yves and Jacquemet, Guillaume},
	year         = 2020,
	journal      = {F1000Research},
	volume       = 9,
	pages        = 1279,
	doi          = {10.12688/f1000research.27019.1},
	issn         = {2046-1402},
	abstract     = {The ability of cells to migrate is a fundamental physiological process involved in embryonic development, tissue homeostasis, immune surveillance, and wound healing. Therefore, the mechanisms governing cellular locomotion have been under intense scrutiny over the last 50 years. One of the main tools of this scrutiny is live-cell quantitative imaging, where researchers image cells over time to study their migration and quantitatively analyze their dynamics by tracking them using the recorded images. Despite the availability of computational tools, manual tracking remains widely used among researchers due to the difficulty setting up robust automated cell tracking and large-scale analysis. Here we provide a detailed analysis pipeline illustrating how the deep learning network StarDist can be combined with the popular tracking software TrackMate to perform 2D automated cell tracking and provide fully quantitative readouts. Our proposed protocol is compatible with both fluorescent and widefield images. It only requires freely available and open-source software (ZeroCostDL4Mic and Fiji), and does not require any coding knowledge from the users, making it a versatile and powerful tool for the field. We demonstrate this pipeline's usability by automatically tracking cancer cells and T cells using fluorescent and brightfield images. Importantly, we provide, as supplementary information, a detailed step-by-step protocol to allow researchers to implement it with their images.},
	language     = {eng},
	pmid         = 33224481,
	pmcid        = {PMC7670479},
	keywords     = {Automated tracking, Cell migration, Cell Movement, Cell Tracking, Deep-learning, Fiji, Image analysis, Image Processing, Computer-Assisted, Software, StarDist, TrackMate},
	url			 = {https://f1000research.com/articles/9-1279/v1}
}
@inproceedings{krizhevsky2012,
	title        = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year         = 2012,
	booktitle    = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher    = {Curran Associates, Inc.},
	volume       = 25,
	url          = {https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	urldate      = {2023-10-05},
	abstract     = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.}
}
@misc{ahlers2023,
	title        = {napari: a multi-dimensional image viewer for {Python}},
	shorttitle   = {napari},
	author       = {Ahlers, Jannis and Althviz Moré, Daniel and Amsalem, Oren and Anderson, Ashley and Bokota, Grzegorz and Boone, Peter and Bragantini, Jordão and Buckley, Genevieve and Burt, Alister and Bussonnier, Matthias and Can Solak, Ahmet and Caporal, Clément and Doncila Pop, Draga and Evans, Kira and Freeman, Jeremy and Gaifas, Lorenzo and Gohlke, Christoph and Gunalan, Kabilar and Har-Gil, Hagai and Harfouche, Mark and Harrington, Kyle I. S. and Hilsenstein, Volker and Hutchings, Katherine and Lambert, Talley and Lauer, Jessy and Lichtner, Gregor and Liu, Ziyang and Liu, Lucy and Lowe, Alan and Marconato, Luca and Martin, Sean and McGovern, Abigail and Migas, Lukasz and Miller, Nadalyn and Muñoz, Hector and Müller, Jan-Hendrik and Nauroth-Kreß, Christopher and Nunez-Iglesias, Juan and Pape, Constantin and Pevey, Kim and Peña-Castellanos, Gonzalo and Pierré, Andrea and Rodríguez-Guerra, Jaime and Ross, David and Royer, Loic and Russell, Craig T. and Selzer, Gabriel and Smith, Paul and Sobolewski, Peter and Sofiiuk, Konstantin and Sofroniew, Nicholas and Stansby, David and Sweet, Andrew and Vierdag, Wouter-Michiel and Wadhwa, Pam and Weber Mendonça, Melissa and Windhager, Jonas and Winston, Philip and Yamauchi, Kevin},
	year         = 2023,
	month        = 7,
	publisher    = {Zenodo},
	doi          = {10.5281/zenodo.8115575},
	url          = {https://zenodo.org/record/8115575},
	urldate      = {2023-10-05},
	abstract     = {napari 0.4.18 We're happy to announce the release of napari 0.4.18! napari is a fast, interactive, multi-dimensional image viewer for Python. It's designed for browsing, annotating, and analyzing large multi-dimensional images. It's built on top of Qt (for the GUI), vispy (for performant GPU-based rendering), and the scientific Python stack (numpy, scipy). This is primarily a bug-fix release, addressing many issues from 0.4.17 (see "Bug Fixes", below). However, it also contains some performance improvements and several exciting new features (see "Highlights"), so read on below! For more information, examples, and documentation, please visit our website: https://napari.org Highlights Drawing polygons in the Shapes layer can now be done much faster with the new lasso tool (napari/napari/\#5555) Surface layers now support textures and vertex colors, allowing a whole new type of dataset to be visualised in napari. Have a look at surface\_multi\_texture.py and surface\_texture\_and\_colors.py in the examples directory for some pretty demos! (napari/napari/\#5642) Previously, navigating an image required switching out of whatever drawing mode you might have been using and going back to pan/zoom mode. Now you can use the mouse wheel to zoom in and out in any mode. (napari/napari/\#5701) Painting labels is now much, much faster (achieving 60fps even on an 8k x 8k image) (napari/napari/\#5723 and napari/napari/\#5732) Vectors layers can now be displayed with two different styles of arrowheads, instead of just plain lines. This removes a longstanding limitation of the vectors layer! (napari/napari/\#5740) New Features Overlays 2.0 (napari/napari/\#4894) expose custom image interpolation kernels (napari/napari/\#5130) Add user agent environment variable for pip installations (napari/napari/\#5135) Add option to check if plugin try to set viewer attr outside main thread (napari/napari/\#5195) Set selection color for QListView item. (napari/napari/\#5202) Add warning about set private attr when using proxy (napari/napari/\#5209) Shapes interpolation (napari/napari/\#5334) Add dask settings to preferences (napari/napari/\#5490) Add lasso tool for faster drawing of polygonal Shapes (napari/napari/\#5555) Feature: support for textures and vertex colors on Surface layers (napari/napari/\#5642) Back point selection with a psygnal Selection (napari/napari/\#5691) Zooming with the mouse wheel in any mode (napari/napari/\#5701) Add cancellation functionality to progress (napari/napari/\#5728) Add arrow display styles to Vectors layer (napari/napari/\#5740) Improvements Set keyboard focus on console when opened (napari/napari/\#5208) Push variables to console when instantiated (napari/napari/\#5210) Tracks layer creation performance improvement (napari/napari/\#5303) PERF: Event emissions and perf regression. (napari/napari/\#5307) Much faster FormatStringEncoding (napari/napari/\#5315) Add parent when creating layer context menu to inherit application theme and add style entry for disabled widgets and menus (napari/napari/\#5381) Add correct enablement kwarg to Split Stack action, Convert data type submenu and Projections submenu (napari/napari/\#5437) Apply disabled widgets style only for menus and set menus styles for QModelMenu and QMenu instances (napari/napari/\#5446) Add disabled style rule for QComboBox following the one for QPushButton (napari/napari/\#5469) Allow layers control section to resize to contents (napari/napari/\#5474) Allow to use Optional annotation in function return type for magicgui functions (napari/napari/\#5595) Skip equality comparisons in EventedModel when unnecessary (napari/napari/\#5615) Bugfix: improve layout of Preferences {\textgreater} Shortcuts tables (napari/napari/\#5679) Improve preferences genration (napari/napari/\#5696) Add dev example for adding custom overlays. (napari/napari/\#5719) Disable buffer swapping (napari/napari/\#5741) Remove max brush size from increase brush size keybinding (napari/napari/\#5761) Explicitly list valid layer names in types (napari/napari/\#5823) Sort npe1 widget contributions (napari/napari/\#5865) feat: add since\_version argument of rename\_argument decorator (napari/napari/\#5910) Emit extra information with layer.events.data (napari/napari/\#5967) Performance Return early when no slicing needed (napari/napari/\#5239) Tracks layer creation performance improvement (napari/napari/\#5303) PERF: Event emissions and perf regression. (napari/napari/\#5307) Much faster FormatStringEncoding (napari/napari/\#5315) Fix inefficient label mapping in direct color mode (10-20x speedup) (napari/napari/\#5723) Efficient labels mapping for drawing in Labels (60 FPS even with 8000x8000 images) (napari/napari/\#5732) Disable buffer swapping (napari/napari/\#5741) Bug Fixes Warn instead of failing on empty or invalid alt-text (napari/napari/\#4505) Fix display of order and scale combinations (napari/napari/\#5004) Enforce that contrast limits must be increasing (napari/napari/\#5036) Bugfix: Move Window menu to be before Help (napari/napari/\#5093) Add extra garbage collection for some viewer tests (napari/napari/\#5108) Connect image to plane events and expose them (napari/napari/\#5131) Workaround for discover themes from plugins (napari/napari/\#5150) Add missed dialogs to qtbot in test\_qt\_notifications to prevent segfaults (napari/napari/\#5171) DOC Update docstring of add\_dock\_widget \& \_add\_viewer\_dock\_widget (napari/napari/\#5173) Fix unsortable features (napari/napari/\#5186) Avoid possible divide-by-zero in Vectors layer thumbnail update (napari/napari/\#5192) Disable napari-console button when launched from jupyter (napari/napari/\#5213) Volume rendering updates for isosurface and attenuated MIP (napari/napari/\#5215) Return early when no slicing needed (napari/napari/\#5239) Check strictly increasing values when clipping contrast limits to a new range (napari/napari/\#5258) UI Bugfix: Make disabled QPushButton more distinct (napari/napari/\#5262) Respect background color when calculating scale bar color (napari/napari/\#5270) Fix circular import in \_vispy module (napari/napari/\#5276) Use only data dimensions for cord in status bar (napari/napari/\#5283) Prevent obsolete reports about failure of cleaning viewer instances (napari/napari/\#5317) Add scikit-image[data] to install\_requires, because it's required by builtins (napari/napari/\#5329) Fix repeating close dialog on macOS and qt 5.12 (napari/napari/\#5337) Disable napari-console if napari launched from vanilla python REPL (napari/napari/\#5350) For npe2 plugin, use manifest display\_name for File {\textgreater} Open Samples (napari/napari/\#5351) Bugfix plugin display\_name use (File {\textgreater} Open Sample, Plugin menus) (napari/napari/\#5366) Fix editing shape data above 2 dimensions (napari/napari/\#5383) Fix test keybinding for layer actions (napari/napari/\#5406) fix theme id not being used correctly (napari/napari/\#5412) Clarify layer's editable property and separate interaction with visible property (napari/napari/\#5413) Fix theme reference to get image for success\_label style (napari/napari/\#5447) Bugfix: Ensure layer.\_fixed\_vertex is set when rotating (napari/napari/\#5449) Fix \_n\_selected\_points in \_layerlist\_context.py (napari/napari/\#5450) Refactor Main Window status bar to improve information presentation (napari/napari/\#5451) Bugfix: Fix test\_get\_system\_theme test for name to id change (napari/napari/\#5456) Bugfix: POLL\_INTERVAL\_MS used in QTimer needs to be an int on python 3.10 (napari/napari/\#5467) Bugfix: Add missing Enums and Flags required by PySide6 {\textgreater} 6.4 (napari/napari/\#5480) BugFix: napari does not start with Python v3.11.1: "ValueError: A distribution name is required." (napari/napari/\#5482) Fix inverted LUT and blending (napari/napari/\#5487) Fix opening file dialogs in PySide (napari/napari/\#5492) Handle case when QtDims play thread is partially deleted (napari/napari/\#5499) Ensure surface normals and wireframes are using Models internally (napari/napari/\#5501) Recursively check for dependent property to fire events. (napari/napari/\#5528) Set PYTHONEXECUTABLE as part of macos fixes on (re)startup (napari/napari/\#5531) Un-set unified title and tool bar on mac (Qt property) (napari/napari/\#5533) Fix key error issue of action manager (napari/napari/\#5539) Bugfix: ensure Checkbox state comparisons are correct by using Qt.CheckState(state) (napari/napari/\#5541) Clean dangling widget in test (napari/napari/\#5544) Fix test\_worker\_with\_progress by wait on worker end (napari/napari/\#5548) Fix min req (napari/napari/\#5560) Fix vispy axes labels (napari/napari/\#5565) Fix colormap utils error suggestion code and add a test (napari/napari/\#5571) Fix problem of missing plugin widgets after minimize napari (napari/napari/\#5577) Make point size isotropic (napari/napari/\#5582) Fix guard of qt import in napari.utils.theme (napari/napari/\#5593) Fix empty shapes layer duplication and Convert to Labels enablement logic for selected empty shapes layers (napari/napari/\#5594) Stop using removed multichannel= kwarg to skimage functions (napari/napari/\#5596) Add information about syntax\_style value in error message for theme validation (napari/napari/\#5602) Remove catch\_warnings in slicing (napari/napari/\#5603) Incorret theme should not prevent napari from start (napari/napari/\#5605) Unblock axis labels event to be emitted when slider label changes (napari/napari/\#5631) Bugfix: IndexError slicing Surface with higher-dimensional vertex\_values (napari/napari/\#5635) Bugfix: Convert Viewer Delete button to QtViewerPushButton with action and shortcut (napari/napari/\#5636) Change dim axis\_label resize logic to set width using only displayed labels width (napari/napari/\#5640) Feature: support for textures and vertex colors on Surface layers (napari/napari/\#5642) Fix features issues with init param and property setter (napari/napari/\#5646) Bugfix: Don't double toggle visibility for linked layers (napari/napari/\#5656) Bugfix: ensure pan/zoom buttons work, along with spacebar keybinding (napari/napari/\#5669) Bugfix: Add Tracks to qt\_keyboard\_settings (napari/napari/\#5678) Fix automatic naming and GUI exposure of multiple unnamed colormaps (napari/napari/\#5682) Fix mouse movement handling for TransformBoxOverlay (napari/napari/\#5692) Update environment.yml (napari/napari/\#5693) Resolve symlinks from path to environment for setting path (napari/napari/\#5704) Fix tracks color-by when properties change (napari/napari/\#5708) Fix Sphinx warnings (napari/napari/\#5717) Do not use depth for canvas overlays; allow setting blending mode for overlays (napari/napari/\#5720) Unify event behaviour for points and its qt controls (napari/napari/\#5722) Fix camera 3D absolute rotation bug (napari/napari/\#5726) Maint: Bump mypy (napari/napari/\#5727) Style QGroupBox indicator (napari/napari/\#5729) Fix centering of non-displayed dimensions (napari/napari/\#5736) Don't attempt to use npe1 readers in napari.plugins.\_npe2.read (napari/napari/\#5739) Prevent canvas micro-panning on point add (napari/napari/\#5742) Use text opacity to signal that widget is disabled (napari/napari/\#5745) Bugfix: Add the missed keyReleaseEvent method in QtViewerDockWidget (napari/napari/\#5746) Update status bar on active layer change (napari/napari/\#5754) Use array size directly when checking multiscale arrays to prevent overflow (napari/napari/\#5759) Fix path to check\_updated\_packages.py (napari/napari/\#5762) Brush cursor implementation using an overlay (napari/napari/\#5763) Bugfix: force a redraw to ensure highlight shows when Points are select-all selected (napari/napari/\#5771) Fix copy/paste of points (napari/napari/\#5795) Fix multiple viewer example (napari/napari/\#5796) Fix colormapping nD images (napari/napari/\#5805) Set focus policy for mainwindow to prevent keeping focus on the axis labels (and other QLineEdit based widgets) when clicking outside the widget (napari/napari/\#5812) Enforce Points.selected\_data type as Selection (napari/napari/\#5813) Change toggle menubar visibility functionality to hide menubar and show it on mouse movement validation (napari/napari/\#5824) Bugfix: Disconnect callbacks on object deletion in special functions from event\_utils (napari/napari/\#5826) Do not blend color in QtColorBox with black using opacity (napari/napari/\#5827) Don't allow negative contour values (napari/napari/\#5830) Bugfixes for layer overlays: clean up when layer is removed + fix potential double creation (napari/napari/\#5831) Add compatibility to PySide in file dialogs by using positional arguments (napari/napari/\#5834) Bugfix: fix broken "show selected" in the Labels layer (because of caching) (napari/napari/\#5841) Add tests for popup widgets and fix perspective popup slider initialization (napari/napari/\#5848) [Qt6] Fix AttributeError on renaming layer (napari/napari/\#5850) Bugfix: Ensure QTableWidgetItem(action.description) item is enabled (napari/napari/\#5854) Add constraints file during installation of packages from pip in docs workflow (napari/napari/\#5862) Bugfix: link the Labels model to the "show selected" checkbox (napari/napari/\#5867) Add \_\_all\_\_ to napari/types.py (napari/napari/\#5894) Fix drawing vertical or horizontal line segments in Shapes layer (napari/napari/\#5895) Disallow outside screen geometry napari window position (napari/napari/\#5915) Fix napari-svg version parsing in conftest.py (napari/napari/\#5947) Fix issue in utils.progress for disable=True (napari/napari/\#5964) Set high DPI attributes when using PySide2 (napari/napari/\#5968) [0.4.18rc1] Bugfix/event proxy (napari/napari/\#5994) Fix behavior of PublicOnlyProxy in setattr, wrapped methods, and calling (napari/napari/\#5997) Bugfix: Fix regression from \#5739 for passing plugin name and reader plus add test (napari/napari/\#6013) Avoid passing empty string to importlib.metadata.metadata (napari/napari/\#6018) Use tuple for pip constraints to avoid LRU cache error (napari/napari\#6036 API Changes Overlays 2.0 (napari/napari/\#4894) expose custom image interpolation kernels (napari/napari/\#5130) Connect image to plane events and expose them (napari/napari/\#5131) Deprecations Build Tools ci(dependabot): bump styfle/cancel-workflow-action from 0.10.0 to 0.10.1 (napari/napari/\#5158) ci(dependabot): bump actions/checkout from 2 to 3 (napari/napari/\#5160) ci(dependabot): bump styfle/cancel-workflow-action from 0.10.1 to 0.11.0 (napari/napari/\#5290) ci(dependabot): bump docker/login-action from 2.0.0 to 2.1.0 (napari/napari/\#5291) ci(dependabot): bump actions/upload-artifact from 2 to 3 (napari/napari/\#5292) Pin mypy version (napari/napari/\#5310) MAINT: Start testing on Python 3.11 in CI. (napari/napari/\#5439) Pin test dependencies (napari/napari/\#5715) Documentation Fix failure on benchmark reporting (napari/napari/\#5083) Add NAP-5: proposal for an updated napari logo (napari/napari/\#5084) DOC Update doc contributing guide (napari/napari/\#5114) Napari debugging during plugin development documentation (napari/napari/\#5142) DOC Update docstring of add\_dock\_widget \& \_add\_viewer\_dock\_widget (napari/napari/\#5173) Specified that the path is to the local folder in contributing documentation guide. (napari/napari/\#5191) Fixes broken links in latest docs version (napari/napari/\#5193) Fixes gallery ToC (napari/napari/\#5458) Fix broken link in EmitterGroup docstring (napari/napari/\#5465) Fix Sphinx warnings (napari/napari/\#5717) Add Fourier transform playground example (napari/napari/\#5872) Improve documentation of changed event in EventedList (napari/napari/\#5928) Set removal version in deprecation of Viewer.rounded\_division (napari/napari/\#5944) Update docs using changes from napari/docs (napari/napari/\#5979) Update CITATION.cff file with 0.4.18 contributors (napari/napari/\#5980) Pre commit fixes for 0.4.18 release branch (napari/napari/\#5985) Port changes from docs repo to main repo for 0.4.18 (napari/napari/\#6002) Add favicon and configuration (napari/docs/\#4) Docs for 5195 from main repository (napari/docs/\#7) Use imshow in getting\_started (napari/docs/\#9) DOC Update viewer.md (napari/docs/\#11) Add and/or update documentation alt text (napari/docs/\#12) Adding documents and images from January 2022 plugin testing workshop. (napari/docs/\#35) Add some more docs about packaging details and conda-forge releases (napari/docs/\#48) Add documentation on using virtual environments for testing in napari based on 2022-01 workshop by Talley Lambert (napari/docs/\#50) Added info for conda installation problems (napari/docs/\#51) add best practices about packaging (napari/docs/\#52) Update viewer tutorial, regarding the console button (napari/docs/\#53) add sample database page (napari/docs/\#56) Fix magicgui objects.inv url for intersphinx (napari/docs/\#58) Fix broken links (napari/docs/\#59) Add sphinx-design cards to Usage landing page (napari/docs/\#63) Update to napari viewer tutorial. (napari/docs/\#65) Added environment creation and doc tools install (napari/docs/\#72) Feature: add copy button for code blocks using sphinx-copybutton (napari/docs/\#76) Add NAP-6 - Proposal for contributable menus (napari/docs/\#77) Update contributing docs for [dev] install change needing Qt backend install (napari/docs/\#78) Update theme related documentation (napari/docs/\#81) Feature: implement python version substitution in conf.py (napari/docs/\#84) Fixes gallery ToC (napari/docs/\#85) Clarify arm64 macOS (Apple Silicon) installation (napari/docs/\#89) Add cards to usage landing pages (napari/docs/\#97) Replace pip with python -m pip (napari/docs/\#100) change blob example to be self contained (napari/docs/\#101) Home page update, take 2 (napari/docs/\#102) Update the 'ensuring correctness' mission clause (napari/docs/\#105) Update steering council listing on website (napari/docs/\#106) Update version switcher json (napari/docs/\#109) Installation: Add libmamba solver to conda Note (napari/docs/\#110) Update requirements and config for sphinx-favicon for 1.0 (napari/docs/\#116) change print to f-string (napari/docs/\#117) Replace non-breaking spaces with regular spaces (napari/docs/\#118) Bugfix: documentation update for napari PR \#5636 (napari/docs/\#123) Add matplotlib image scraper for gallery (napari/docs/\#130) Fix missing links and references (napari/docs/\#133) Update URL of version switcher (napari/docs/\#139) Harmonize release notes to new mandatory labels (napari/docs/\#141) Update installation docs to remove briefcase bundle mentions (napari/docs/\#147) Fix version switcher URL for the latest docs version (napari/docs/\#148) Update Shapes How-To for new Lasso tool (napari/\#5555) (napari/docs/\#149) Fix signpost to make\_napari\_viewer code (napari/docs/\#151) Docs for adding LayerData tuple to viewer (napari/docs/\#152) Update viewer tutorial 3D mode docs (napari/docs/\#159) Add a napari plugin debugging quick start section to the debugging guide (napari/docs/\#161) Pin npe2 version to match installed one (napari/docs/\#175) Add Wouter-Michiel Vierdag to list of core devs (napari/docs/\#181) Update SC information (napari/docs/\#192) Other Pull Requests use app-model for view menu (napari/napari/\#4826) Overlay backend refactor (napari/napari/\#4907) Migrate help menu to use app model (napari/napari/\#4922) Refactor layer slice/dims/view/render state (napari/napari/\#5003) MAINT: increase min numpy version. (napari/napari/\#5089) Refactor qt notification and its test solve problem of segfaults (napari/napari/\#5138) Decouple changing viewer.theme from changing theme settings/preferences (napari/napari/\#5143) [DOCS] misc invalid syntax updates. (napari/napari/\#5176) MAINT: remove vendored colorconv from skimage. (napari/napari/\#5180) Re-add README screenshot (napari/napari/\#5220) MAINT: remove requirements.txt and cache actions based on setup.cfg. (napari/napari/\#5234) Explicitly set test array data to fix a flaky test (napari/napari/\#5245) Add ruff linter to pre-commit (napari/napari/\#5275) Run tests on release branch (napari/napari/\#5277) Vispy 0.12: per-point symbol and several bugfixes (napari/napari/\#5312) Make all imports absolute (napari/napari/\#5318) Fix track ids features ordering for unordered tracks (napari/napari/\#5320) tests: remove private magicgui access in tests (napari/napari/\#5331) Make settings and cache separate per each environment. (napari/napari/\#5333) Remove internal event connection on SelectableEventedList (napari/napari/\#5339) Unset PYTHON* vars and use entitlements in macOS conda menu shortcut (napari/napari/\#5354) Distinguish between update\_dims, extent changes, and refresh (napari/napari/\#5363) Add checks for pending Qt threads and timers in tests (napari/napari/\#5373) Suppress color conversion warning when converting invalid LAB coordinates (napari/napari/\#5386) Fix warning when fail to import qt binding. (napari/napari/\#5388) Update MANIFEST.in to remove warning when run tox (napari/napari/\#5393) [Automatic] Update albertosottile/darkdetect vendored module (napari/napari/\#5394) Update citation metadata (napari/napari/\#5398) Feature: making the Help menu more helpful via weblinks (re-do of \#5094) (napari/napari/\#5399) [pre-commit.ci] pre-commit autoupdate (napari/napari/\#5403) Fix flaky dims playback test by waiting for playing condition (napari/napari/\#5414) [Automatic] Update albertosottile/darkdetect vendored module (napari/napari/\#5416) [pre-commit.ci] pre-commit autoupdate (napari/napari/\#5422) Avoid setting corner pixels for empty layers (napari/napari/\#5423) Maint: Typing and ImportError -{\textgreater} ModuleNotFoundError. (napari/napari/\#5431) Fix tox passenv setup for DISPLAY and XAUTHORITY environment variables (napari/napari/\#5441) Add error color to themes and change application close/exit dialogs (napari/napari/\#5442) Update screenshot in readme (napari/napari/\#5452) Maint: Fix sporadic QtDims garbage collection failures by converting some stray references to weakrefs (napari/napari/\#5471) Replace GabrielBB/xvfb-action (napari/napari/\#5478) Add tags to recently added examples (napari/napari/\#5486) Remove layer ndisplay event (napari/napari/\#5491) MAINT: Don't format logs in log call (napari/napari/\#5504) Replace flake8, isort and pyupgrade by ruff, enable additional usefull rules (napari/napari/\#5513) Second PR that enables more ruff rules. (napari/napari/\#5520) Use pytest-pretty for better log readability (napari/napari/\#5525) MAINT: Follow Nep29, bump minimum numpy. (napari/napari/\#5532) [pre-commit.ci] pre-commit autoupdate (napari/napari/\#5534) Move layer editable change from slicing to controls (napari/napari/\#5546) update conda\_menu\_config.json for latest fixes in menuinst (napari/napari/\#5564) Enable the COM and SIM rules in ruff configuration (napari/napari/\#5566) Move from ubuntu 18.04 to ubuntu 20.04 in workflows (napari/napari/\#5578) FIX: Fix --pre skimage that have a more precise warning message (napari/napari/\#5580) Remove leftover duplicated code (napari/napari/\#5586) Remove napari-hub API access code (napari/napari/\#5587) Enable ruff rules part 4. (napari/napari/\#5590) [pre-commit.ci] pre-commit autoupdate (napari/napari/\#5592) Maint: ImportError -{\textgreater} ModuleNotFoundError. (napari/napari/\#5628) [pre-commit.ci] pre-commit autoupdate (napari/napari/\#5645) MAINT: Do not use mutable default for dataclass. (napari/napari/\#5647) MAINT: Do not use cgi-traceback on 3.11+ (deprecated, marked for removal) (napari/napari/\#5648) MAINT: Add explicit level to warn. (napari/napari/\#5649) MAINT: Split test file in two to find hanging test. (napari/napari/\#5680) Skip pyside6 version 6.4.3 for tests (napari/napari/\#5683) Pin pydantic. (napari/napari/\#5695) fix test\_viewer\_open\_no\_plugin exception message expectation (napari/napari/\#5698) fix: Block PySide6==6.5.0 in tests (napari/napari/\#5702) Don't resize shape after Shift release until mouse moves (napari/napari/\#5707) Update test\_examples job dependencies, unskip surface\_timeseries\_.py and update some examples validations (napari/napari/\#5716) Add test to check basic interactions with layer controls widgets (napari/napari/\#5757) test: [Automatic] Constraints upgrades: dask, hypothesis, imageio, npe2, numpy, pandas, psutil, pygments, pytest, rich, tensorstore, tifffile, virtualenv, xarray (napari/napari/\#5776) [MAINT, packaging] Remove support for briefcase installers (napari/napari/\#5804) Update PIP\_CONSTRAINT value to fix failing comprehensive jobs (napari/napari/\#5809) [pre-commit.ci] pre-commit autoupdate (napari/napari/\#5836) [pre-commit.ci] pre-commit autoupdate (napari/napari/\#5860) Fix Dev Docker Container (napari/napari/\#5877) Make mypy error checking opt-out instead of opt-in (napari/napari/\#5885) Update Error description when plugin not installed (napari/napari/\#5899) maint: add fixture to disable throttling (napari/napari/\#5908) Update upgrade dependecies and test workflows (napari/napari/\#5919) [Maint] Fix comprehensive tests by skipping labels controls test on py311 pyqt6 (napari/napari/\#5922) Fix typo in resources/requirements\_mypy.in file name (napari/napari/\#5924) Add Python 3.11 trove classifier. (napari/napari/\#5937) Change license\_file to license\_files in setup.cfg (napari/napari/\#5948) test: [Automatic] Constraints upgrades: dask, fsspec, hypothesis, imageio, ipython, napari-plugin-manager, napari-svg, numpy, psygnal, pydantic, pyqt6, pytest, rich, scikit-image, virtualenv, zarr (napari/napari/\#5963) Update deprecation information (napari/napari/\#5984) Pin napari and pydantic when installing a plugin (napari/napari/\#6022) 40 authors added to this release (alphabetical) Alister Burt - @alisterburt Andrea Pierré - @kir0ul Andrew Sweet - @andy-sweet Ashley Anderson - @aganders3 Clément Caporal - @ClementCaporal Constantin Pape - @constantinpape Craig T. Russell - @ctr26 Daniel Althviz Moré - @dalthviz David Ross - @davidpross David Stansby - @dstansby Gabriel Selzer - @gselzer Gonzalo Peña-Castellanos - @goanpeca Gregor Lichtner - @glichtner Grzegorz Bokota - @Czaki Jaime Rodríguez-Guerra - @jaimergp Jan-Hendrik Müller - @kolibril13 Jannis Ahlers - @jnahlers Jessy Lauer - @jeylau Jonas Windhager - @jwindhager Jordão Bragantini - @JoOkuma Juan Nunez-Iglesias - @jni Jules Vanaret - @jules-vanaret Kabilar Gunalan - @kabilar Katherine Hutchings - @katherine-hutchings Kim Pevey - @kcpevey Konstantin Sofiiuk - @ksofiyuk Kyle I. S. Harrington - @kephale Lorenzo Gaifas - @brisvag Luca Marconato - @LucaMarconato Lucy Liu - @lucyleeow Mark Harfouche - @hmaarrfk Matthias Bussonnier - @Carreau Melissa Weber Mendonça - @melissawm Nadalyn Miller - @Nadalyn-CZI Pam Wadhwa - @ppwadhwa Paul Smith - @p-j-smith Peter Sobolewski - @psobolewskiPhD Sean Martin - @seankmartin Talley Lambert - @tlambert03 Wouter-Michiel Vierdag - @melonora 43 reviewers added to this release (alphabetical) Alan Lowe - @quantumjot Alister Burt - @alisterburt Andrew Sweet - @andy-sweet Ashley Anderson - @aganders3 Charlie Marsh - @charliermarsh Daniel Althviz Moré - @dalthviz David Ross - @davidpross David Stansby - @dstansby Draga Doncila Pop - @DragaDoncila Eric Perlman - @perlman Gabriel Selzer - @gselzer Genevieve Buckley - @GenevieveBuckley Gonzalo Peña-Castellanos - @goanpeca Grzegorz Bokota - @Czaki Isabela Presedo-Floyd - @isabela-pf Jaime Rodríguez-Guerra - @jaimergp Jan-Hendrik Müller - @kolibril13 Jessy Lauer - @jeylau Jordão Bragantini - @JoOkuma Juan Nunez-Iglesias - @jni Jules Vanaret - @jules-vanaret Kevin Yamauchi - @kevinyamauchi Kim Pevey - @kcpevey Kira Evans - @kne42 Konstantin Sofiiuk - @ksofiyuk Kyle I. S. Harrington - @kephale Lorenzo Gaifas - @brisvag Luca Marconato - @LucaMarconato Lucy Liu - @lucyleeow Lucy Obus - @LCObus Mark Harfouche - @hmaarrfk Matthias Bussonnier - @Carreau Melissa Weber Mendonça - @melissawm Nathan Clack - @nclack Nicholas Sofroniew - @sofroniewn Oren Amsalem - @orena1 Pam Wadhwa - @ppwadhwa Paul Smith - @p-j-smith Peter Sobolewski - @psobolewskiPhD Sean Martin - @seankmartin Talley Lambert - @tlambert03 Wouter-Michiel Vierdag - @melonora Ziyang Liu - @liu-ziyang 19 docs authors added to this release (alphabetical) Ashley Anderson - @aganders3 chili-chiu - @chili-chiu Christopher Nauroth-Kreß - @Chris-N-K Curtis Rueden - @ctrueden Daniel Althviz Moré - @dalthviz David Stansby - @dstansby Draga Doncila Pop - @DragaDoncila Grzegorz Bokota - @Czaki Jaime Rodríguez-Guerra - @jaimergp Juan Nunez-Iglesias - @jni Lorenzo Gaifas - @brisvag Lucy Liu - @lucyleeow Matthias Bussonnier - @Carreau Melissa Weber Mendonça - @melissawm Nadalyn Miller - @Nadalyn-CZI Oren Amsalem - @orena1 Peter Sobolewski - @psobolewskiPhD Sean Martin - @seankmartin Wouter-Michiel Vierdag - @melonora 20 docs reviewers added to this release (alphabetical) Alister Burt - @alisterburt Andrew Sweet - @andy-sweet Ashley Anderson - @aganders3 Christopher Nauroth-Kreß - @Chris-N-K David Stansby - @dstansby Draga Doncila Pop - @DragaDoncila Gonzalo Peña-Castellanos - @goanpeca Grzegorz Bokota - @Czaki Jaime Rodríguez-Guerra - @jaimergp Juan Nunez-Iglesias - @jni Kevin Yamauchi - @kevinyamauchi Kira Evans - @kne42 Lorenzo Gaifas - @brisvag Lucy Liu - @lucyleeow Melissa Weber Mendonça - @melissawm Nadalyn Miller - @Nadalyn-CZI Nicholas Sofroniew - @sofroniewn Peter Sobolewski - @psobolewskiPhD Sean Martin - @seankmartin Wouter-Michiel Vierdag - @melonora New Contributors There are 19 new contributors for this release: Christopher Nauroth-Kreß docs - @Chris-N-K Clément Caporal napari - @ClementCaporal Constantin Pape napari - @constantinpape Craig T. Russell napari - @ctr26 Daniel Althviz Moré docs napari - @dalthviz David Ross napari - @davidpross Gregor Lichtner napari - @glichtner Jannis Ahlers napari - @jnahlers Jessy Lauer napari - @jeylau Jules Vanaret napari - @jules-vanaret Kabilar Gunalan napari - @kabilar Katherine Hutchings napari - @katherine-hutchings Konstantin Sofiiuk napari - @ksofiyuk LucaMarconato napari - @LucaMarconato Nadalyn Miller docs napari - @Nadalyn-CZI Oren Amsalem docs - @orena1 Paul Smith napari - @p-j-smith Sean Martin docs napari - @seankmartin Wouter-Michiel Vierdag docs napari - @melonora}
}
@misc{russell2024,
	title        = {bia-binder: {A} web-native cloud compute service for the bioimage analysis community},
	shorttitle   = {bia-binder},
	author       = {Russell, Craig T. and Burel, Jean-Marie and Athar, Awais and Li, Simon and Sarkans, Ugis and Swedlow, Jason and Brazma, Alvis and Hartley, Matthew and Uhlmann, Virginie},
	year         = 2024,
	month        = 11,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2411.12662},
	url          = {http://arxiv.org/abs/2411.12662},
	urldate      = {2025-04-09},
	note         = {arXiv:2411.12662 [q-bio]},
	abstract     = {We introduce bia-binder (BioImage Archive Binder), an open-source, cloud-architectured, and web-based coding environment tailored to bioimage analysis that is freely accessible to all researchers. The service generates easy-to-use Jupyter Notebook coding environments hosted on EMBL-EBI's Embassy Cloud, which provides significant computational resources. The bia-binder architecture is free, open-source and publicly available for deployment. It features fast and direct access to images in the BioImage Archive, the Image Data Resource, and the BioStudies databases. We believe that this service can play a role in mitigating the current inequalities in access to scientific resources across academia. As bia-binder produces permanent links to compiled coding environments, we foresee the service to become widely-used within the community and enable exploratory research. bia-binder is built and deployed using helmsman and helm and released under the MIT licence. It can be accessed at binder.bioimagearchive.org and runs on any standard web browser.},
	keywords     = {Quantitative Biology - Quantitative Methods}
}
@misc{cardoso2022,
	title        = {{MONAI}: {An} open-source framework for deep learning in healthcare},
	shorttitle   = {{MONAI}},
	author       = {Cardoso, M. Jorge and Li, Wenqi and Brown, Richard and Ma, Nic and Kerfoot, Eric and Wang, Yiheng and Murrey, Benjamin and Myronenko, Andriy and Zhao, Can and Yang, Dong and Nath, Vishwesh and He, Yufan and Xu, Ziyue and Hatamizadeh, Ali and Myronenko, Andriy and Zhu, Wentao and Liu, Yun and Zheng, Mingxin and Tang, Yucheng and Yang, Isaac and Zephyr, Michael and Hashemian, Behrooz and Alle, Sachidanand and Darestani, Mohammad Zalbagi and Budd, Charlie and Modat, Marc and Vercauteren, Tom and Wang, Guotai and Li, Yiwen and Hu, Yipeng and Fu, Yunguan and Gorman, Benjamin and Johnson, Hans and Genereaux, Brad and Erdal, Barbaros S. and Gupta, Vikash and Diaz-Pinto, Andres and Dourson, Andre and Maier-Hein, Lena and Jaeger, Paul F. and Baumgartner, Michael and Kalpathy-Cramer, Jayashree and Flores, Mona and Kirby, Justin and Cooper, Lee A. D. and Roth, Holger R. and Xu, Daguang and Bericat, David and Floca, Ralf and Zhou, S. Kevin and Shuaib, Haris and Farahani, Keyvan and Maier-Hein, Klaus H. and Aylward, Stephen and Dogra, Prerna and Ourselin, Sebastien and Feng, Andrew},
	year         = 2022,
	month        = 11,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2211.02701},
	url          = {http://arxiv.org/abs/2211.02701},
	urldate      = {2025-04-09},
	note         = {arXiv:2211.02701 [cs]},
	abstract     = {Artificial Intelligence (AI) is having a tremendous impact across most areas of science. Applications of AI in healthcare have the potential to improve our ability to detect, diagnose, prognose, and intervene on human disease. For AI models to be used clinically, they need to be made safe, reproducible and robust, and the underlying software framework must be aware of the particularities (e.g. geometry, physiology, physics) of medical data being processed. This work introduces MONAI, a freely available, community-supported, and consortium-led PyTorch-based framework for deep learning in healthcare. MONAI extends PyTorch to support medical data, with a particular focus on imaging, and provide purpose-specific AI model architectures, transformations and utilities that streamline the development and deployment of medical AI models. MONAI follows best practices for software-development, providing an easy-to-use, robust, well-documented, and well-tested software framework. MONAI preserves the simple, additive, and compositional approach of its underlying PyTorch libraries. MONAI is being used by and receiving contributions from research, clinical and industrial teams from around the world, who are pursuing applications spanning nearly every aspect of healthcare.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning}
}
@misc{follain2024,
	title        = {Fast label-free live imaging reveals key roles of flow dynamics and {CD44}-{HA} interaction in cancer cell arrest on endothelial monolayers},
	author       = {Follain, Gautier and Ghimire, Sujan and Pylvänäinen, Joanna W. and Vaitkevičiūtė, Monika and Wurzinger, Diana and Guzmán, Camilo and Conway, James RW and Dibus, Michal and Oikari, Sanna and Rilla, Kirsi and Salmi, Marko and Ivaska, Johanna and Jacquemet, Guillaume},
	year         = 2024,
	month        = 10,
	publisher    = {bioRxiv},
	doi          = {10.1101/2024.09.30.615654},
	url          = {https://www.biorxiv.org/content/10.1101/2024.09.30.615654v1},
	urldate      = {2025-03-21},
	copyright    = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	note         = {Pages: 2024.09.30.615654 Section: New Results},
	abstract     = {Leukocyte extravasation is a key component of the innate immune response, while circulating tumor cell extravasation is a critical step in metastasis formation. Despite their importance, the mechanisms underlying leukocyte and tumor cell extravasation remain incompletely understood. Here, we developed an imaging pipeline that integrates fast, label-free live-cell imaging with deep learning-based image analysis to quantitatively track and compare the initial steps of extravasation—cell landing and arrest on an endothelial monolayer—under physiological flow conditions. We find that pancreatic ductal adenocarcinoma (PDAC) cells exhibit variable adhesion strength and flow sensitivity. Remarkably, some PDAC cells demonstrate comparable endothelial engagement as leukocytes, preferentially arresting at endothelial junctions, potentially due to increased stiffness at these sites, which leads to exposure to the underlying basal extracellular matrix. PDAC cells also tend to cluster in regions with high, heterogeneous expression of the endothelial CD44 receptor. Simulations suggest that clustering results from the combination of CD44-mediated attachment and localized flow disturbances that facilitate subsequent cell attachment. Targeting CD44 using siRNA or function-blocking antibodies, or degrading its ligand hyaluronic acid (HA), almost completely abolishes PDAC cell attachment. Overall, we demonstrate that cancer and immune cells share both common and unique features in endothelial adhesion under flow, and identify CD44 and HA as key mediators of PDAC cell arrest.},
	language     = {en}
}
@article{moen2019,
	title        = {Deep learning for cellular image analysis},
	author       = {Moen, Erick and Bannon, Dylan and Kudo, Takamasa and Graf, William and Covert, Markus and Van Valen, David},
	year         = 2019,
	month        = 12,
	journal      = {Nature Methods},
	volume       = 16,
	number       = 12,
	pages        = {1233--1246},
	doi          = {10.1038/s41592-019-0403-1},
	issn         = {1548-7105},
	url          = {https://www.nature.com/articles/s41592-019-0403-1},
	urldate      = {2023-11-30},
	copyright    = {2019 Springer Nature America, Inc.},
	note         = {Number: 12 Publisher: Nature Publishing Group},
	abstract     = {Recent advances in computer vision and machine learning underpin a collection of algorithms with an impressive ability to decipher the content of images. These deep learning algorithms are being applied to biological images and are transforming the analysis and interpretation of imaging data. These advances are positioned to render difficult analyses routine and to enable researchers to carry out new, previously impossible experiments. Here we review the intersection between deep learning and cellular image analysis and provide an overview of both the mathematical mechanics and the programming frameworks of deep learning that are pertinent to life scientists. We survey the field’s progress in four key applications: image classification, image segmentation, object tracking, and augmented microscopy. Last, we relay our labs’ experience with three key aspects of implementing deep learning in the laboratory: annotating training data, selecting and training a range of neural network architectures, and deploying solutions. We also highlight existing datasets and implementations for each surveyed application.},
	language     = {en},
	keywords     = {Image processing, Software}
}
@article{pylvanainen2023,
	title        = {Live-cell imaging in the deep learning era},
	author       = {Pylvänäinen, Joanna W. and Gómez-de-Mariscal, Estibaliz and Henriques, Ricardo and Jacquemet, Guillaume},
	year         = 2023,
	month        = 12,
	journal      = {Current Opinion in Cell Biology},
	volume       = 85,
	pages        = 102271,
	doi          = {10.1016/j.ceb.2023.102271},
	issn         = {09550674},
	url          = {https://linkinghub.elsevier.com/retrieve/pii/S0955067423001205},
	urldate      = {2023-11-07},
	language     = {en}
}
@article{bannon2021,
	title        = {{DeepCell} {Kiosk}: scaling deep learning–enabled cellular image analysis with {Kubernetes}},
	shorttitle   = {{DeepCell} {Kiosk}},
	author       = {Bannon, Dylan and Moen, Erick and Schwartz, Morgan and Borba, Enrico and Kudo, Takamasa and Greenwald, Noah and Vijayakumar, Vibha and Chang, Brian and Pao, Edward and Osterman, Erik and Graf, William and Van Valen, David},
	year         = 2021,
	month        = 1,
	journal      = {Nature Methods},
	volume       = 18,
	number       = 1,
	pages        = {43--45},
	doi          = {10.1038/s41592-020-01023-0},
	issn         = {1548-7105},
	url          = {https://www.nature.com/articles/s41592-020-01023-0},
	urldate      = {2023-10-25},
	copyright    = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	note         = {Number: 1 Publisher: Nature Publishing Group},
	abstract     = {Deep learning is transforming the analysis of biological images, but applying these models to large datasets remains challenging. Here we describe the DeepCell Kiosk, cloud-native software that dynamically scales deep learning workflows to accommodate large imaging datasets. To demonstrate the scalability and affordability of this software, we identified cell nuclei in 106 1-megapixel images in {\textasciitilde}5.5 h for {\textasciitilde}US\$250, with a cost below US\$100 achievable depending on cluster configuration. The DeepCell Kiosk can be downloaded at https://github.com/vanvalenlab/kiosk-console; a persistent deployment is available at https://deepcell.org/.},
	language     = {en},
	keywords     = {Hardware and infrastructure, Image processing}
}
@article{laine2021,
	title        = {Avoiding a replication crisis in deep-learning-based bioimage analysis},
	author       = {Laine, Romain F. and Arganda-Carreras, Ignacio and Henriques, Ricardo and Jacquemet, Guillaume},
	year         = 2021,
	month        = 10,
	journal      = {Nature methods},
	volume       = 18,
	number       = 10,
	pages        = {1136--1144},
	doi          = {10.1038/s41592-021-01284-3},
	issn         = {1548-7091},
	url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7611896/},
	urldate      = {2023-10-06},
	abstract     = {Deep learning algorithms are powerful tools to analyse, restore and transform bioimaging data, increasingly used in life sciences research. These approaches now outperform most other algorithms for a broad range of image analysis tasks. In particular, one of the promises of deep learning is the possibility to provide parameter-free, one-click data analysis achieving expert-level performances in a fraction of the time previously required. However, as with most new and upcoming technologies, the potential for inappropriate use is raising concerns among the biomedical research community. This perspective aims to provide a short overview of key concepts that we believe are important for researchers to consider when using deep learning for their microscopy studies. These comments are based on our own experience gained while optimising various deep learning tools for bioimage analysis and discussions with colleagues from both the developer and user community. In particular, we focus on describing how results obtained using deep learning can be validated and discuss what should, in our views, be considered when choosing a suitable tool. We also suggest what aspects of a deep learning analysis would need to be reported in publications to describe the use of such tools to guarantee that the work can be reproduced. We hope this perspective will foster further discussion between developers, image analysis specialists, users and journal editors to define adequate guidelines and ensure that this transformative technology is used appropriately.},
	pmid         = 34608322,
	pmcid        = {PMC7611896}
}
@article{heinrich2021,
	title        = {Whole-cell organelle segmentation in volume electron microscopy},
	author       = {Heinrich, Larissa and Bennett, Davis and Ackerman, David and Park, Woohyun and Bogovic, John and Eckstein, Nils and Petruncio, Alyson and Clements, Jody and Pang, Song and Xu, C. Shan and Funke, Jan and Korff, Wyatt and Hess, Harald F. and Lippincott-Schwartz, Jennifer and Saalfeld, Stephan and Weigel, Aubrey V.},
	year         = 2021,
	month        = 11,
	journal      = {Nature},
	volume       = 599,
	number       = 7883,
	pages        = {141--146},
	doi          = {10.1038/s41586-021-03977-3},
	issn         = {1476-4687},
	url          = {https://www.nature.com/articles/s41586-021-03977-3},
	urldate      = {2025-04-17},
	copyright    = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	note         = {Publisher: Nature Publishing Group},
	abstract     = {Cells contain hundreds of organelles and macromolecular assemblies. Obtaining a complete understanding of their intricate organization requires the nanometre-level, three-dimensional reconstruction of whole cells, which is only feasible with robust and scalable automatic methods. Here, to support the development of such methods, we annotated up to 35 different cellular organelle classes—ranging from endoplasmic reticulum to microtubules to ribosomes—in diverse sample volumes from multiple cell types imaged at a near-isotropic resolution of 4 nm per voxel with focused ion beam scanning electron microscopy (FIB-SEM)1. We trained deep learning architectures to segment these structures in 4 nm and 8 nm per voxel FIB-SEM volumes, validated their performance and showed that automatic reconstructions can be used to directly quantify previously inaccessible metrics including spatial interactions between cellular components. We also show that such reconstructions can be used to automatically register light and electron microscopy images for correlative studies. We have created an open data and open-source web repository, ‘OpenOrganelle’, to share the data, computer code and trained models, which will enable scientists everywhere to query and further improve automatic reconstruction of these datasets.},
	language     = {en},
	keywords     = {Cellular imaging, Data mining, Image processing, Machine learning, Organelles}
}
@article{liu2025,
	title        = {Self-supervised learning reveals clinically relevant histomorphological patterns for therapeutic strategies in colon cancer},
	author       = {Liu, Bojing and Polack, Meaghan and Coudray, Nicolas and Claudio Quiros, Adalberto and Sakellaropoulos, Theodore and Le, Hortense and Karimkhan, Afreen and Crobach, Augustinus S. L. P. and van Krieken, J. Han J. M. and Yuan, Ke and Tollenaar, Rob A. E. M. and Mesker, Wilma E. and Tsirigos, Aristotelis},
	year         = 2025,
	month        = 3,
	journal      = {Nature Communications},
	volume       = 16,
	number       = 1,
	pages        = 2328,
	doi          = {10.1038/s41467-025-57541-y},
	issn         = {2041-1723},
	url          = {https://www.nature.com/articles/s41467-025-57541-y},
	urldate      = {2025-04-17},
	copyright    = {2025 The Author(s)},
	note         = {Publisher: Nature Publishing Group},
	abstract     = {Self-supervised learning (SSL) automates the extraction and interpretation of histopathology features on unannotated hematoxylin-eosin-stained whole slide images (WSIs). We train an SSL Barlow Twins encoder on 435 colon adenocarcinoma WSIs from The Cancer Genome Atlas to extract features from small image patches (tiles). Leiden community detection groups tiles into histomorphological phenotype clusters (HPCs). HPC reproducibility and predictive ability for overall survival are confirmed in an independent clinical trial (N = 1213 WSIs). This unbiased atlas results in 47 HPCs displaying unique and shared clinically significant histomorphological traits, highlighting tissue type, quantity, and architecture, especially in the context of tumor stroma. Through in-depth analyses of these HPCs, including immune landscape and gene set enrichment analyses, and associations to clinical outcomes, we shine light on the factors influencing survival and responses to treatments of standard adjuvant chemotherapy and experimental therapies. Further exploration of HPCs may unveil additional insights and aid decision-making and personalized treatments for colon cancer patients.},
	language     = {en},
	keywords     = {Cancer imaging, Image processing, Machine learning}
}
@article{moshkov2024,
	title        = {Learning representations for image-based profiling of perturbations},
	author       = {Moshkov, Nikita and Bornholdt, Michael and Benoit, Santiago and Smith, Matthew and McQuin, Claire and Goodman, Allen and Senft, Rebecca A. and Han, Yu and Babadi, Mehrtash and Horvath, Peter and Cimini, Beth A. and Carpenter, Anne E. and Singh, Shantanu and Caicedo, Juan C.},
	year         = 2024,
	month        = 2,
	journal      = {Nature Communications},
	volume       = 15,
	number       = 1,
	pages        = 1594,
	doi          = {10.1038/s41467-024-45999-1},
	issn         = {2041-1723},
	url          = {https://www.nature.com/articles/s41467-024-45999-1},
	urldate      = {2025-04-17},
	copyright    = {2024 The Author(s)},
	note         = {Publisher: Nature Publishing Group},
	abstract     = {Measuring the phenotypic effect of treatments on cells through imaging assays is an efficient and powerful way of studying cell biology, and requires computational methods for transforming images into quantitative data. Here, we present an improved strategy for learning representations of treatment effects from high-throughput imaging, following a causal interpretation. We use weakly supervised learning for modeling associations between images and treatments, and show that it encodes both confounding factors and phenotypic features in the learned representation. To facilitate their separation, we constructed a large training dataset with images from five different studies to maximize experimental diversity, following insights from our causal analysis. Training a model with this dataset successfully improves downstream performance, and produces a reusable convolutional network for image-based profiling, which we call Cell Painting CNN. We evaluated our strategy on three publicly available Cell Painting datasets, and observed that the Cell Painting CNN improves performance in downstream analysis up to 30\% with respect to classical features, while also being more computationally efficient.},
	language     = {en},
	keywords     = {Image processing, Machine learning, Phenotypic screening}
}
@inproceedings{caicedo2018,
	title        = {Weakly {Supervised} {Learning} of {Single}-{Cell} {Feature} {Embeddings}},
	author       = {Caicedo, Juan C. and McQuin, Claire and Goodman, Allen and Singh, Shantanu and Carpenter, Anne E.},
	year         = 2018,
	month        = 6,
	booktitle    = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	pages        = {9309--9318},
	doi          = {10.1109/CVPR.2018.00970},
	url          = {https://ieeexplore.ieee.org/document/8579068},
	urldate      = {2025-04-17},
	note         = {ISSN: 2575-7075},
	abstract     = {We study the problem of learning representations for single cells in microscopy images to discover biological relationships between their experimental conditions. Many new applications in drug discovery and functional genomics require capturing the morphology of individual cells as comprehensively as possible. Deep convolutional neural networks (CNNs) can learn powerful visual representations, but require ground truth for training; this is rarely available in biomedical profiling experiments. While we do not know which experimental treatments produce cells that look alike, we do know that cells exposed to the same experimental treatment should generally look similar. Thus, we explore training CNNs using a weakly supervised approach that uses this information for feature learning. In addition, the training stage is regularized to control for unwanted variations using mixup or RNNs. We conduct experiments on two different datasets; the proposed approach yields single-cell embeddings that are more accurate than the widely adopted classical features, and are competitive with previously proposed transfer learning approaches.},
	keywords     = {Biology, Compounds, Feature extraction, Microscopy, Sociology, Statistics, Training}
}
@article{li2018,
	title        = {{cC}-{GAN}: {A} {Robust} {Transfer}-{Learning} {Framework} for {HEp}-2 {Specimen} {Image} {Segmentation}},
	shorttitle   = {{cC}-{GAN}},
	author       = {Li, Yuexiang and Shen, Linlin},
	year         = 2018,
	journal      = {IEEE Access},
	volume       = 6,
	pages        = {14048--14058},
	doi          = {10.1109/ACCESS.2018.2808938},
	issn         = {2169-3536},
	url          = {https://ieeexplore.ieee.org/document/8301400},
	urldate      = {2025-04-17},
	abstract     = {Human epithelial type 2 (HEp-2) cell images play an important role for the detection of antinuclear autoantibodies in autoimmune diseases. As the HEp-2 cell has hundreds of different patterns, none of currently available HEp-2 datasets contain all of the types. Therefore, existing automatic processing systems for HEp-2 cells, e.g., cell segmentation and classification, needs to be transferred between different data sets. However, the performances of transferred system often dramatically decrease, especially when transferring supervised-approaches, e.g., deep learning network, from large dataset to the small but similar ones. In this paper, a novel transfer-learning framework using generative adversarial networks (cC-GAN) is proposed for robust segmentation of different HEp-2 datasets. The proposed cC-GAN tries to solve the overfitting problem of most deep learning networks and improves their transfer-capacity. An improved U-net, so-called Residual U-net (RU-net), is developed to work as the generator for cC-GAN model. The cC-GAN was first trained and tested using I3A dataset and then directly evaluated using MIVIA dataset, which is much smaller than I3A. The segmentation result demonstrates the excellent transferring-capacity of our cC-GAN framework, i.e., a new state-of-the-art segmentation accuracy of 75.27\% was achieved on MIVIA without finetuning.},
	keywords     = {Cell segmentation, Computer architecture, fully convolutional network, Gallium nitride, generative adversarial networks, Generators, Image segmentation, Machine learning, Microprocessors, Training}
}
@article{morid2021,
	title        = {A scoping review of transfer learning research on medical image analysis using {ImageNet}},
	author       = {Morid, Mohammad Amin and Borjali, Alireza and Del Fiol, Guilherme},
	year         = 2021,
	month        = 1,
	journal      = {Computers in Biology and Medicine},
	volume       = 128,
	pages        = 104115,
	doi          = {10.1016/j.compbiomed.2020.104115},
	issn         = {0010-4825},
	url          = {https://www.sciencedirect.com/science/article/pii/S0010482520304467},
	urldate      = {2025-04-17},
	abstract     = {Objective Employing transfer learning (TL) with convolutional neural networks (CNNs), well-trained on non-medical ImageNet dataset, has shown promising results for medical image analysis in recent years. We aimed to conduct a scoping review to identify these studies and summarize their characteristics in terms of the problem description, input, methodology, and outcome. Materials and methods To identify relevant studies, MEDLINE, IEEE, and ACM digital library were searched for studies published between June 1st, 2012 and January 2nd, 2020. Two investigators independently reviewed articles to determine eligibility and to extract data according to a study protocol defined a priori. Results After screening of 8421 articles, 102 met the inclusion criteria. Of 22 anatomical areas, eye (18\%), breast (14\%), and brain (12\%) were the most commonly studied. Data augmentation was performed in 72\% of fine-tuning TL studies versus 15\% of the feature-extracting TL studies. Inception models were the most commonly used in breast related studies (50\%), while VGGNet was the common in eye (44\%), skin (50\%) and tooth (57\%) studies. AlexNet for brain (42\%) and DenseNet for lung studies (38\%) were the most frequently used models. Inception models were the most frequently used for studies that analyzed ultrasound (55\%), endoscopy (57\%), and skeletal system X-rays (57\%). VGGNet was the most common for fundus (42\%) and optical coherence tomography images (50\%). AlexNet was the most frequent model for brain MRIs (36\%) and breast X-Rays (50\%). 35\% of the studies compared their model with other well-trained CNN models and 33\% of them provided visualization for interpretation. Discussion This study identified the most prevalent tracks of implementation in the literature for data preparation, methodology selection and output evaluation for various medical image analysis tasks. Also, we identified several critical research gaps existing in the TL studies on medical image analysis. The findings of this scoping review can be used in future TL studies to guide the selection of appropriate research approaches, as well as identify research gaps and opportunities for innovation.},
	keywords     = {Convolutional neural network, ImageNet, Medical imaging, Transfer learning}
}
@article{kochetov2024,
	title        = {{UNSEG}: unsupervised segmentation of cells and their nuclei in complex tissue samples},
	shorttitle   = {{UNSEG}},
	author       = {Kochetov, Bogdan and Bell, Phoenix D. and Garcia, Paulo S. and Shalaby, Akram S. and Raphael, Rebecca and Raymond, Benjamin and Leibowitz, Brian J. and Schoedel, Karen and Brand, Rhonda M. and Brand, Randall E. and Yu, Jian and Zhang, Lin and Diergaarde, Brenda and Schoen, Robert E. and Singhi, Aatur and Uttam, Shikhar},
	year         = 2024,
	month        = 8,
	journal      = {Communications Biology},
	volume       = 7,
	number       = 1,
	pages        = {1--14},
	doi          = {10.1038/s42003-024-06714-4},
	issn         = {2399-3642},
	url          = {https://www.nature.com/articles/s42003-024-06714-4},
	urldate      = {2025-04-22},
	copyright    = {2024 The Author(s)},
	note         = {Publisher: Nature Publishing Group},
	abstract     = {Multiplexed imaging technologies have made it possible to interrogate complex tissue microenvironments at sub-cellular resolution within their native spatial context. However, proper quantification of this complexity requires the ability to easily and accurately segment cells into their sub-cellular compartments. Within the supervised learning paradigm, deep learning-based segmentation methods demonstrating human level performance have emerged. However, limited work has been done in developing such generalist methods within the unsupervised context. Here we present an easy-to-use unsupervised segmentation (UNSEG) method that achieves deep learning level performance without requiring any training data via leveraging a Bayesian-like framework, and nucleus and cell membrane markers. We show that UNSEG is internally consistent and better at generalizing to the complexity of tissue morphology than current deep learning methods, allowing it to unambiguously identify the cytoplasmic compartment of a cell, and localize molecules to their correct sub-cellular compartment. We also introduce a perturbed watershed algorithm for stably and automatically segmenting a cluster of cell nuclei into individual nuclei that increases the accuracy of classical watershed. Finally, we demonstrate the efficacy of UNSEG on a high-quality annotated gastrointestinal tissue dataset we have generated, on publicly available datasets, and in a range of practical scenarios.},
	language     = {en},
	keywords     = {Computer science, Gastrointestinal system, Image processing}
}
@misc{chen2020,
	title        = {The {Allen} {Cell} and {Structure} {Segmenter}: a new open source toolkit for segmenting {3D} intracellular structures in fluorescence microscopy images},
	shorttitle   = {The {Allen} {Cell} and {Structure} {Segmenter}},
	author       = {Chen, Jianxu and Ding, Liya and Viana, Matheus P. and Lee, HyeonWoo and Sluezwski, M. Filip and Morris, Benjamin and Hendershott, Melissa C. and Yang, Ruian and Mueller, Irina A. and Rafelski, Susanne M.},
	year         = 2020,
	month        = 12,
	publisher    = {bioRxiv},
	doi          = {10.1101/491035},
	url          = {https://www.biorxiv.org/content/10.1101/491035v2},
	urldate      = {2025-04-22},
	copyright    = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	note         = {Pages: 491035 Section: New Results},
	abstract     = {A continuing challenge in quantitative cell biology is the accurate and robust 3D segmentation of structures of interest from fluorescence microscopy images in an automated, reproducible, and widely accessible manner for subsequent interpretable data analysis. We describe the Allen Cell and Structure Segmenter (Segmenter), a Python-based open source toolkit developed for 3D segmentation of cells and intracellular structures in fluorescence microscope images. This toolkit brings together classic image segmentation and iterative deep learning workflows first to generate initial high-quality 3D intracellular structure segmentations and then to easily curate these results to generate the ground truths for building robust and accurate deep learning models. The toolkit takes advantage of the high-replicate 3D live cell image data collected at the Allen Institute for Cell Science of over 30 endogenous fluorescently tagged human induced pluripotent stem cell (hiPSC) lines. Each cell line represents a different intracellular structure with one or more distinct localization patterns within undifferentiated hiPS cells and hiPSC-derived cardiomyocytes. The Segmenter consists of two complementary elements, a classic image segmentation workflow with a restricted set of algorithms and parameters and an iterative deep learning segmentation workflow. We created a collection of 20 classic image segmentation workflows based on 20 distinct and representative intracellular structure localization patterns as a “lookup table” reference and starting point for users. The iterative deep learning workflow can take over when the classic segmentation workflow is insufficient. Two straightforward “human-in-the-loop” curation strategies convert a set of classic image segmentation workflow results into a set of 3D ground truth images for iterative model training without the need for manual painting in 3D. The deep learning model architectures used in this toolkit were designed and tested specifically for 3D fluorescence microscope images and implemented as readable scripts. The Segmenter thus leverages state of the art computer vision algorithms in an accessible way to facilitate their application by the experimental biology researcher. We include two useful applications to demonstrate how we used the classic image segmentation and iterative deep learning workflows to solve more challenging 3D segmentation tasks. First, we introduce the ‘Training Assay’ approach, a new experimental-computational co-design concept to generate more biologically accurate segmentation ground truths. We combined the iterative deep learning workflow with three Training Assays to develop a robust, scalable cell and nuclear instance segmentation algorithm, which could achieve accurate target segmentation for over 98\% of individual cells and over 80\% of entire fields of view. Second, we demonstrate how to extend the lamin B1 segmentation model built from the iterative deep learning workflow to obtain more biologically accurate lamin B1 segmentation by utilizing multi-channel inputs and combining multiple ML models. The steps and workflows used to develop these algorithms are generalizable to other similar segmentation challenges. More information, including tutorials and code repositories, are available at allencell.org/segmenter.},
	language     = {en}
}
@article{conrad2023,
	title        = {Instance segmentation of mitochondria in electron microscopy images with a generalist deep learning model trained on a diverse dataset},
	author       = {Conrad, Ryan and Narayan, Kedar},
	year         = 2023,
	month        = 1,
	journal      = {Cell Systems},
	volume       = 14,
	number       = 1,
	pages        = {58--71.e5},
	doi          = {10.1016/j.cels.2022.12.006},
	issn         = {2405-4712},
	url          = {https://www.sciencedirect.com/science/article/pii/S240547122200494X},
	urldate      = {2025-04-22},
	abstract     = {Mitochondria are extremely pleomorphic organelles. Automatically annotating each one accurately and precisely in any 2D or volume electron microscopy (EM) image is an unsolved computational challenge. Current deep learning-based approaches train models on images that provide limited cellular contexts, precluding generality. To address this, we amassed a highly heterogeneous ∼1.5 × 106 image 2D unlabeled cellular EM dataset and segmented ∼135,000 mitochondrial instances therein. MitoNet, a model trained on these resources, performs well on challenging benchmarks and on previously unseen volume EM datasets containing tens of thousands of mitochondria. We release a Python package and napari plugin, empanada, to rapidly run inference, visualize, and proofread instance segmentations. A record of this paper’s transparent peer review process is included in the supplemental information.},
	keywords     = {benchmark, crowdsourcing, deep learning, electron microscopy, image dataset, mitochondria, panoptic, segmentation, volume electron miscroscopy, volume EM}
}
@article{fisch2024,
	title        = {Molecular definition of the endogenous {Toll}-like receptor signalling pathways},
	author       = {Fisch, Daniel and Zhang, Tian and Sun, He and Ma, Weiyi and Tan, Yunhao and Gygi, Steven P. and Higgins, Darren E. and Kagan, Jonathan C.},
	year         = 2024,
	month        = 7,
	journal      = {Nature},
	volume       = 631,
	number       = 8021,
	pages        = {635--644},
	doi          = {10.1038/s41586-024-07614-7},
	issn         = {1476-4687},
	url          = {https://www.nature.com/articles/s41586-024-07614-7},
	urldate      = {2025-04-22},
	copyright    = {2024 The Author(s), under exclusive licence to Springer Nature Limited},
	note         = {Publisher: Nature Publishing Group},
	abstract     = {Innate immune pattern recognition receptors, such as the Toll-like receptors (TLRs), are key mediators of the immune response to infection and central to our understanding of health and disease1. After microbial detection, these receptors activate inflammatory signal transduction pathways that involve IκB kinases, mitogen-activated protein kinases, ubiquitin ligases and other adaptor proteins. The mechanisms that connect the proteins in the TLR pathways are poorly defined. To delineate TLR pathway activities, we engineered macrophages to enable microscopy and proteomic analysis of the endogenous myddosome constituent MyD88. We found that myddosomes form transient contacts with activated TLRs and that TLR-free myddosomes are dynamic in size, number and composition over the course of 24 h. Analysis using super-resolution microscopy revealed that, within most myddosomes, MyD88 forms barrel-like structures that function as scaffolds for effector protein recruitment. Proteomic analysis demonstrated that myddosomes contain proteins that act at all stages and regulate all effector responses of the TLR pathways, and genetic analysis defined the epistatic relationship between these effector modules. Myddosome assembly was evident in cells infected with Listeria monocytogenes, but these bacteria evaded myddosome assembly and TLR signalling during cell-to-cell spread. On the basis of these findings, we propose that the entire TLR signalling pathway is executed from within the myddosome.},
	language     = {en},
	keywords     = {Innate immune cells, Innate immunity}
}
@article{bejarano2024,
	title        = {Interrogation of endothelial and mural cells in brain metastasis reveals key immune-regulatory mechanisms},
	author       = {Bejarano, Leire and Kauzlaric, Annamaria and Lamprou, Eleni and Lourenco, Joao and Fournier, Nadine and Ballabio, Michelle and Colotti, Roberto and Maas, Roeltje and Galland, Sabine and Massara, Matteo and Soukup, Klara and Lilja, Johanna and Brouland, Jean-Philippe and Hottinger, Andreas F. and Daniel, Roy T. and Hegi, Monika E. and Joyce, Johanna A.},
	year         = 2024,
	month        = 3,
	journal      = {Cancer Cell},
	volume       = 42,
	number       = 3,
	pages        = {378--395.e10},
	doi          = {10.1016/j.ccell.2023.12.018},
	issn         = {1535-6108},
	url          = {https://www.sciencedirect.com/science/article/pii/S1535610823004464},
	urldate      = {2025-04-22},
	abstract     = {Brain metastasis (BrM) is a common malignancy, predominantly originating from lung, melanoma, and breast cancers. The vasculature is a key component of the BrM tumor microenvironment with critical roles in regulating metastatic seeding and progression. However, the heterogeneity of the major BrM vascular components, namely endothelial and mural cells, is still poorly understood. We perform single-cell and bulk RNA-sequencing of sorted vascular cell types and detect multiple subtypes enriched specifically in BrM compared to non-tumor brain, including previously unrecognized immune regulatory subtypes. We integrate the human data with mouse models, creating a platform to interrogate vascular targets for the treatment of BrM. We find that the CD276 immune checkpoint molecule is significantly upregulated in the BrM vasculature, and anti-CD276 blocking antibodies prolonged survival in preclinical trials. This study provides important insights into the complex interactions between the vasculature, immune cells, and cancer cells, with translational relevance for designing therapeutic interventions.},
	keywords     = {Brain metastasis, endothelial cells, immune regulation, mural cells, single-cell, vasculature}
}
@article{rangel_dacosta2024,
	title        = {A robust synthetic data generation framework for machine learning in high-resolution transmission electron microscopy ({HRTEM})},
	author       = {Rangel DaCosta, Luis and Sytwu, Katherine and Groschner, C. K. and Scott, M. C.},
	year         = 2024,
	month        = 7,
	journal      = {npj Computational Materials},
	volume       = 10,
	number       = 1,
	pages        = {1--11},
	doi          = {10.1038/s41524-024-01336-0},
	issn         = {2057-3960},
	url          = {https://www.nature.com/articles/s41524-024-01336-0},
	urldate      = {2025-04-22},
	copyright    = {2024 The Author(s)},
	note         = {Publisher: Nature Publishing Group},
	abstract     = {Machine learning techniques are attractive options for developing highly-accurate analysis tools for nanomaterials characterization, including high-resolution transmission electron microscopy (HRTEM). However, successfully implementing such machine learning tools can be difficult due to the challenges in procuring sufficiently large, high-quality training datasets from experiments. In this work, we introduce Construction Zone, a Python package for rapid generation of complex nanoscale atomic structures which enables fast, systematic sampling of realistic nanomaterial structures and can be used as a random structure generator for large, diverse synthetic datasets. Using Construction Zone, we develop an end-to-end machine learning workflow for training neural network models to analyze experimental atomic resolution HRTEM images on the task of nanoparticle image segmentation purely with simulated databases. Further, we study the data curation process to understand how various aspects of the curated simulated data—including simulation fidelity, the distribution of atomic structures, and the distribution of imaging conditions—affect model performance across three benchmark experimental HRTEM image datasets. Using our workflow, we are able to achieve state-of-the-art segmentation performance on these experimental benchmarks and, further, we discuss robust strategies for consistently achieving high performance with machine learning in experimental settings using purely synthetic data. Construction Zone and its documentation are available at https://github.com/lerandc/construction\_zone.},
	language     = {en},
	keywords     = {Atomistic models, Nanoparticles, Transmission electron microscopy}
}
@article{lin2022,
	title        = {A deep learned nanowire segmentation model using synthetic data augmentation},
	author       = {Lin, Binbin and Emami, Nima and Santos, David A. and Luo, Yuting and Banerjee, Sarbajit and Xu, Bai-Xiang},
	year         = 2022,
	month        = 4,
	journal      = {npj Computational Materials},
	volume       = 8,
	number       = 1,
	pages        = {1--12},
	doi          = {10.1038/s41524-022-00767-x},
	issn         = {2057-3960},
	url          = {https://www.nature.com/articles/s41524-022-00767-x},
	urldate      = {2025-04-22},
	copyright    = {2022 The Author(s)},
	note         = {Publisher: Nature Publishing Group},
	abstract     = {Automated particle segmentation and feature analysis of experimental image data are indispensable for data-driven material science. Deep learning-based image segmentation algorithms are promising techniques to achieve this goal but are challenging to use due to the acquisition of a large number of training images. In the present work, synthetic images are applied, resembling the experimental images in terms of geometrical and visual features, to train the state-of-art Mask region-based convolutional neural networks to segment vanadium pentoxide nanowires, a cathode material within optical density-based images acquired using spectromicroscopy. The results demonstrate the instance segmentation power in real optical intensity-based spectromicroscopy images of complex nanowires in overlapped networks and provide reliable statistical information. The model can further be used to segment nanowires in scanning electron microscopy images, which are fundamentally different from the training dataset known to the model. The proposed methodology can be extended to any optical intensity-based images of variable particle morphology, material class, and beyond.},
	language     = {en},
	keywords     = {Characterization and analytical techniques, Imaging techniques, Optical spectroscopy}
}
@article{shorten2019,
	title        = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	author       = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	year         = 2019,
	month        = 7,
	journal      = {Journal of Big Data},
	volume       = 6,
	number       = 1,
	pages        = 60,
	doi          = {10.1186/s40537-019-0197-0},
	issn         = {2196-1115},
	url          = {https://doi.org/10.1186/s40537-019-0197-0},
	urldate      = {2025-04-22},
	abstract     = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	keywords     = {Big data, Data Augmentation, Deep Learning, GANs, Image data}
}
@article{lecun1998,
	title        = {Gradient-based learning applied to document recognition},
	author       = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	year         = 1998,
	month        = 11,
	journal      = {Proceedings of the IEEE},
	volume       = 86,
	number       = 11,
	pages        = {2278--2324},
	doi          = {10.1109/5.726791},
	issn         = {1558-2256},
	url          = {https://ieeexplore.ieee.org/document/726791},
	urldate      = {2025-04-22},
	abstract     = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	keywords     = {Character recognition, Feature extraction, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis}
}
@inproceedings{alibrahim2021,
	title        = {Hyperparameter Optimization: Comparing Genetic Algorithm against Grid Search and Bayesian Optimization},
	author       = {Alibrahim, Hussain and Ludwig, Simone A.},
	year         = 2021,
	booktitle    = {2021 IEEE Congress on Evolutionary Computation (CEC)},
	pages        = {1551--1559},
	doi          = {10.1109/CEC45853.2021.9504761},
	url			 = {https://ieeexplore.ieee.org/document/9504761},
	keywords     = {Training;Machine learning algorithms;Neural networks;Prediction algorithms;Search problems;Time measurement;Bayes methods;Hyperparmeter optimization;Grid Search;Bayesian;Genetic Algorithm}
}
@article{ilievski2017,
	title        = {Efficient Hyperparameter Optimization for Deep Learning Algorithms Using Deterministic RBF Surrogates},
	author       = {Ilievski, Ilija and Akhtar, Taimoor and Feng, Jiashi and Shoemaker, Christine},
	year         = 2017,
	month        = 2,
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 31,
	number       = 1,
	doi          = {10.1609/aaai.v31i1.10647},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/10647},
	abstractnote = {&lt;p&gt; Automatically searching for optimal hyperparameter configurations is of crucial importance for applying deep learning algorithms in practice. Recently, Bayesian optimization has been proposed for optimizing hyperparameters of various machine learning algorithms. Those methods adopt probabilistic surrogate models like Gaussian processes to approximate and minimize the validation error function of hyperparameter values. However, probabilistic surrogates require accurate estimates of sufficient statistics (e.g., covariance) of the error distribution and thus need many function evaluations with a sizeable number of hyperparameters. This makes them inefficient for optimizing hyperparameters of deep learning algorithms, which are highly expensive to evaluate. In this work, we propose a new deterministic and efficient hyperparameter optimization method that employs radial basis functions as error surrogates. The proposed mixed integer algorithm, called HORD, searches the surrogate for the most promising hyperparameter values through dynamic coordinate search and requires many fewer function evaluations. HORD does well in low dimensions but it is exceptionally better in higher dimensions. Extensive evaluations on MNIST and CIFAR-10 for four deep neural networks demonstrate HORD significantly outperforms the well-established Bayesian optimization methods such as GP, SMAC, and TPE. For instance, on average, HORD is more than 6 times faster than GP-EI in obtaining the best configuration of 19 hyperparameters. &lt;/p&gt;}
}
