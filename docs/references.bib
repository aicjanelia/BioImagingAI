@article{stringer2021,
	title = {Cellpose: a generalist algorithm for cellular segmentation},
	volume = {18},
	issn = {1548-7105},
	shorttitle = {Cellpose},
	url = {https://www.nature.com/articles/s41592-020-01018-x},
	doi = {10.1038/s41592-020-01018-x},
	language = {en},
	number = {1},
	urldate = {2024-06-03},
	journal = {Nature Methods},
	author = {Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},
	month = jan,
	year = {2021},
  publisher = {Nature Publishing Group},
	pages = {100--106}
  }

@article{von_chamier2021,
	title = {Democratising deep learning for microscopy with {ZeroCostDL4Mic}},
	volume = {12},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-22518-0},
	doi = {10.1038/s41467-021-22518-0},
	language = {en},
	number = {1},
	urldate = {2022-06-24},
	journal = {Nature Communications},
	author = {von Chamier, Lucas and Laine, Romain F. and Jukkala, Johanna and Spahn, Christoph and Krentzel, Daniel and Nehme, Elias and Lerche, Martina and Hernández-Pérez, Sara and Mattila, Pieta K. and Karinou, Eleni and Holden, Séamus and Solak, Ahmet Can and Krull, Alexander and Buchholz, Tim-Oliver and Jones, Martin L. and Royer, Loïc A. and Leterrier, Christophe and Shechtman, Yoav and Jug, Florian and Heilemann, Mike and Jacquemet, Guillaume and Henriques, Ricardo},
	month = apr,
	year = {2021},
  ublisher = {Nature Publishing Group},
	pages = {2276},
}

@article{guo_deep_2025,
	title = {Deep learning-based aberration compensation improves contrast and resolution in fluorescence microscopy},
	volume = {16},
	issn = {2041-1723},
	url = {https://doi.org/10.1038/s41467-024-55267-x},
	doi = {10.1038/s41467-024-55267-x},
	abstract = {Optical aberrations hinder fluorescence microscopy of thick samples, reducing image signal, contrast, and resolution. Here we introduce a deep learning-based strategy for aberration compensation, improving image quality without slowing image acquisition, applying additional dose, or introducing more optics. Our method (i) introduces synthetic aberrations to images acquired on the shallow side of image stacks, making them resemble those acquired deeper into the volume and (ii) trains neural networks to reverse the effect of these aberrations. We use simulations and experiments to show that applying the trained ‘de-aberration’ networks outperforms alternative methods, providing restoration on par with adaptive optics techniques; and subsequently apply the networks to diverse datasets captured with confocal, light-sheet, multi-photon, and super-resolution microscopy. In all cases, the improved quality of the restored data facilitates qualitative image inspection and improves downstream image quantitation, including orientational analysis of blood vessels in mouse tissue and improved membrane and nuclear segmentation in C. elegans embryos.},
	number = {1},
	journal = {Nature Communications},
	author = {Guo, Min and Wu, Yicong and Hobson, Chad M. and Su, Yijun and Qian, Shuhao and Krueger, Eric and Christensen, Ryan and Kroeschell, Grant and Bui, Johnny and Chaw, Matthew and Zhang, Lixia and Liu, Jiamin and Hou, Xuekai and Han, Xiaofei and Lu, Zhiye and Ma, Xuefei and Zhovmer, Alexander and Combs, Christian and Moyle, Mark and Yemini, Eviatar and Liu, Huafeng and Liu, Zhiyi and Benedetto, Alexandre and La Riviere, Patrick and Colón-Ramos, Daniel and Shroff, Hari},
	month = jan,
	year = {2025},
	pages = {313},
}

###################################
#    Chapter 8 references 
###################################

@ARTICLE{Haase2024,
  title    = "A call for {FAIR} and open-access training materials to advance
              {BioImage} Analysis",
  author   = "Haase, Robert and Tischer, Christian and Bankhead, Peter and
              Miura, Kota and Cimini, Beth",
  abstract = "Interdisciplinary communities, such as the life-sciences, have a
              strong need for efficient knowledge-transfer. In our community,
              computer scientists, bioimage analysts and biologists frequently
              come together to train each other in quantitative microscopy
              bioimage data analysis. For these trainings, re-usable
              high-quality training materials can be key. We advocate for
              publishing training materials according to the FAIR principles:
              Materials must be findable, openly accessible, stored in
              interoperable file formats, and most importantly made reusable by
              attaching open-access licenses. We are convinced that the path
              towards FAIR training materials leads us to more advanced and
              higher quality training, facilitating the advance of BioImage
              Analysis as a whole.",
  month    =  mar,
  year     =  2024
}

@ARTICLE{Ljosa2012,
  title    = "Annotated high-throughput microscopy image sets for validation",
  author   = "Ljosa, Vebjorn and Sokolnicki, Katherine L and Carpenter, Anne E",
  journal  = "Nat. Methods",
  volume   =  9,
  number   =  7,
  pages    =  637,
  month    =  jun,
  year     =  2012,
  language = "en"
}

@MISC{Cimini2019,
  title     = "When to say 'good enough'",
  author    = "Cimini, Beth A",
  publisher = "Zenodo",
  abstract  = "Blog post from the ``Measure Everything... Ask Questions Later''
               blog of the Broad Institute Imaging Platform",
  month     =  oct,
  year      =  2019
}

@ARTICLE{Jamali2021,
  title     = "2020 {BioImage} Analysis Survey: Community experiences and needs
               for the future",
  author    = "Jamali, Nasim and Dobson, Ellen T A and Eliceiri, Kevin W and
               Carpenter, Anne E and Cimini, Beth A",
  journal   = "Biological Imaging",
  publisher = "Cambridge University Press",
  volume    =  1,
  pages     = "e4",
  abstract  = "In this paper, we summarize a global survey of 484 participants
               of the imaging community, conducted in 2020 through the
               NIH-funded Center for Open Bioimage Analysis (COBA). This
               23-question survey covered experience with image analysis,
               scientific background and demographics, and views and requests
               from different members of the imaging community. Through
               open-ended questions, we asked the community to provide feedback
               for the open-source tool developers and tool user groups. The
               community’s requests for tool developers include general
               improvement of tool documentation and easy-to-follow tutorials.
               Respondents encourage tool users to follow the best practice
               guidelines for imaging and ask their image analysis questions on
               the Scientific Community Image Forum (forum.image.sc). We
               analyzed the community’s preferred method of learning based on
               level of computational proficiency and work description. In
               general, written step-by-step and video tutorials are preferred
               methods of learning by the community, followed by interactive
               webinars and office hours with an expert. There is also
               enthusiasm for a centralized online location for existing
               educational resources. The survey results will help the
               community, especially developers, trainers, and organizations
               like COBA, decide how to structure and prioritize their efforts.",
  month     =  jan,
  year      =  2021,
  keywords  = "Bioimage analysis; community; open-source software; survey;
               training"
}

@ARTICLE{Sivagurunathan2023,
  title    = "Bridging imaging users to imaging analysis - A community survey",
  author   = "Sivagurunathan, Suganya and Marcotti, Stefania and Nelson, Carl J
              and Jones, Martin L and Barry, David J and Slater, Thomas J A and
              Eliceiri, Kevin W and Cimini, Beth A",
  journal  = "J. Microsc.",
  abstract = "The 'Bridging Imaging Users to Imaging Analysis' survey was
              conducted in 2022 by the Center for Open Bioimage Analysis (COBA),
              BioImaging North America (BINA) and the Royal Microscopical
              Society Data Analysis in Imaging Section (RMS DAIM) to understand
              the needs of the imaging community. Through multichoice and
              open-ended questions, the survey inquired about demographics,
              image analysis experiences, future needs and suggestions on the
              role of tool developers and users. Participants of the survey were
              from diverse roles and domains of the life and physical sciences.
              To our knowledge, this is the first attempt to survey
              cross-community to bridge knowledge gaps between physical and life
              sciences imaging. Survey results indicate that respondents'
              overarching needs are documentation, detailed tutorials on the
              usage of image analysis tools, user-friendly intuitive software,
              and better solutions for segmentation, ideally in a format
              tailored to their specific use cases. The tool creators suggested
              the users familiarise themselves with the fundamentals of image
              analysis, provide constant feedback and report the issues faced
              during image analysis while the users would like more
              documentation and an emphasis on tool friendliness. Regardless of
              the computational experience, there is a strong preference for
              'written tutorials' to acquire knowledge on image analysis. We
              also observed that the interest in having 'office hours' to get an
              expert opinion on their image analysis methods has increased over
              the years. The results also showed less-than-expected usage of
              online discussion forums in the imaging community for solving
              image analysis problems. Surprisingly, we also observed a
              decreased interest among the survey respondents in deep/machine
              learning despite the increasing adoption of artificial
              intelligence in biology. In addition, the community suggests the
              need for a common repository for the available image analysis
              tools and their applications. The opinions and suggestions of the
              community, released here in full, will help the image analysis
              tool creation and education communities to design and deliver the
              resources accordingly.",
  month    =  sep,
  year     =  2023,
  keywords = "deep learning; image analysis; life science; physical science;
              survey",
  language = "en"
}

@ARTICLE{Schindelin2012,
  title    = "Fiji: an open-source platform for biological-image analysis",
  author   = "Schindelin, Johannes and Arganda-Carreras, Ignacio and Frise,
              Erwin and Kaynig, Verena and Longair, Mark and Pietzsch, Tobias
              and Preibisch, Stephan and Rueden, Curtis and Saalfeld, Stephan
              and Schmid, Benjamin and Tinevez, Jean-Yves and White, Daniel
              James and Hartenstein, Volker and Eliceiri, Kevin and Tomancak,
              Pavel and Cardona, Albert",
  journal  = "Nat. Methods",
  volume   =  9,
  number   =  7,
  pages    = "676--682",
  abstract = "Fiji is a distribution of the popular open-source software ImageJ
              focused on biological-image analysis. Fiji uses modern software
              engineering practices to combine powerful software libraries with
              a broad range of scripting languages to enable rapid prototyping
              of image-processing algorithms. Fiji facilitates the
              transformation of new algorithms into ImageJ plugins that can be
              shared with end users through an integrated update system. We
              propose Fiji as a platform for productive collaboration between
              computer science and biology research communities.",
  month    =  jun,
  year     =  2012,
  language = "en"
}

@ARTICLE{Schindelin2015,
  title     = "The {ImageJ} ecosystem: An open platform for biomedical image
               analysis",
  author    = "Schindelin, Johannes and Rueden, Curtis T and Hiner, Mark C and
               Eliceiri, Kevin W",
  journal   = "Mol. Reprod. Dev.",
  publisher = "Wiley",
  volume    =  82,
  number    = "7-8",
  pages     = "518--529",
  abstract  = "Technology in microscopy advances rapidly, enabling increasingly
               affordable, faster, and more precise quantitative biomedical
               imaging, which necessitates correspondingly more-advanced image
               processing and analysis techniques. A wide range of software is
               available-from commercial to academic, special-purpose to Swiss
               army knife, small to large-but a key characteristic of software
               that is suitable for scientific inquiry is its accessibility.
               Open-source software is ideal for scientific endeavors because it
               can be freely inspected, modified, and redistributed; in
               particular, the open-software platform ImageJ has had a huge
               impact on the life sciences, and continues to do so. From its
               inception, ImageJ has grown significantly due largely to being
               freely available and its vibrant and helpful user community.
               Scientists as diverse as interested hobbyists, technical
               assistants, students, scientific staff, and advanced biology
               researchers use ImageJ on a daily basis, and exchange knowledge
               via its dedicated mailing list. Uses of ImageJ range from data
               visualization and teaching to advanced image processing and
               statistical analysis. The software's extensibility continues to
               attract biologists at all career stages as well as computer
               scientists who wish to effectively implement specific
               image-processing algorithms. In this review, we use the ImageJ
               project as a case study of how open-source software fosters its
               suites of software tools, making multitudes of image-analysis
               technology easily accessible to the scientific community. We
               specifically explore what makes ImageJ so popular, how it impacts
               the life sciences, how it inspires other projects, and how it is
               self-influenced by coevolving projects within the ImageJ
               ecosystem.",
  month     =  jul,
  year      =  2015,
  language  = "en"
}

@MISC{napari,
  title = "napari: a multi-dimensional image viewer for Python",
  doi = {10.5281/zenodo.3555620}
}

@ARTICLE{Stirling2021,
  title    = "{CellProfiler} 4: improvements in speed, utility and usability",
  author   = "Stirling, David R and Swain-Bowden, Madison J and Lucas, Alice M
              and Carpenter, Anne E and Cimini, Beth A and Goodman, Allen",
  journal  = "BMC Bioinformatics",
  volume   =  22,
  number   =  1,
  pages    =  433,
  abstract = "BACKGROUND: Imaging data contains a substantial amount of
              information which can be difficult to evaluate by eye. With the
              expansion of high throughput microscopy methodologies producing
              increasingly large datasets, automated and objective analysis of
              the resulting images is essential to effectively extract
              biological information from this data. CellProfiler is a free,
              open source image analysis program which enables researchers to
              generate modular pipelines with which to process microscopy images
              into interpretable measurements. RESULTS: Herein we describe
              CellProfiler 4, a new version of this software with expanded
              functionality. Based on user feedback, we have made several user
              interface refinements to improve the usability of the software. We
              introduced new modules to expand the capabilities of the software.
              We also evaluated performance and made targeted optimizations to
              reduce the time and cost associated with running common
              large-scale analysis pipelines. CONCLUSIONS: CellProfiler 4
              provides significantly improved performance in complex workflows
              compared to previous versions. This release will ensure that
              researchers will have continued access to CellProfiler's powerful
              computational tools in the coming years.",
  month    =  sep,
  year     =  2021,
  keywords = "Bioimaging; Image analysis; Image quantitation; Image
              segmentation; Microscopy",
  language = "en"
}

@ARTICLE{Bankhead2017,
  title    = "{QuPath}: Open source software for digital pathology image
              analysis",
  author   = "Bankhead, Peter and Loughrey, Maurice B and Fernández, José A and
              Dombrowski, Yvonne and McArt, Darragh G and Dunne, Philip D and
              McQuaid, Stephen and Gray, Ronan T and Murray, Liam J and Coleman,
              Helen G and James, Jacqueline A and Salto-Tellez, Manuel and
              Hamilton, Peter W",
  journal  = "Sci. Rep.",
  volume   =  7,
  number   =  1,
  pages    =  16878,
  abstract = "QuPath is new bioimage analysis software designed to meet the
              growing need for a user-friendly, extensible, open-source solution
              for digital pathology and whole slide image analysis. In addition
              to offering a comprehensive panel of tumor identification and
              high-throughput biomarker evaluation tools, QuPath provides
              researchers with powerful batch-processing and scripting
              functionality, and an extensible platform with which to develop
              and share new algorithms to analyze complex tissue images.
              Furthermore, QuPath's flexible design makes it suitable for a wide
              range of additional image analysis applications across biomedical
              research.",
  month    =  dec,
  year     =  2017,
  language = "en"
}

@ARTICLE{de-Chaumont2012,
  title     = "Icy: an open bioimage informatics platform for extended
               reproducible research",
  author    = "de Chaumont, Fabrice and Dallongeville, Stéphane and Chenouard,
               Nicolas and Hervé, Nicolas and Pop, Sorin and Provoost, Thomas
               and Meas-Yedid, Vannary and Pankajakshan, Praveen and Lecomte,
               Timothée and Le Montagner, Yoann and Lagache, Thibault and
               Dufour, Alexandre and Olivo-Marin, Jean-Christophe",
  journal   = "Nat. Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  9,
  number    =  7,
  pages     = "690--696",
  abstract  = "Current research in biology uses evermore complex computational
               and imaging tools. Here we describe Icy, a collaborative bioimage
               informatics platform that combines a community website for
               contributing and sharing tools and material, and software with a
               high-end visual programming framework for seamless development of
               sophisticated imaging workflows. Icy extends the reproducible
               research principles, by encouraging and facilitating the
               reusability, modularity, standardization and management of
               algorithms and protocols. Icy is free, open-source and available
               at http://icy.bioimageanalysis.org/.",
  month     =  jun,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Berg2019,
  title     = "Ilastik: Interactive machine learning for (bio)image analysis",
  author    = "Berg, Stuart and Kutra, Dominik and Kroeger, Thorben and
               Straehle, Christoph N and Kausler, Bernhard X and Haubold,
               Carsten and Schiegg, Martin and Ales, Janez and Beier, Thorsten
               and Rudy, Markus and Eren, Kemal and Cervantes, Jaime I and Xu,
               Buote and Beuttenmueller, Fynn and Wolny, Adrian and Zhang, Chong
               and Koethe, Ullrich and Hamprecht, Fred A and Kreshuk, Anna",
  journal   = "Nat. Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  16,
  number    =  12,
  pages     = "1226--1232",
  abstract  = "We present ilastik, an easy-to-use interactive tool that brings
               machine-learning-based (bio)image analysis to end users without
               substantial computational expertise. It contains pre-defined
               workflows for image segmentation, object classification, counting
               and tracking. Users adapt the workflows to the problem at hand by
               interactively providing sparse training annotations for a
               nonlinear classifier. ilastik can process data in up to five
               dimensions (3D, time and number of channels). Its computational
               back end runs operations on-demand wherever possible, allowing
               for interactive prediction on data larger than RAM. Once the
               classifiers are trained, ilastik workflows can be applied to new
               data from the command line without further user interaction. We
               describe all ilastik workflows in detail, including three case
               studies and a discussion on the expected performance.",
  month     =  dec,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Arzt2022,
  title     = "{LABKIT}: Labeling and segmentation toolkit for big image data",
  author    = "Arzt, Matthias and Deschamps, Joran and Schmied, Christopher and
               Pietzsch, Tobias and Schmidt, Deborah and Tomancak, Pavel and
               Haase, Robert and Jug, Florian",
  journal   = "Front. Comput. Sci.",
  publisher = "Frontiers Media SA",
  volume    =  4,
  abstract  = "We present LABKIT, a user-friendly Fiji plugin for the
               segmentation of microscopy image data. It offers easy to use
               manual and automated image segmentation routines that can be
               rapidly applied to single- and multi-channel images as well as to
               timelapse movies in 2D or 3D. LABKIT is specifically designed to
               work efficiently on big image data and enables users of consumer
               laptops to conveniently work with multiple-terabyte images. This
               efficiency is achieved by using ImgLib2 and BigDataViewer as well
               as a memory efficient and fast implementation of the random
               forest based pixel classification algorithm as the foundation of
               our software. Optionally we harness the power of graphics
               processing units (GPU) to gain additional runtime performance.
               LABKIT is easy to install on virtually all laptops and
               workstations. Additionally, LABKIT is compatible with high
               performance computing (HPC) clusters for distributed processing
               of big image data. The ability to use pixel classifiers trained
               in LABKIT via the ImageJ macro language enables our users to
               integrate this functionality as a processing step in automated
               image processing workflows. Finally, LABKIT comes with rich
               online resources such as tutorials and examples that will help
               users to familiarize themselves with available features and how
               to best use LABKIT in a number of practical real-world use-cases.",
  month     =  feb,
  year      =  2022
}

@INPROCEEDINGS{Schmidt2018,
  title     = "Cell Detection with Star-Convex Polygons",
  author    = "Schmidt, Uwe and Weigert, Martin and Broaddus, Coleman and Myers,
               Gene",
  booktitle = "Medical Image Computing and Computer Assisted Intervention –
               MICCAI 2018",
  publisher = "Springer International Publishing",
  pages     = "265--273",
  abstract  = "Automatic detection and segmentation of cells and nuclei in
               microscopy images is important for many biological applications.
               Recent successful learning-based approaches include per-pixel
               cell segmentation with subsequent pixel grouping, or localization
               of bounding boxes with subsequent shape refinement. In situations
               of crowded cells, these can be prone to segmentation errors, such
               as falsely merging bordering cells or suppressing valid cell
               instances due to the poor approximation with bounding boxes. To
               overcome these issues, we propose to localize cell nuclei via
               star-convex polygons, which are a much better shape
               representation as compared to bounding boxes and thus do not need
               shape refinement. To that end, we train a convolutional neural
               network that predicts for every pixel a polygon for the cell
               instance at that position. We demonstrate the merits of our
               approach on two synthetic datasets and one challenging dataset of
               diverse fluorescence microscopy images.",
  year      =  2018
}

@ARTICLE{Goldsborough2024,
  title         = "{InstanSeg}: an embedding-based instance segmentation
                   algorithm optimized for accurate, efficient and portable cell
                   segmentation",
  author        = "Goldsborough, Thibaut and Philps, Ben and O'Callaghan, Alan
                   and Inglis, Fiona and Leplat, Leo and Filby, Andrew and
                   Bilen, Hakan and Bankhead, Peter",
  journal       = "arXiv [cs.CV]",
  abstract      = "Cell and nucleus segmentation are fundamental tasks for
                   quantitative bioimage analysis. Despite progress in recent
                   years, biologists and other domain experts still require
                   novel algorithms to handle increasingly large and complex
                   real-world datasets. These algorithms must not only achieve
                   state-of-the-art accuracy, but also be optimized for
                   efficiency, portability and user-friendliness. Here, we
                   introduce InstanSeg: a novel embedding-based instance
                   segmentation pipeline designed to identify cells and nuclei
                   in microscopy images. Using six public cell segmentation
                   datasets, we demonstrate that InstanSeg can significantly
                   improve accuracy when compared to the most widely used
                   alternative methods, while reducing the processing time by at
                   least 60\%. Furthermore, InstanSeg is designed to be fully
                   serializable as TorchScript and supports GPU acceleration on
                   a range of hardware. We provide an open-source implementation
                   of InstanSeg in Python, in addition to a user-friendly,
                   interactive QuPath extension for inference written in Java.
                   Our code and pre-trained models are available at
                   https://github.com/instanseg/instanseg .",
  month         =  aug,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Archit2025,
  title     = "Segment Anything for microscopy",
  author    = "Archit, Anwai and Freckmann, Luca and Nair, Sushmita and Khalid,
               Nabeel and Hilt, Paul and Rajashekar, Vikas and Freitag, Marei
               and Teuber, Carolin and Buckley, Genevieve and von Haaren,
               Sebastian and Gupta, Sagnik and Dengel, Andreas and Ahmed, Sheraz
               and Pape, Constantin",
  journal   = "Nat. Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  22,
  number    =  3,
  pages     = "579--591",
  abstract  = "Accurate segmentation of objects in microscopy images remains a
               bottleneck for many researchers despite the number of tools
               developed for this purpose. Here, we present Segment Anything for
               Microscopy (μSAM), a tool for segmentation and tracking in
               multidimensional microscopy data. It is based on Segment
               Anything, a vision foundation model for image segmentation. We
               extend it by fine-tuning generalist models for light and electron
               microscopy that clearly improve segmentation quality for a wide
               range of imaging conditions. We also implement interactive and
               automatic segmentation in a napari plugin that can speed up
               diverse segmentation tasks and provides a unified solution for
               microscopy annotation across different microscopy modalities. Our
               work constitutes the application of vision foundation models in
               microscopy, laying the groundwork for solving image analysis
               tasks in this domain with a small set of powerful deep learning
               models.",
  month     =  mar,
  year      =  2025,
  language  = "en"
}

@ARTICLE{Pachitariu2022,
  title     = "Cellpose 2.0: how to train your own model",
  author    = "Pachitariu, Marius and Stringer, Carsen",
  journal   = "Nat. Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  19,
  number    =  12,
  pages     = "1634--1641",
  abstract  = "Pretrained neural network models for biological segmentation can
               provide good out-of-the-box results for many image types.
               However, such models do not allow users to adapt the segmentation
               style to their specific needs and can perform suboptimally for
               test images that are very different from the training images.
               Here we introduce Cellpose 2.0, a new package that includes an
               ensemble of diverse pretrained models as well as a
               human-in-the-loop pipeline for rapid prototyping of new custom
               models. We show that models pretrained on the Cellpose dataset
               can be fine-tuned with only 500-1,000 user-annotated regions of
               interest (ROI) to perform nearly as well as models trained on
               entire datasets with up to 200,000 ROI. A human-in-the-loop
               approach further reduced the required user annotation to 100-200
               ROI, while maintaining high-quality segmentations. We provide
               software tools such as an annotation graphical user interface, a
               model zoo and a human-in-the-loop pipeline to facilitate the
               adoption of Cellpose 2.0.",
  month     =  dec,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Zhang2023,
  title         = "Bio-Image Informatics Index {BIII}: A unique database of
                   image analysis tools and workflows for and by the bioimaging
                   community",
  author        = "Zhang, Chong and Gaignard, Alban and Kalas, Matus and Levet,
                   Florian and Delestro, Felipe and Lindblad, Joakim and
                   Sladoje, Natasa and Plantard, Laure and Latour, Alain and
                   Haase, Robert and Martins, Gabriel and Sampaio, Paula and
                   Scholz, Leandro and Taggers, Neubias and Tosi, Sébastien and
                   Miura, Kota and Colombelli, Julien and Paul-Gilloteaux,
                   Perrine",
  journal       = "arXiv [q-bio.QM]",
  abstract      = "Bio image analysis has recently become one keystone of
                   biological research but biologists tend to get lost in a
                   plethora of available software and the way to adjust
                   available tools to their own image analysis problem. We
                   present BIII, BioImage Informatic Index (www.biii.eu), the
                   result of the first large community effort to bridge the
                   communities of algorithm and software developers, bioimage
                   analysts and biologists, under the form of a web-based
                   knowledge database crowdsourced by these communities.
                   Software tools (> 1300), image databases for benchmarking
                   (>20) and training materials (>70) for bio image analysis are
                   referenced and curated following standards constructed by the
                   community and then reaching a broader audience. Software
                   tools are organized as full protocol of analysis (workflow),
                   specific brick (component) to construct a workflow, or
                   software platform or library (collection). They are described
                   using Edam Bio Imaging, which is iteratively defined using
                   this website. All entries are exposed following FAIR
                   principles and accessible for other usage.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.QM"
}

@ARTICLE{Ouyang2022,
  title    = "{BioImage} Model Zoo: A Community-Driven Resource for Accessible
              Deep Learning in {BioImage} Analysis",
  author   = "Ouyang, Wei and Beuttenmueller, Fynn and Gómez-de-Mariscal,
              Estibaliz and Pape, Constantin and Burke, Tom and
              Garcia-López-de-Haro, Carlos and Russell, Craig and Moya-Sans,
              Lucía and de-la-Torre-Gutiérrez, Cristina and Schmidt, Deborah and
              Kutra, Dominik and Novikov, Maksim and Weigert, Martin and
              Schmidt, Uwe and Bankhead, Peter and Jacquemet, Guillaume and
              Sage, Daniel and Henriques, Ricardo and Muñoz-Barrutia, Arrate and
              Lundberg, Emma and Jug, Florian and Kreshuk, Anna",
  journal  = "bioRxiv",
  pages    = "2022.06.07.495102",
  abstract = "Deep learning-based approaches are revolutionizing imaging-driven
              scientific research. However, the accessibility and
              reproducibility of deep learning-based workflows for imaging
              scientists remain far from sufficient. Several tools have recently
              risen to the challenge of democratizing deep learning by providing
              user-friendly interfaces to analyze new data with pre-trained or
              fine-tuned models. Still, few of the existing pre-trained models
              are interoperable between these tools, critically restricting a
              model’s overall utility and the possibility of validating and
              reproducing scientific analyses. Here, we present the BioImage
              Model Zoo (): a community-driven, fully open resource where
              standardized pre-trained models can be shared, explored, tested,
              and downloaded for further adaptation or direct deployment in
              multiple end user-facing tools (e.g., ilastik, deepImageJ, QuPath,
              StarDist, ImJoy, ZeroCostDL4Mic, CSBDeep). To enable everyone to
              contribute and consume the Zoo resources, we provide a model
              standard to enable cross-compatibility, a rich list of example
              models and practical use-cases, developer tools, documentation,
              and the accompanying infrastructure for model upload, download and
              testing. Our contribution aims to lay the groundwork to make deep
              learning methods for microscopy imaging findable, accessible,
              interoperable, and reusable (FAIR) across software tools and
              platforms. \#\#\# Competing Interest Statement The authors have
              declared no competing interest.",
  month    =  jun,
  year     =  2022,
  language = "en"
}

@ARTICLE{Rueden2019,
  title    = "Scientific Community Image Forum: A discussion forum for
              scientific image software",
  author   = "Rueden, Curtis T and Ackerman, Jeanelle and Arena, Ellen T and
              Eglinger, Jan and Cimini, Beth A and Goodman, Allen and Carpenter,
              Anne E and Eliceiri, Kevin W",
  journal  = "PLoS Biol.",
  volume   =  17,
  number   =  6,
  pages    = "e3000340",
  abstract = "Forums and email lists play a major role in assisting scientists
              in using software. Previously, each open-source bioimaging
              software package had its own distinct forum or email list.
              Although each provided access to experts from various software
              teams, this fragmentation resulted in many scientists not knowing
              where to begin with their projects. Thus, the scientific imaging
              community lacked a central platform where solutions could be
              discussed in an open, software-independent manner. In response, we
              introduce the Scientific Community Image Forum, where users can
              pose software-related questions about digital image analysis,
              acquisition, and data management.",
  month    =  jun,
  year     =  2019,
  language = "en"
}

@ARTICLE{Gomez-de-Mariscal2021,
  title     = "{DeepImageJ}: A user-friendly environment to run deep learning
               models in {ImageJ}",
  author    = "Gómez-de-Mariscal, Estibaliz and García-López-de-Haro, Carlos and
               Ouyang, Wei and Donati, Laurène and Lundberg, Emma and Unser,
               Michael and Muñoz-Barrutia, Arrate and Sage, Daniel",
  journal   = "Nat. Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  18,
  number    =  10,
  pages     = "1192--1195",
  abstract  = "DeepImageJ is a user-friendly solution that enables the generic
               use of pre-trained deep learning models for biomedical image
               analysis in ImageJ. The deepImageJ environment gives access to
               the largest bioimage repository of pre-trained deep learning
               models (BioImage Model Zoo). Hence, nonexperts can easily perform
               common image processing tasks in life-science research with deep
               learning-based tools including pixel and object classification,
               instance segmentation, denoising or virtual staining. DeepImageJ
               is compatible with existing state of the art solutions and it is
               equipped with utility tools for developers to include new models.
               Very recently, several training frameworks have adopted the
               deepImageJ format to deploy their work in one of the most used
               softwares in the field (ImageJ). Beyond its direct use, we expect
               deepImageJ to contribute to the broader dissemination and reuse
               of deep learning models in life sciences applications and
               bioimage informatics.",
  month     =  oct,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Ouyang2019,
  title     = "{ImJoy}: an open-source computational platform for the deep
               learning era",
  author    = "Ouyang, Wei and Mueller, Florian and Hjelmare, Martin and
               Lundberg, Emma and Zimmer, Christophe",
  journal   = "Nat. Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  16,
  number    =  12,
  pages     = "1199--1200",
  month     =  dec,
  year      =  2019,
  language  = "en"
}

@MISC{Shah,
  title    = "Bilayers",
  author   = "Shah, Rajavi and Gogoberidze, Nodar and Cimini, Beth",
  abstract = "A Container Specification and CI/CD built for whole-community
              support",
  url      = "https://github.com/bilayer-containers/bilayers"
}

@ARTICLE{Hidalgo-Cenalmor2024,
  title     = "{DL4MicEverywhere}: deep learning for microscopy made flexible,
               shareable and reproducible",
  author    = "Hidalgo-Cenalmor, Iván and Pylvänäinen, Joanna W and G Ferreira,
               Mariana and Russell, Craig T and Saguy, Alon and
               Arganda-Carreras, Ignacio and Shechtman, Yoav and {AI4Life
               Horizon Europe Program Consortium} and Jacquemet, Guillaume and
               Henriques, Ricardo and Gómez-de-Mariscal, Estibaliz",
  journal   = "Nat. Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  21,
  number    =  6,
  pages     = "925--927",
  month     =  jun,
  year      =  2024,
  language  = "en"
}

@inproceedings{jupyter,
       booktitle = {Positioning and Power in Academic Publishing: Players, Agents and Agendas},
          editor = {Fernando Loizides and Birgit Scmidt},
           title = {Jupyter Notebooks - a publishing format for reproducible computational workflows},
          author = {Thomas Kluyver and Benjamin Ragan-Kelley and Fernando P{\'e}rez and Brian Granger and Matthias Bussonnier and Jonathan Frederic and Kyle Kelley and Jessica Hamrick and Jason Grout and Sylvain Corlay and Paul Ivanov and Dami{\'a}n Avila and Safia Abdalla and Carol Willing and  Jupyter development team},
       publisher = {IOS Press},
         address = {Netherlands},
            year = {2016},
           pages = {87--90},
             url = {https://eprints.soton.ac.uk/403913/},
        abstract = {It is increasingly necessary for researchers in all fields to write computer code, and in order to reproduce research results, it is important that this code is published. We present Jupyter notebooks, a document format for publishing code, results and explanations in a form that is both readable and executable. We discuss various tools and use cases for notebook documents.}
}

@ARTICLE{Kreshuk2019,
  title    = "Machine Learning: Advanced Image Segmentation Using ilastik",
  author   = "Kreshuk, Anna and Zhang, Chong",
  journal  = "Methods Mol. Biol.",
  volume   =  2040,
  pages    = "449--463",
  abstract = "Segmentation is one of the most ubiquitous problems in biological
              image analysis. Here we present a machine learning-based solution
              to it as implemented in the open source ilastik toolkit. We give a
              broad description of the underlying theory and demonstrate two
              workflows: Pixel Classification and Autocontext. We illustrate
              their use on a challenging problem in electron microscopy image
              segmentation. After following this walk-through, we expect the
              readers to be able to apply the necessary steps to their own data
              and segment their images by either workflow.",
  year     =  2019,
  keywords = "Machine learning; Random forest; Semantic segmentation; ilastik",
  language = "en"
}

###################################
#    Chapter 9 references 
###################################


@article{arganda-carreras2017,
	title = {Trainable {Weka} {Segmentation}: a machine learning tool for microscopy pixel classification},
	volume = {33},
	issn = {1367-4803},
	shorttitle = {Trainable {Weka} {Segmentation}},
	url = {https://doi.org/10.1093/bioinformatics/btx180},
	doi = {10.1093/bioinformatics/btx180},
	abstract = {State-of-the-art light and electron microscopes are capable of acquiring large image datasets, but quantitatively evaluating the data often involves manually annotating structures of interest. This process is time-consuming and often a major bottleneck in the evaluation pipeline. To overcome this problem, we have introduced the Trainable Weka Segmentation (TWS), a machine learning tool that leverages a limited number of manual annotations in order to train a classifier and segment the remaining data automatically. In addition, TWS can provide unsupervised segmentation learning schemes (clustering) and can be customized to employ user-designed image features or classifiers.TWS is distributed as open-source software as part of the Fiji image processing distribution of ImageJ at http://imagej.net/Trainable\_Weka\_Segmentation.Supplementary data are available at Bioinformatics online.},
	number = {15},
	urldate = {2023-08-22},
	journal = {Bioinformatics},
	author = {Arganda-Carreras, Ignacio and Kaynig, Verena and Rueden, Curtis and Eliceiri, Kevin W and Schindelin, Johannes and Cardona, Albert and Sebastian Seung, H},
	month = aug,
	year = {2017},
	pages = {2424--2426},
}

@article{berg2019,
	title = {ilastik: interactive machine learning for (bio)image analysis},
	volume = {16},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {ilastik},
	url = {https://www.nature.com/articles/s41592-019-0582-9},
	doi = {10.1038/s41592-019-0582-9},
	abstract = {We present ilastik, an easy-to-use interactive tool that brings machine-learning-based (bio)image analysis to end users without substantial computational expertise. It contains pre-defined workflows for image segmentation, object classification, counting and tracking. Users adapt the workflows to the problem at hand by interactively providing sparse training annotations for a nonlinear classifier. ilastik can process data in up to five dimensions (3D, time and number of channels). Its computational back end runs operations on-demand wherever possible, allowing for interactive prediction on data larger than RAM. Once the classifiers are trained, ilastik workflows can be applied to new data from the command line without further user interaction. We describe all ilastik workflows in detail, including three case studies and a discussion on the expected performance.},
	language = {en},
	number = {12},
	urldate = {2023-08-22},
	journal = {Nature Methods},
	author = {Berg, Stuart and Kutra, Dominik and Kroeger, Thorben and Straehle, Christoph N. and Kausler, Bernhard X. and Haubold, Carsten and Schiegg, Martin and Ales, Janez and Beier, Thorsten and Rudy, Markus and Eren, Kemal and Cervantes, Jaime I. and Xu, Buote and Beuttenmueller, Fynn and Wolny, Adrian and Zhang, Chong and Koethe, Ullrich and Hamprecht, Fred A. and Kreshuk, Anna},
	month = dec,
	year = {2019},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning, Software},
	pages = {1226--1232},
}

@article{pachitariu2022,
	title = {Cellpose 2.0: how to train your own model},
	copyright = {2022 The Author(s)},
	issn = {1548-7105},
	shorttitle = {Cellpose 2.0},
	url = {http://www.nature.com/articles/s41592-022-01663-4},
	doi = {10.1038/s41592-022-01663-4},
	abstract = {Cellpose 2.0 improves cell segmentation by offering pretrained models that can be fine-tuned using a human-in-the-loop training pipeline and fewer than 1,000 user-annotated regions of interest.},
	language = {en},
	urldate = {2022-11-09},
	journal = {Nature Methods},
	author = {Pachitariu, Marius and Stringer, Carsen},
	month = nov,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational platforms and environments, Image processing},
	pages = {1--8},
}

@inproceedings{schmidt2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Cell {Detection} with {Star}-{Convex} {Polygons}},
	isbn = {978-3-030-00934-2},
	doi = {10.1007/978-3-030-00934-2_30},
	abstract = {Automatic detection and segmentation of cells and nuclei in microscopy images is important for many biological applications. Recent successful learning-based approaches include per-pixel cell segmentation with subsequent pixel grouping, or localization of bounding boxes with subsequent shape refinement. In situations of crowded cells, these can be prone to segmentation errors, such as falsely merging bordering cells or suppressing valid cell instances due to the poor approximation with bounding boxes. To overcome these issues, we propose to localize cell nuclei via star-convex polygons, which are a much better shape representation as compared to bounding boxes and thus do not need shape refinement. To that end, we train a convolutional neural network that predicts for every pixel a polygon for the cell instance at that position. We demonstrate the merits of our approach on two synthetic datasets and one challenging dataset of diverse fluorescence microscopy images.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2018},
	publisher = {Springer International Publishing},
	author = {Schmidt, Uwe and Weigert, Martin and Broaddus, Coleman and Myers, Gene},
	editor = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and Alberola-López, Carlos and Fichtinger, Gabor},
	year = {2018},
	pages = {265--273},
}

@article{fazeli2020,
	title = {Automated cell tracking using {StarDist} and {TrackMate}},
	volume = {9},
	issn = {2046-1402},
	doi = {10.12688/f1000research.27019.1},
	abstract = {The ability of cells to migrate is a fundamental physiological process involved in embryonic development, tissue homeostasis, immune surveillance, and wound healing. Therefore, the mechanisms governing cellular locomotion have been under intense scrutiny over the last 50 years. One of the main tools of this scrutiny is live-cell quantitative imaging, where researchers image cells over time to study their migration and quantitatively analyze their dynamics by tracking them using the recorded images. Despite the availability of computational tools, manual tracking remains widely used among researchers due to the difficulty setting up robust automated cell tracking and large-scale analysis. Here we provide a detailed analysis pipeline illustrating how the deep learning network StarDist can be combined with the popular tracking software TrackMate to perform 2D automated cell tracking and provide fully quantitative readouts. Our proposed protocol is compatible with both fluorescent and widefield images. It only requires freely available and open-source software (ZeroCostDL4Mic and Fiji), and does not require any coding knowledge from the users, making it a versatile and powerful tool for the field. We demonstrate this pipeline's usability by automatically tracking cancer cells and T cells using fluorescent and brightfield images. Importantly, we provide, as supplementary information, a detailed step-by-step protocol to allow researchers to implement it with their images.},
	language = {eng},
	journal = {F1000Research},
	author = {Fazeli, Elnaz and Roy, Nathan H. and Follain, Gautier and Laine, Romain F. and von Chamier, Lucas and Hänninen, Pekka E. and Eriksson, John E. and Tinevez, Jean-Yves and Jacquemet, Guillaume},
	year = {2020},
	pmid = {33224481},
	pmcid = {PMC7670479},
	keywords = {Automated tracking, Cell migration, Cell Movement, Cell Tracking, Deep-learning, Fiji, Image analysis, Image Processing, Computer-Assisted, Software, StarDist, TrackMate},
	pages = {1279},
}

@article{schindelin2012,
	title = {Fiji: an open-source platform for biological-image analysis},
	volume = {9},
	url = {https://www.nature.com/articles/nmeth.2019},
	doi = {10.1038/nmeth.2019},
	abstract = {Presented is an overview of the image-analysis software platform Fiji, a distribution of ImageJ that updates the underlying ImageJ architecture and adds modern software design elements to expand the capabilities of the platform and facilitate collaboration between biologists and computer scientists.},
	language = {en},
	number = {7},
	journal = {Nature Methods},
	author = {Schindelin, Johannes and Arganda-Carreras, Ignacio and Frise, Erwin and Kaynig, Verena and Longair, Mark and Pietzsch, Tobias and Preibisch, Stephan and Rueden, Curtis and Saalfeld, Stephan and Schmid, Benjamin and Tinevez, Jean-Yves and White, Daniel James and Hartenstein, Volker and Eliceiri, Kevin and Tomancak, Pavel and Cardona, Albert},
	year = {2012},
	note = {ISBN: 1548-7105},
	pages = {676--682},
}

@misc{ouyang2022,
	title = {{BioImage} {Model} {Zoo}: {A} {Community}-{Driven} {Resource} for {Accessible} {Deep} {Learning} in {BioImage} {Analysis}},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	shorttitle = {{BioImage} {Model} {Zoo}},
	url = {https://www.biorxiv.org/content/10.1101/2022.06.07.495102v1},
	doi = {10.1101/2022.06.07.495102},
	abstract = {Deep learning-based approaches are revolutionizing imaging-driven scientific research. However, the accessibility and reproducibility of deep learning-based workflows for imaging scientists remain far from sufficient. Several tools have recently risen to the challenge of democratizing deep learning by providing user-friendly interfaces to analyze new data with pre-trained or fine-tuned models. Still, few of the existing pre-trained models are interoperable between these tools, critically restricting a model’s overall utility and the possibility of validating and reproducing scientific analyses. Here, we present the BioImage Model Zoo (https://bioimage.io): a community-driven, fully open resource where standardized pre-trained models can be shared, explored, tested, and downloaded for further adaptation or direct deployment in multiple end user-facing tools (e.g., ilastik, deepImageJ, QuPath, StarDist, ImJoy, ZeroCostDL4Mic, CSBDeep). To enable everyone to contribute and consume the Zoo resources, we provide a model standard to enable cross-compatibility, a rich list of example models and practical use-cases, developer tools, documentation, and the accompanying infrastructure for model upload, download and testing. Our contribution aims to lay the groundwork to make deep learning methods for microscopy imaging findable, accessible, interoperable, and reusable (FAIR) across software tools and platforms.},
	language = {en},
	urldate = {2023-10-05},
	publisher = {bioRxiv},
	author = {Ouyang, Wei and Beuttenmueller, Fynn and Gómez-de-Mariscal, Estibaliz and Pape, Constantin and Burke, Tom and Garcia-López-de-Haro, Carlos and Russell, Craig and Moya-Sans, Lucía and de-la-Torre-Gutiérrez, Cristina and Schmidt, Deborah and Kutra, Dominik and Novikov, Maksim and Weigert, Martin and Schmidt, Uwe and Bankhead, Peter and Jacquemet, Guillaume and Sage, Daniel and Henriques, Ricardo and Muñoz-Barrutia, Arrate and Lundberg, Emma and Jug, Florian and Kreshuk, Anna},
	month = jun,
	year = {2022},
	note = {Pages: 2022.06.07.495102
Section: New Results},
}

@inproceedings{krizhevsky2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	urldate = {2023-10-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
}

@misc{ahlers2023,
	title = {napari: a multi-dimensional image viewer for {Python}},
	shorttitle = {napari},
	url = {https://zenodo.org/record/8115575},
	abstract = {napari 0.4.18 We're happy to announce the release of napari 0.4.18! napari is a fast, interactive, multi-dimensional image viewer for Python. It's designed for browsing, annotating, and analyzing large multi-dimensional images. It's built on top of Qt (for the GUI), vispy (for performant GPU-based rendering), and the scientific Python stack (numpy, scipy). This is primarily a bug-fix release, addressing many issues from 0.4.17 (see "Bug Fixes", below). However, it also contains some performance improvements and several exciting new features (see "Highlights"), so read on below! For more information, examples, and documentation, please visit our website: https://napari.org Highlights Drawing polygons in the Shapes layer can now be done much faster with the new lasso tool (napari/napari/\#5555) Surface layers now support textures and vertex colors, allowing a whole new type of dataset to be visualised in napari. Have a look at surface\_multi\_texture.py and surface\_texture\_and\_colors.py in the examples directory for some pretty demos! (napari/napari/\#5642) Previously, navigating an image required switching out of whatever drawing mode you might have been using and going back to pan/zoom mode. Now you can use the mouse wheel to zoom in and out in any mode. (napari/napari/\#5701) Painting labels is now much, much faster (achieving 60fps even on an 8k x 8k image) (napari/napari/\#5723 and napari/napari/\#5732) Vectors layers can now be displayed with two different styles of arrowheads, instead of just plain lines. This removes a longstanding limitation of the vectors layer! (napari/napari/\#5740) New Features Overlays 2.0 (napari/napari/\#4894) expose custom image interpolation kernels (napari/napari/\#5130) Add user agent environment variable for pip installations (napari/napari/\#5135) Add option to check if plugin try to set viewer attr outside main thread (napari/napari/\#5195) Set selection color for QListView item. (napari/napari/\#5202) Add warning about set private attr when using proxy (napari/napari/\#5209) Shapes interpolation (napari/napari/\#5334) Add dask settings to preferences (napari/napari/\#5490) Add lasso tool for faster drawing of polygonal Shapes (napari/napari/\#5555) Feature: support for textures and vertex colors on Surface layers (napari/napari/\#5642) Back point selection with a psygnal Selection (napari/napari/\#5691) Zooming with the mouse wheel in any mode (napari/napari/\#5701) Add cancellation functionality to progress (napari/napari/\#5728) Add arrow display styles to Vectors layer (napari/napari/\#5740) Improvements Set keyboard focus on console when opened (napari/napari/\#5208) Push variables to console when instantiated (napari/napari/\#5210) Tracks layer creation performance improvement (napari/napari/\#5303) PERF: Event emissions and perf regression. (napari/napari/\#5307) Much faster FormatStringEncoding (napari/napari/\#5315) Add parent when creating layer context menu to inherit application theme and add style entry for disabled widgets and menus (napari/napari/\#5381) Add correct enablement kwarg to Split Stack action, Convert data type submenu and Projections submenu (napari/napari/\#5437) Apply disabled widgets style only for menus and set menus styles for QModelMenu and QMenu instances (napari/napari/\#5446) Add disabled style rule for QComboBox following the one for QPushButton (napari/napari/\#5469) Allow layers control section to resize to contents (napari/napari/\#5474) Allow to use Optional annotation in function return type for magicgui functions (napari/napari/\#5595) Skip equality comparisons in EventedModel when unnecessary (napari/napari/\#5615) Bugfix: improve layout of Preferences {\textgreater} Shortcuts tables (napari/napari/\#5679) Improve preferences genration (napari/napari/\#5696) Add dev example for adding custom overlays. (napari/napari/\#5719) Disable buffer swapping (napari/napari/\#5741) Remove max brush size from increase brush size keybinding (napari/napari/\#5761) Explicitly list valid layer names in types (napari/napari/\#5823) Sort npe1 widget contributions (napari/napari/\#5865) feat: add since\_version argument of rename\_argument decorator (napari/napari/\#5910) Emit extra information with layer.events.data (napari/napari/\#5967) Performance Return early when no slicing needed (napari/napari/\#5239) Tracks layer creation performance improvement (napari/napari/\#5303) PERF: Event emissions and perf regression. (napari/napari/\#5307) Much faster FormatStringEncoding (napari/napari/\#5315) Fix inefficient label mapping in direct color mode (10-20x speedup) (napari/napari/\#5723) Efficient labels mapping for drawing in Labels (60 FPS even with 8000x8000 images) (napari/napari/\#5732) Disable buffer swapping (napari/napari/\#5741) Bug Fixes Warn instead of failing on empty or invalid alt-text (napari/napari/\#4505) Fix display of order and scale combinations (napari/napari/\#5004) Enforce that contrast limits must be increasing (napari/napari/\#5036) Bugfix: Move Window menu to be before Help (napari/napari/\#5093) Add extra garbage collection for some viewer tests (napari/napari/\#5108) Connect image to plane events and expose them (napari/napari/\#5131) Workaround for discover themes from plugins (napari/napari/\#5150) Add missed dialogs to qtbot in test\_qt\_notifications to prevent segfaults (napari/napari/\#5171) DOC Update docstring of add\_dock\_widget \& \_add\_viewer\_dock\_widget (napari/napari/\#5173) Fix unsortable features (napari/napari/\#5186) Avoid possible divide-by-zero in Vectors layer thumbnail update (napari/napari/\#5192) Disable napari-console button when launched from jupyter (napari/napari/\#5213) Volume rendering updates for isosurface and attenuated MIP (napari/napari/\#5215) Return early when no slicing needed (napari/napari/\#5239) Check strictly increasing values when clipping contrast limits to a new range (napari/napari/\#5258) UI Bugfix: Make disabled QPushButton more distinct (napari/napari/\#5262) Respect background color when calculating scale bar color (napari/napari/\#5270) Fix circular import in \_vispy module (napari/napari/\#5276) Use only data dimensions for cord in status bar (napari/napari/\#5283) Prevent obsolete reports about failure of cleaning viewer instances (napari/napari/\#5317) Add scikit-image[data] to install\_requires, because it's required by builtins (napari/napari/\#5329) Fix repeating close dialog on macOS and qt 5.12 (napari/napari/\#5337) Disable napari-console if napari launched from vanilla python REPL (napari/napari/\#5350) For npe2 plugin, use manifest display\_name for File {\textgreater} Open Samples (napari/napari/\#5351) Bugfix plugin display\_name use (File {\textgreater} Open Sample, Plugin menus) (napari/napari/\#5366) Fix editing shape data above 2 dimensions (napari/napari/\#5383) Fix test keybinding for layer actions (napari/napari/\#5406) fix theme id not being used correctly (napari/napari/\#5412) Clarify layer's editable property and separate interaction with visible property (napari/napari/\#5413) Fix theme reference to get image for success\_label style (napari/napari/\#5447) Bugfix: Ensure layer.\_fixed\_vertex is set when rotating (napari/napari/\#5449) Fix \_n\_selected\_points in \_layerlist\_context.py (napari/napari/\#5450) Refactor Main Window status bar to improve information presentation (napari/napari/\#5451) Bugfix: Fix test\_get\_system\_theme test for name to id change (napari/napari/\#5456) Bugfix: POLL\_INTERVAL\_MS used in QTimer needs to be an int on python 3.10 (napari/napari/\#5467) Bugfix: Add missing Enums and Flags required by PySide6 {\textgreater} 6.4 (napari/napari/\#5480) BugFix: napari does not start with Python v3.11.1: "ValueError: A distribution name is required." (napari/napari/\#5482) Fix inverted LUT and blending (napari/napari/\#5487) Fix opening file dialogs in PySide (napari/napari/\#5492) Handle case when QtDims play thread is partially deleted (napari/napari/\#5499) Ensure surface normals and wireframes are using Models internally (napari/napari/\#5501) Recursively check for dependent property to fire events. (napari/napari/\#5528) Set PYTHONEXECUTABLE as part of macos fixes on (re)startup (napari/napari/\#5531) Un-set unified title and tool bar on mac (Qt property) (napari/napari/\#5533) Fix key error issue of action manager (napari/napari/\#5539) Bugfix: ensure Checkbox state comparisons are correct by using Qt.CheckState(state) (napari/napari/\#5541) Clean dangling widget in test (napari/napari/\#5544) Fix test\_worker\_with\_progress by wait on worker end (napari/napari/\#5548) Fix min req (napari/napari/\#5560) Fix vispy axes labels (napari/napari/\#5565) Fix colormap utils error suggestion code and add a test (napari/napari/\#5571) Fix problem of missing plugin widgets after minimize napari (napari/napari/\#5577) Make point size isotropic (napari/napari/\#5582) Fix guard of qt import in napari.utils.theme (napari/napari/\#5593) Fix empty shapes layer duplication and Convert to Labels enablement logic for selected empty shapes layers (napari/napari/\#5594) Stop using removed multichannel= kwarg to skimage functions (napari/napari/\#5596) Add information about syntax\_style value in error message for theme validation (napari/napari/\#5602) Remove catch\_warnings in slicing (napari/napari/\#5603) Incorret theme should not prevent napari from start (napari/napari/\#5605) Unblock axis labels event to be emitted when slider label changes (napari/napari/\#5631) Bugfix: IndexError slicing Surface with higher-dimensional vertex\_values (napari/napari/\#5635) Bugfix: Convert Viewer Delete button to QtViewerPushButton with action and shortcut (napari/napari/\#5636) Change dim axis\_label resize logic to set width using only displayed labels width (napari/napari/\#5640) Feature: support for textures and vertex colors on Surface layers (napari/napari/\#5642) Fix features issues with init param and property setter (napari/napari/\#5646) Bugfix: Don't double toggle visibility for linked layers (napari/napari/\#5656) Bugfix: ensure pan/zoom buttons work, along with spacebar keybinding (napari/napari/\#5669) Bugfix: Add Tracks to qt\_keyboard\_settings (napari/napari/\#5678) Fix automatic naming and GUI exposure of multiple unnamed colormaps (napari/napari/\#5682) Fix mouse movement handling for TransformBoxOverlay (napari/napari/\#5692) Update environment.yml (napari/napari/\#5693) Resolve symlinks from path to environment for setting path (napari/napari/\#5704) Fix tracks color-by when properties change (napari/napari/\#5708) Fix Sphinx warnings (napari/napari/\#5717) Do not use depth for canvas overlays; allow setting blending mode for overlays (napari/napari/\#5720) Unify event behaviour for points and its qt controls (napari/napari/\#5722) Fix camera 3D absolute rotation bug (napari/napari/\#5726) Maint: Bump mypy (napari/napari/\#5727) Style QGroupBox indicator (napari/napari/\#5729) Fix centering of non-displayed dimensions (napari/napari/\#5736) Don't attempt to use npe1 readers in napari.plugins.\_npe2.read (napari/napari/\#5739) Prevent canvas micro-panning on point add (napari/napari/\#5742) Use text opacity to signal that widget is disabled (napari/napari/\#5745) Bugfix: Add the missed keyReleaseEvent method in QtViewerDockWidget (napari/napari/\#5746) Update status bar on active layer change (napari/napari/\#5754) Use array size directly when checking multiscale arrays to prevent overflow (napari/napari/\#5759) Fix path to check\_updated\_packages.py (napari/napari/\#5762) Brush cursor implementation using an overlay (napari/napari/\#5763) Bugfix: force a redraw to ensure highlight shows when Points are select-all selected (napari/napari/\#5771) Fix copy/paste of points (napari/napari/\#5795) Fix multiple viewer example (napari/napari/\#5796) Fix colormapping nD images (napari/napari/\#5805) Set focus policy for mainwindow to prevent keeping focus on the axis labels (and other QLineEdit based widgets) when clicking outside the widget (napari/napari/\#5812) Enforce Points.selected\_data type as Selection (napari/napari/\#5813) Change toggle menubar visibility functionality to hide menubar and show it on mouse movement validation (napari/napari/\#5824) Bugfix: Disconnect callbacks on object deletion in special functions from event\_utils (napari/napari/\#5826) Do not blend color in QtColorBox with black using opacity (napari/napari/\#5827) Don't allow negative contour values (napari/napari/\#5830) Bugfixes for layer overlays: clean up when layer is removed + fix potential double creation (napari/napari/\#5831) Add compatibility to PySide in file dialogs by using positional arguments (napari/napari/\#5834) Bugfix: fix broken "show selected" in the Labels layer (because of caching) (napari/napari/\#5841) Add tests for popup widgets and fix perspective popup slider initialization (napari/napari/\#5848) [Qt6] Fix AttributeError on renaming layer (napari/napari/\#5850) Bugfix: Ensure QTableWidgetItem(action.description) item is enabled (napari/napari/\#5854) Add constraints file during installation of packages from pip in docs workflow (napari/napari/\#5862) Bugfix: link the Labels model to the "show selected" checkbox (napari/napari/\#5867) Add \_\_all\_\_ to napari/types.py (napari/napari/\#5894) Fix drawing vertical or horizontal line segments in Shapes layer (napari/napari/\#5895) Disallow outside screen geometry napari window position (napari/napari/\#5915) Fix napari-svg version parsing in conftest.py (napari/napari/\#5947) Fix issue in utils.progress for disable=True (napari/napari/\#5964) Set high DPI attributes when using PySide2 (napari/napari/\#5968) [0.4.18rc1] Bugfix/event proxy (napari/napari/\#5994) Fix behavior of PublicOnlyProxy in setattr, wrapped methods, and calling (napari/napari/\#5997) Bugfix: Fix regression from \#5739 for passing plugin name and reader plus add test (napari/napari/\#6013) Avoid passing empty string to importlib.metadata.metadata (napari/napari/\#6018) Use tuple for pip constraints to avoid LRU cache error (napari/napari\#6036 API Changes Overlays 2.0 (napari/napari/\#4894) expose custom image interpolation kernels (napari/napari/\#5130) Connect image to plane events and expose them (napari/napari/\#5131) Deprecations Build Tools ci(dependabot): bump styfle/cancel-workflow-action from 0.10.0 to 0.10.1 (napari/napari/\#5158) ci(dependabot): bump actions/checkout from 2 to 3 (napari/napari/\#5160) ci(dependabot): bump styfle/cancel-workflow-action from 0.10.1 to 0.11.0 (napari/napari/\#5290) ci(dependabot): bump docker/login-action from 2.0.0 to 2.1.0 (napari/napari/\#5291) ci(dependabot): bump actions/upload-artifact from 2 to 3 (napari/napari/\#5292) Pin mypy version (napari/napari/\#5310) MAINT: Start testing on Python 3.11 in CI. (napari/napari/\#5439) Pin test dependencies (napari/napari/\#5715) Documentation Fix failure on benchmark reporting (napari/napari/\#5083) Add NAP-5: proposal for an updated napari logo (napari/napari/\#5084) DOC Update doc contributing guide (napari/napari/\#5114) Napari debugging during plugin development documentation (napari/napari/\#5142) DOC Update docstring of add\_dock\_widget \& \_add\_viewer\_dock\_widget (napari/napari/\#5173) Specified that the path is to the local folder in contributing documentation guide. (napari/napari/\#5191) Fixes broken links in latest docs version (napari/napari/\#5193) Fixes gallery ToC (napari/napari/\#5458) Fix broken link in EmitterGroup docstring (napari/napari/\#5465) Fix Sphinx warnings (napari/napari/\#5717) Add Fourier transform playground example (napari/napari/\#5872) Improve documentation of changed event in EventedList (napari/napari/\#5928) Set removal version in deprecation of Viewer.rounded\_division (napari/napari/\#5944) Update docs using changes from napari/docs (napari/napari/\#5979) Update CITATION.cff file with 0.4.18 contributors (napari/napari/\#5980) Pre commit fixes for 0.4.18 release branch (napari/napari/\#5985) Port changes from docs repo to main repo for 0.4.18 (napari/napari/\#6002) Add favicon and configuration (napari/docs/\#4) Docs for 5195 from main repository (napari/docs/\#7) Use imshow in getting\_started (napari/docs/\#9) DOC Update viewer.md (napari/docs/\#11) Add and/or update documentation alt text (napari/docs/\#12) Adding documents and images from January 2022 plugin testing workshop. (napari/docs/\#35) Add some more docs about packaging details and conda-forge releases (napari/docs/\#48) Add documentation on using virtual environments for testing in napari based on 2022-01 workshop by Talley Lambert (napari/docs/\#50) Added info for conda installation problems (napari/docs/\#51) add best practices about packaging (napari/docs/\#52) Update viewer tutorial, regarding the console button (napari/docs/\#53) add sample database page (napari/docs/\#56) Fix magicgui objects.inv url for intersphinx (napari/docs/\#58) Fix broken links (napari/docs/\#59) Add sphinx-design cards to Usage landing page (napari/docs/\#63) Update to napari viewer tutorial. (napari/docs/\#65) Added environment creation and doc tools install (napari/docs/\#72) Feature: add copy button for code blocks using sphinx-copybutton (napari/docs/\#76) Add NAP-6 - Proposal for contributable menus (napari/docs/\#77) Update contributing docs for [dev] install change needing Qt backend install (napari/docs/\#78) Update theme related documentation (napari/docs/\#81) Feature: implement python version substitution in conf.py (napari/docs/\#84) Fixes gallery ToC (napari/docs/\#85) Clarify arm64 macOS (Apple Silicon) installation (napari/docs/\#89) Add cards to usage landing pages (napari/docs/\#97) Replace pip with python -m pip (napari/docs/\#100) change blob example to be self contained (napari/docs/\#101) Home page update, take 2 (napari/docs/\#102) Update the 'ensuring correctness' mission clause (napari/docs/\#105) Update steering council listing on website (napari/docs/\#106) Update version switcher json (napari/docs/\#109) Installation: Add libmamba solver to conda Note (napari/docs/\#110) Update requirements and config for sphinx-favicon for 1.0 (napari/docs/\#116) change print to f-string (napari/docs/\#117) Replace non-breaking spaces with regular spaces (napari/docs/\#118) Bugfix: documentation update for napari PR \#5636 (napari/docs/\#123) Add matplotlib image scraper for gallery (napari/docs/\#130) Fix missing links and references (napari/docs/\#133) Update URL of version switcher (napari/docs/\#139) Harmonize release notes to new mandatory labels (napari/docs/\#141) Update installation docs to remove briefcase bundle mentions (napari/docs/\#147) Fix version switcher URL for the latest docs version (napari/docs/\#148) Update Shapes How-To for new Lasso tool (napari/\#5555) (napari/docs/\#149) Fix signpost to make\_napari\_viewer code (napari/docs/\#151) Docs for adding LayerData tuple to viewer (napari/docs/\#152) Update viewer tutorial 3D mode docs (napari/docs/\#159) Add a napari plugin debugging quick start section to the debugging guide (napari/docs/\#161) Pin npe2 version to match installed one (napari/docs/\#175) Add Wouter-Michiel Vierdag to list of core devs (napari/docs/\#181) Update SC information (napari/docs/\#192) Other Pull Requests use app-model for view menu (napari/napari/\#4826) Overlay backend refactor (napari/napari/\#4907) Migrate help menu to use app model (napari/napari/\#4922) Refactor layer slice/dims/view/render state (napari/napari/\#5003) MAINT: increase min numpy version. (napari/napari/\#5089) Refactor qt notification and its test solve problem of segfaults (napari/napari/\#5138) Decouple changing viewer.theme from changing theme settings/preferences (napari/napari/\#5143) [DOCS] misc invalid syntax updates. (napari/napari/\#5176) MAINT: remove vendored colorconv from skimage. (napari/napari/\#5180) Re-add README screenshot (napari/napari/\#5220) MAINT: remove requirements.txt and cache actions based on setup.cfg. (napari/napari/\#5234) Explicitly set test array data to fix a flaky test (napari/napari/\#5245) Add ruff linter to pre-commit (napari/napari/\#5275) Run tests on release branch (napari/napari/\#5277) Vispy 0.12: per-point symbol and several bugfixes (napari/napari/\#5312) Make all imports absolute (napari/napari/\#5318) Fix track ids features ordering for unordered tracks (napari/napari/\#5320) tests: remove private magicgui access in tests (napari/napari/\#5331) Make settings and cache separate per each environment. (napari/napari/\#5333) Remove internal event connection on SelectableEventedList (napari/napari/\#5339) Unset PYTHON* vars and use entitlements in macOS conda menu shortcut (napari/napari/\#5354) Distinguish between update\_dims, extent changes, and refresh (napari/napari/\#5363) Add checks for pending Qt threads and timers in tests (napari/napari/\#5373) Suppress color conversion warning when converting invalid LAB coordinates (napari/napari/\#5386) Fix warning when fail to import qt binding. (napari/napari/\#5388) Update MANIFEST.in to remove warning when run tox (napari/napari/\#5393) [Automatic] Update albertosottile/darkdetect vendored module (napari/napari/\#5394) Update citation metadata (napari/napari/\#5398) Feature: making the Help menu more helpful via weblinks (re-do of \#5094) (napari/napari/\#5399) [pre-commit.ci] pre-commit autoupdate (napari/napari/\#5403) Fix flaky dims playback test by waiting for playing condition (napari/napari/\#5414) [Automatic] Update albertosottile/darkdetect vendored module (napari/napari/\#5416) [pre-commit.ci] pre-commit autoupdate (napari/napari/\#5422) Avoid setting corner pixels for empty layers (napari/napari/\#5423) Maint: Typing and ImportError -{\textgreater} ModuleNotFoundError. (napari/napari/\#5431) Fix tox passenv setup for DISPLAY and XAUTHORITY environment variables (napari/napari/\#5441) Add error color to themes and change application close/exit dialogs (napari/napari/\#5442) Update screenshot in readme (napari/napari/\#5452) Maint: Fix sporadic QtDims garbage collection failures by converting some stray references to weakrefs (napari/napari/\#5471) Replace GabrielBB/xvfb-action (napari/napari/\#5478) Add tags to recently added examples (napari/napari/\#5486) Remove layer ndisplay event (napari/napari/\#5491) MAINT: Don't format logs in log call (napari/napari/\#5504) Replace flake8, isort and pyupgrade by ruff, enable additional usefull rules (napari/napari/\#5513) Second PR that enables more ruff rules. (napari/napari/\#5520) Use pytest-pretty for better log readability (napari/napari/\#5525) MAINT: Follow Nep29, bump minimum numpy. (napari/napari/\#5532) [pre-commit.ci] pre-commit autoupdate (napari/napari/\#5534) Move layer editable change from slicing to controls (napari/napari/\#5546) update conda\_menu\_config.json for latest fixes in menuinst (napari/napari/\#5564) Enable the COM and SIM rules in ruff configuration (napari/napari/\#5566) Move from ubuntu 18.04 to ubuntu 20.04 in workflows (napari/napari/\#5578) FIX: Fix --pre skimage that have a more precise warning message (napari/napari/\#5580) Remove leftover duplicated code (napari/napari/\#5586) Remove napari-hub API access code (napari/napari/\#5587) Enable ruff rules part 4. (napari/napari/\#5590) [pre-commit.ci] pre-commit autoupdate (napari/napari/\#5592) Maint: ImportError -{\textgreater} ModuleNotFoundError. (napari/napari/\#5628) [pre-commit.ci] pre-commit autoupdate (napari/napari/\#5645) MAINT: Do not use mutable default for dataclass. (napari/napari/\#5647) MAINT: Do not use cgi-traceback on 3.11+ (deprecated, marked for removal) (napari/napari/\#5648) MAINT: Add explicit level to warn. (napari/napari/\#5649) MAINT: Split test file in two to find hanging test. (napari/napari/\#5680) Skip pyside6 version 6.4.3 for tests (napari/napari/\#5683) Pin pydantic. (napari/napari/\#5695) fix test\_viewer\_open\_no\_plugin exception message expectation (napari/napari/\#5698) fix: Block PySide6==6.5.0 in tests (napari/napari/\#5702) Don't resize shape after Shift release until mouse moves (napari/napari/\#5707) Update test\_examples job dependencies, unskip surface\_timeseries\_.py and update some examples validations (napari/napari/\#5716) Add test to check basic interactions with layer controls widgets (napari/napari/\#5757) test: [Automatic] Constraints upgrades: dask, hypothesis, imageio, npe2, numpy, pandas, psutil, pygments, pytest, rich, tensorstore, tifffile, virtualenv, xarray (napari/napari/\#5776) [MAINT, packaging] Remove support for briefcase installers (napari/napari/\#5804) Update PIP\_CONSTRAINT value to fix failing comprehensive jobs (napari/napari/\#5809) [pre-commit.ci] pre-commit autoupdate (napari/napari/\#5836) [pre-commit.ci] pre-commit autoupdate (napari/napari/\#5860) Fix Dev Docker Container (napari/napari/\#5877) Make mypy error checking opt-out instead of opt-in (napari/napari/\#5885) Update Error description when plugin not installed (napari/napari/\#5899) maint: add fixture to disable throttling (napari/napari/\#5908) Update upgrade dependecies and test workflows (napari/napari/\#5919) [Maint] Fix comprehensive tests by skipping labels controls test on py311 pyqt6 (napari/napari/\#5922) Fix typo in resources/requirements\_mypy.in file name (napari/napari/\#5924) Add Python 3.11 trove classifier. (napari/napari/\#5937) Change license\_file to license\_files in setup.cfg (napari/napari/\#5948) test: [Automatic] Constraints upgrades: dask, fsspec, hypothesis, imageio, ipython, napari-plugin-manager, napari-svg, numpy, psygnal, pydantic, pyqt6, pytest, rich, scikit-image, virtualenv, zarr (napari/napari/\#5963) Update deprecation information (napari/napari/\#5984) Pin napari and pydantic when installing a plugin (napari/napari/\#6022) 40 authors added to this release (alphabetical) Alister Burt - @alisterburt Andrea Pierré - @kir0ul Andrew Sweet - @andy-sweet Ashley Anderson - @aganders3 Clément Caporal - @ClementCaporal Constantin Pape - @constantinpape Craig T. Russell - @ctr26 Daniel Althviz Moré - @dalthviz David Ross - @davidpross David Stansby - @dstansby Gabriel Selzer - @gselzer Gonzalo Peña-Castellanos - @goanpeca Gregor Lichtner - @glichtner Grzegorz Bokota - @Czaki Jaime Rodríguez-Guerra - @jaimergp Jan-Hendrik Müller - @kolibril13 Jannis Ahlers - @jnahlers Jessy Lauer - @jeylau Jonas Windhager - @jwindhager Jordão Bragantini - @JoOkuma Juan Nunez-Iglesias - @jni Jules Vanaret - @jules-vanaret Kabilar Gunalan - @kabilar Katherine Hutchings - @katherine-hutchings Kim Pevey - @kcpevey Konstantin Sofiiuk - @ksofiyuk Kyle I. S. Harrington - @kephale Lorenzo Gaifas - @brisvag Luca Marconato - @LucaMarconato Lucy Liu - @lucyleeow Mark Harfouche - @hmaarrfk Matthias Bussonnier - @Carreau Melissa Weber Mendonça - @melissawm Nadalyn Miller - @Nadalyn-CZI Pam Wadhwa - @ppwadhwa Paul Smith - @p-j-smith Peter Sobolewski - @psobolewskiPhD Sean Martin - @seankmartin Talley Lambert - @tlambert03 Wouter-Michiel Vierdag - @melonora 43 reviewers added to this release (alphabetical) Alan Lowe - @quantumjot Alister Burt - @alisterburt Andrew Sweet - @andy-sweet Ashley Anderson - @aganders3 Charlie Marsh - @charliermarsh Daniel Althviz Moré - @dalthviz David Ross - @davidpross David Stansby - @dstansby Draga Doncila Pop - @DragaDoncila Eric Perlman - @perlman Gabriel Selzer - @gselzer Genevieve Buckley - @GenevieveBuckley Gonzalo Peña-Castellanos - @goanpeca Grzegorz Bokota - @Czaki Isabela Presedo-Floyd - @isabela-pf Jaime Rodríguez-Guerra - @jaimergp Jan-Hendrik Müller - @kolibril13 Jessy Lauer - @jeylau Jordão Bragantini - @JoOkuma Juan Nunez-Iglesias - @jni Jules Vanaret - @jules-vanaret Kevin Yamauchi - @kevinyamauchi Kim Pevey - @kcpevey Kira Evans - @kne42 Konstantin Sofiiuk - @ksofiyuk Kyle I. S. Harrington - @kephale Lorenzo Gaifas - @brisvag Luca Marconato - @LucaMarconato Lucy Liu - @lucyleeow Lucy Obus - @LCObus Mark Harfouche - @hmaarrfk Matthias Bussonnier - @Carreau Melissa Weber Mendonça - @melissawm Nathan Clack - @nclack Nicholas Sofroniew - @sofroniewn Oren Amsalem - @orena1 Pam Wadhwa - @ppwadhwa Paul Smith - @p-j-smith Peter Sobolewski - @psobolewskiPhD Sean Martin - @seankmartin Talley Lambert - @tlambert03 Wouter-Michiel Vierdag - @melonora Ziyang Liu - @liu-ziyang 19 docs authors added to this release (alphabetical) Ashley Anderson - @aganders3 chili-chiu - @chili-chiu Christopher Nauroth-Kreß - @Chris-N-K Curtis Rueden - @ctrueden Daniel Althviz Moré - @dalthviz David Stansby - @dstansby Draga Doncila Pop - @DragaDoncila Grzegorz Bokota - @Czaki Jaime Rodríguez-Guerra - @jaimergp Juan Nunez-Iglesias - @jni Lorenzo Gaifas - @brisvag Lucy Liu - @lucyleeow Matthias Bussonnier - @Carreau Melissa Weber Mendonça - @melissawm Nadalyn Miller - @Nadalyn-CZI Oren Amsalem - @orena1 Peter Sobolewski - @psobolewskiPhD Sean Martin - @seankmartin Wouter-Michiel Vierdag - @melonora 20 docs reviewers added to this release (alphabetical) Alister Burt - @alisterburt Andrew Sweet - @andy-sweet Ashley Anderson - @aganders3 Christopher Nauroth-Kreß - @Chris-N-K David Stansby - @dstansby Draga Doncila Pop - @DragaDoncila Gonzalo Peña-Castellanos - @goanpeca Grzegorz Bokota - @Czaki Jaime Rodríguez-Guerra - @jaimergp Juan Nunez-Iglesias - @jni Kevin Yamauchi - @kevinyamauchi Kira Evans - @kne42 Lorenzo Gaifas - @brisvag Lucy Liu - @lucyleeow Melissa Weber Mendonça - @melissawm Nadalyn Miller - @Nadalyn-CZI Nicholas Sofroniew - @sofroniewn Peter Sobolewski - @psobolewskiPhD Sean Martin - @seankmartin Wouter-Michiel Vierdag - @melonora New Contributors There are 19 new contributors for this release: Christopher Nauroth-Kreß docs - @Chris-N-K Clément Caporal napari - @ClementCaporal Constantin Pape napari - @constantinpape Craig T. Russell napari - @ctr26 Daniel Althviz Moré docs napari - @dalthviz David Ross napari - @davidpross Gregor Lichtner napari - @glichtner Jannis Ahlers napari - @jnahlers Jessy Lauer napari - @jeylau Jules Vanaret napari - @jules-vanaret Kabilar Gunalan napari - @kabilar Katherine Hutchings napari - @katherine-hutchings Konstantin Sofiiuk napari - @ksofiyuk LucaMarconato napari - @LucaMarconato Nadalyn Miller docs napari - @Nadalyn-CZI Oren Amsalem docs - @orena1 Paul Smith napari - @p-j-smith Sean Martin docs napari - @seankmartin Wouter-Michiel Vierdag docs napari - @melonora},
	urldate = {2023-10-05},
	publisher = {Zenodo},
	author = {Ahlers, Jannis and Althviz Moré, Daniel and Amsalem, Oren and Anderson, Ashley and Bokota, Grzegorz and Boone, Peter and Bragantini, Jordão and Buckley, Genevieve and Burt, Alister and Bussonnier, Matthias and Can Solak, Ahmet and Caporal, Clément and Doncila Pop, Draga and Evans, Kira and Freeman, Jeremy and Gaifas, Lorenzo and Gohlke, Christoph and Gunalan, Kabilar and Har-Gil, Hagai and Harfouche, Mark and Harrington, Kyle I. S. and Hilsenstein, Volker and Hutchings, Katherine and Lambert, Talley and Lauer, Jessy and Lichtner, Gregor and Liu, Ziyang and Liu, Lucy and Lowe, Alan and Marconato, Luca and Martin, Sean and McGovern, Abigail and Migas, Lukasz and Miller, Nadalyn and Muñoz, Hector and Müller, Jan-Hendrik and Nauroth-Kreß, Christopher and Nunez-Iglesias, Juan and Pape, Constantin and Pevey, Kim and Peña-Castellanos, Gonzalo and Pierré, Andrea and Rodríguez-Guerra, Jaime and Ross, David and Royer, Loic and Russell, Craig T. and Selzer, Gabriel and Smith, Paul and Sobolewski, Peter and Sofiiuk, Konstantin and Sofroniew, Nicholas and Stansby, David and Sweet, Andrew and Vierdag, Wouter-Michiel and Wadhwa, Pam and Weber Mendonça, Melissa and Windhager, Jonas and Winston, Philip and Yamauchi, Kevin},
	month = jul,
	year = {2023},
	doi = {10.5281/zenodo.8115575},
}

@article{arzt2022,
	title = {{LABKIT}: {Labeling} and {Segmentation} {Toolkit} for {Big} {Image} {Data}},
	volume = {4},
	issn = {2624-9898},
	shorttitle = {{LABKIT}},
	url = {https://www.frontiersin.org/articles/10.3389/fcomp.2022.777728},
	abstract = {We present LABKIT, a user-friendly Fiji plugin for the segmentation of microscopy image data. It offers easy to use manual and automated image segmentation routines that can be rapidly applied to single- and multi-channel images as well as to timelapse movies in 2D or 3D. LABKIT is specifically designed to work efficiently on big image data and enables users of consumer laptops to conveniently work with multiple-terabyte images. This efficiency is achieved by using ImgLib2 and BigDataViewer as well as a memory efficient and fast implementation of the random forest based pixel classification algorithm as the foundation of our software. Optionally we harness the power of graphics processing units (GPU) to gain additional runtime performance. LABKIT is easy to install on virtually all laptops and workstations. Additionally, LABKIT is compatible with high performance computing (HPC) clusters for distributed processing of big image data. The ability to use pixel classifiers trained in LABKIT via the ImageJ macro language enables our users to integrate this functionality as a processing step in automated image processing workflows. Finally, LABKIT comes with rich online resources such as tutorials and examples that will help users to familiarize themselves with available features and how to best use LABKIT in a number of practical real-world use-cases.},
	urldate = {2022-08-30},
	journal = {Frontiers in Computer Science},
	author = {Arzt, Matthias and Deschamps, Joran and Schmied, Christopher and Pietzsch, Tobias and Schmidt, Deborah and Tomancak, Pavel and Haase, Robert and Jug, Florian},
	year = {2022},
}

@misc{russell2024,
	title = {bia-binder: {A} web-native cloud compute service for the bioimage analysis community},
	shorttitle = {bia-binder},
	url = {http://arxiv.org/abs/2411.12662},
	doi = {10.48550/arXiv.2411.12662},
	abstract = {We introduce bia-binder (BioImage Archive Binder), an open-source, cloud-architectured, and web-based coding environment tailored to bioimage analysis that is freely accessible to all researchers. The service generates easy-to-use Jupyter Notebook coding environments hosted on EMBL-EBI's Embassy Cloud, which provides significant computational resources. The bia-binder architecture is free, open-source and publicly available for deployment. It features fast and direct access to images in the BioImage Archive, the Image Data Resource, and the BioStudies databases. We believe that this service can play a role in mitigating the current inequalities in access to scientific resources across academia. As bia-binder produces permanent links to compiled coding environments, we foresee the service to become widely-used within the community and enable exploratory research. bia-binder is built and deployed using helmsman and helm and released under the MIT licence. It can be accessed at binder.bioimagearchive.org and runs on any standard web browser.},
	urldate = {2025-04-09},
	publisher = {arXiv},
	author = {Russell, Craig T. and Burel, Jean-Marie and Athar, Awais and Li, Simon and Sarkans, Ugis and Swedlow, Jason and Brazma, Alvis and Hartley, Matthew and Uhlmann, Virginie},
	month = nov,
	year = {2024},
	note = {arXiv:2411.12662 [q-bio]},
	keywords = {Quantitative Biology - Quantitative Methods},
}

@misc{cardoso2022,
	title = {{MONAI}: {An} open-source framework for deep learning in healthcare},
	shorttitle = {{MONAI}},
	url = {http://arxiv.org/abs/2211.02701},
	doi = {10.48550/arXiv.2211.02701},
	abstract = {Artificial Intelligence (AI) is having a tremendous impact across most areas of science. Applications of AI in healthcare have the potential to improve our ability to detect, diagnose, prognose, and intervene on human disease. For AI models to be used clinically, they need to be made safe, reproducible and robust, and the underlying software framework must be aware of the particularities (e.g. geometry, physiology, physics) of medical data being processed. This work introduces MONAI, a freely available, community-supported, and consortium-led PyTorch-based framework for deep learning in healthcare. MONAI extends PyTorch to support medical data, with a particular focus on imaging, and provide purpose-specific AI model architectures, transformations and utilities that streamline the development and deployment of medical AI models. MONAI follows best practices for software-development, providing an easy-to-use, robust, well-documented, and well-tested software framework. MONAI preserves the simple, additive, and compositional approach of its underlying PyTorch libraries. MONAI is being used by and receiving contributions from research, clinical and industrial teams from around the world, who are pursuing applications spanning nearly every aspect of healthcare.},
	urldate = {2025-04-09},
	publisher = {arXiv},
	author = {Cardoso, M. Jorge and Li, Wenqi and Brown, Richard and Ma, Nic and Kerfoot, Eric and Wang, Yiheng and Murrey, Benjamin and Myronenko, Andriy and Zhao, Can and Yang, Dong and Nath, Vishwesh and He, Yufan and Xu, Ziyue and Hatamizadeh, Ali and Myronenko, Andriy and Zhu, Wentao and Liu, Yun and Zheng, Mingxin and Tang, Yucheng and Yang, Isaac and Zephyr, Michael and Hashemian, Behrooz and Alle, Sachidanand and Darestani, Mohammad Zalbagi and Budd, Charlie and Modat, Marc and Vercauteren, Tom and Wang, Guotai and Li, Yiwen and Hu, Yipeng and Fu, Yunguan and Gorman, Benjamin and Johnson, Hans and Genereaux, Brad and Erdal, Barbaros S. and Gupta, Vikash and Diaz-Pinto, Andres and Dourson, Andre and Maier-Hein, Lena and Jaeger, Paul F. and Baumgartner, Michael and Kalpathy-Cramer, Jayashree and Flores, Mona and Kirby, Justin and Cooper, Lee A. D. and Roth, Holger R. and Xu, Daguang and Bericat, David and Floca, Ralf and Zhou, S. Kevin and Shuaib, Haris and Farahani, Keyvan and Maier-Hein, Klaus H. and Aylward, Stephen and Dogra, Prerna and Ourselin, Sebastien and Feng, Andrew},
	month = nov,
	year = {2022},
	note = {arXiv:2211.02701 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{follain2024,
	title = {Fast label-free live imaging reveals key roles of flow dynamics and {CD44}-{HA} interaction in cancer cell arrest on endothelial monolayers},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2024.09.30.615654v1},
	doi = {10.1101/2024.09.30.615654},
	abstract = {Leukocyte extravasation is a key component of the innate immune response, while circulating tumor cell extravasation is a critical step in metastasis formation. Despite their importance, the mechanisms underlying leukocyte and tumor cell extravasation remain incompletely understood. Here, we developed an imaging pipeline that integrates fast, label-free live-cell imaging with deep learning-based image analysis to quantitatively track and compare the initial steps of extravasation—cell landing and arrest on an endothelial monolayer—under physiological flow conditions. We find that pancreatic ductal adenocarcinoma (PDAC) cells exhibit variable adhesion strength and flow sensitivity. Remarkably, some PDAC cells demonstrate comparable endothelial engagement as leukocytes, preferentially arresting at endothelial junctions, potentially due to increased stiffness at these sites, which leads to exposure to the underlying basal extracellular matrix. PDAC cells also tend to cluster in regions with high, heterogeneous expression of the endothelial CD44 receptor. Simulations suggest that clustering results from the combination of CD44-mediated attachment and localized flow disturbances that facilitate subsequent cell attachment. Targeting CD44 using siRNA or function-blocking antibodies, or degrading its ligand hyaluronic acid (HA), almost completely abolishes PDAC cell attachment. Overall, we demonstrate that cancer and immune cells share both common and unique features in endothelial adhesion under flow, and identify CD44 and HA as key mediators of PDAC cell arrest.},
	language = {en},
	urldate = {2025-03-21},
	publisher = {bioRxiv},
	author = {Follain, Gautier and Ghimire, Sujan and Pylvänäinen, Joanna W. and Vaitkevičiūtė, Monika and Wurzinger, Diana and Guzmán, Camilo and Conway, James RW and Dibus, Michal and Oikari, Sanna and Rilla, Kirsi and Salmi, Marko and Ivaska, Johanna and Jacquemet, Guillaume},
	month = oct,
	year = {2024},
	note = {Pages: 2024.09.30.615654
Section: New Results},
}

@article{hidalgo-cenalmor2024,
	title = {{DL4MicEverywhere}: deep learning for microscopy made flexible, shareable and reproducible},
	issn = {1548-7091, 1548-7105},
	shorttitle = {{DL4MicEverywhere}},
	url = {https://www.nature.com/articles/s41592-024-02295-6},
	doi = {10.1038/s41592-024-02295-6},
	language = {en},
	urldate = {2024-05-20},
	journal = {Nature Methods},
	author = {Hidalgo-Cenalmor, Iván and Pylvänäinen, Joanna W. and G. Ferreira, Mariana and Russell, Craig T. and Saguy, Alon and Arganda-Carreras, Ignacio and Shechtman, Yoav and {AI4Life Horizon Europe Program Consortium} and Muñoz-Barrutia, Arrate and Serrano-Solano, Beatriz and Barcelo, Caterina Fuster and Pape, Constantin and Lundberg, Emma and Jug, Florian and Deschamps, Joran and Ferreira, Mariana G. and Hartley, Matthew and Seifi, Mehdi and Zulueta-Coarasa, Teresa and Galinova, Vera and Ouyang, Wei and Jacquemet, Guillaume and Henriques, Ricardo and Gómez-de-Mariscal, Estibaliz},
	month = may,
	year = {2024},
}

@inproceedings{ronneberger2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	isbn = {978-3-319-24574-4},
	shorttitle = {U-{Net}},
	doi = {10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
	pages = {234--241},
}

@article{moen2019,
	title = {Deep learning for cellular image analysis},
	volume = {16},
	copyright = {2019 Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-019-0403-1},
	doi = {10.1038/s41592-019-0403-1},
	abstract = {Recent advances in computer vision and machine learning underpin a collection of algorithms with an impressive ability to decipher the content of images. These deep learning algorithms are being applied to biological images and are transforming the analysis and interpretation of imaging data. These advances are positioned to render difficult analyses routine and to enable researchers to carry out new, previously impossible experiments. Here we review the intersection between deep learning and cellular image analysis and provide an overview of both the mathematical mechanics and the programming frameworks of deep learning that are pertinent to life scientists. We survey the field’s progress in four key applications: image classification, image segmentation, object tracking, and augmented microscopy. Last, we relay our labs’ experience with three key aspects of implementing deep learning in the laboratory: annotating training data, selecting and training a range of neural network architectures, and deploying solutions. We also highlight existing datasets and implementations for each surveyed application.},
	language = {en},
	number = {12},
	urldate = {2023-11-30},
	journal = {Nature Methods},
	author = {Moen, Erick and Bannon, Dylan and Kudo, Takamasa and Graf, William and Covert, Markus and Van Valen, David},
	month = dec,
	year = {2019},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Image processing, Software},
	pages = {1233--1246},
}

@article{pylvanainen2023,
	title = {Live-cell imaging in the deep learning era},
	volume = {85},
	issn = {09550674},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0955067423001205},
	doi = {10.1016/j.ceb.2023.102271},
	language = {en},
	urldate = {2023-11-07},
	journal = {Current Opinion in Cell Biology},
	author = {Pylvänäinen, Joanna W. and Gómez-de-Mariscal, Estibaliz and Henriques, Ricardo and Jacquemet, Guillaume},
	month = dec,
	year = {2023},
	pages = {102271},
}

@article{bannon2021,
	title = {{DeepCell} {Kiosk}: scaling deep learning–enabled cellular image analysis with {Kubernetes}},
	volume = {18},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {{DeepCell} {Kiosk}},
	url = {https://www.nature.com/articles/s41592-020-01023-0},
	doi = {10.1038/s41592-020-01023-0},
	abstract = {Deep learning is transforming the analysis of biological images, but applying these models to large datasets remains challenging. Here we describe the DeepCell Kiosk, cloud-native software that dynamically scales deep learning workflows to accommodate large imaging datasets. To demonstrate the scalability and affordability of this software, we identified cell nuclei in 106 1-megapixel images in {\textasciitilde}5.5 h for {\textasciitilde}US\$250, with a cost below US\$100 achievable depending on cluster configuration. The DeepCell Kiosk can be downloaded at https://github.com/vanvalenlab/kiosk-console; a persistent deployment is available at https://deepcell.org/.},
	language = {en},
	number = {1},
	urldate = {2023-10-25},
	journal = {Nature Methods},
	author = {Bannon, Dylan and Moen, Erick and Schwartz, Morgan and Borba, Enrico and Kudo, Takamasa and Greenwald, Noah and Vijayakumar, Vibha and Chang, Brian and Pao, Edward and Osterman, Erik and Graf, William and Van Valen, David},
	month = jan,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Hardware and infrastructure, Image processing},
	pages = {43--45},
}

@article{bankhead2017,
	title = {{QuPath}: {Open} source software for digital pathology image analysis},
	volume = {7},
	copyright = {2017 The Author(s)},
	issn = {2045-2322},
	shorttitle = {{QuPath}},
	url = {https://www.nature.com/articles/s41598-017-17204-5},
	doi = {10.1038/s41598-017-17204-5},
	abstract = {QuPath is new bioimage analysis software designed to meet the growing need for a user-friendly, extensible, open-source solution for digital pathology and whole slide image analysis. In addition to offering a comprehensive panel of tumor identification and high-throughput biomarker evaluation tools, QuPath provides researchers with powerful batch-processing and scripting functionality, and an extensible platform with which to develop and share new algorithms to analyze complex tissue images. Furthermore, QuPath’s flexible design makes it suitable for a wide range of additional image analysis applications across biomedical research.},
	language = {en},
	number = {1},
	urldate = {2023-10-11},
	journal = {Scientific Reports},
	author = {Bankhead, Peter and Loughrey, Maurice B. and Fernández, José A. and Dombrowski, Yvonne and McArt, Darragh G. and Dunne, Philip D. and McQuaid, Stephen and Gray, Ronan T. and Murray, Liam J. and Coleman, Helen G. and James, Jacqueline A. and Salto-Tellez, Manuel and Hamilton, Peter W.},
	month = dec,
	year = {2017},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Biomarkers, Cancer imaging, Colon cancer, Image processing, Software},
	pages = {16878},
}

@article{laine2021,
	title = {Avoiding a replication crisis in deep-learning-based bioimage analysis},
	volume = {18},
	issn = {1548-7091},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7611896/},
	doi = {10.1038/s41592-021-01284-3},
	abstract = {Deep learning algorithms are powerful tools to analyse, restore and transform bioimaging data, increasingly used in life sciences research. These approaches now outperform most other algorithms for a broad range of image analysis tasks. In particular, one of the promises of deep learning is the possibility to provide parameter-free, one-click data analysis achieving expert-level performances in a fraction of the time previously required. However, as with most new and upcoming technologies, the potential for inappropriate use is raising concerns among the biomedical research community. This perspective aims to provide a short overview of key concepts that we believe are important for researchers to consider when using deep learning for their microscopy studies. These comments are based on our own experience gained while optimising various deep learning tools for bioimage analysis and discussions with colleagues from both the developer and user community. In particular, we focus on describing how results obtained using deep learning can be validated and discuss what should, in our views, be considered when choosing a suitable tool. We also suggest what aspects of a deep learning analysis would need to be reported in publications to describe the use of such tools to guarantee that the work can be reproduced. We hope this perspective will foster further discussion between developers, image analysis specialists, users and journal editors to define adequate guidelines and ensure that this transformative technology is used appropriately.},
	number = {10},
	urldate = {2023-10-06},
	journal = {Nature methods},
	author = {Laine, Romain F. and Arganda-Carreras, Ignacio and Henriques, Ricardo and Jacquemet, Guillaume},
	month = oct,
	year = {2021},
	pmid = {34608322},
	pmcid = {PMC7611896},
	pages = {1136--1144},
}

@article{heinrich2021,
	title = {Whole-cell organelle segmentation in volume electron microscopy},
	volume = {599},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03977-3},
	doi = {10.1038/s41586-021-03977-3},
	abstract = {Cells contain hundreds of organelles and macromolecular assemblies. Obtaining a complete understanding of their intricate organization requires the nanometre-level, three-dimensional reconstruction of whole cells, which is only feasible with robust and scalable automatic methods. Here, to support the development of such methods, we annotated up to 35 different cellular organelle classes—ranging from endoplasmic reticulum to microtubules to ribosomes—in diverse sample volumes from multiple cell types imaged at a near-isotropic resolution of 4 nm per voxel with focused ion beam scanning electron microscopy (FIB-SEM)1. We trained deep learning architectures to segment these structures in 4 nm and 8 nm per voxel FIB-SEM volumes, validated their performance and showed that automatic reconstructions can be used to directly quantify previously inaccessible metrics including spatial interactions between cellular components. We also show that such reconstructions can be used to automatically register light and electron microscopy images for correlative studies. We have created an open data and open-source web repository, ‘OpenOrganelle’, to share the data, computer code and trained models, which will enable scientists everywhere to query and further improve automatic reconstruction of these datasets.},
	language = {en},
	number = {7883},
	urldate = {2025-04-17},
	journal = {Nature},
	author = {Heinrich, Larissa and Bennett, Davis and Ackerman, David and Park, Woohyun and Bogovic, John and Eckstein, Nils and Petruncio, Alyson and Clements, Jody and Pang, Song and Xu, C. Shan and Funke, Jan and Korff, Wyatt and Hess, Harald F. and Lippincott-Schwartz, Jennifer and Saalfeld, Stephan and Weigel, Aubrey V.},
	month = nov,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cellular imaging, Data mining, Image processing, Machine learning, Organelles},
	pages = {141--146},
}

@article{liu2025,
	title = {Self-supervised learning reveals clinically relevant histomorphological patterns for therapeutic strategies in colon cancer},
	volume = {16},
	copyright = {2025 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-025-57541-y},
	doi = {10.1038/s41467-025-57541-y},
	abstract = {Self-supervised learning (SSL) automates the extraction and interpretation of histopathology features on unannotated hematoxylin-eosin-stained whole slide images (WSIs). We train an SSL Barlow Twins encoder on 435 colon adenocarcinoma WSIs from The Cancer Genome Atlas to extract features from small image patches (tiles). Leiden community detection groups tiles into histomorphological phenotype clusters (HPCs). HPC reproducibility and predictive ability for overall survival are confirmed in an independent clinical trial (N = 1213 WSIs). This unbiased atlas results in 47 HPCs displaying unique and shared clinically significant histomorphological traits, highlighting tissue type, quantity, and architecture, especially in the context of tumor stroma. Through in-depth analyses of these HPCs, including immune landscape and gene set enrichment analyses, and associations to clinical outcomes, we shine light on the factors influencing survival and responses to treatments of standard adjuvant chemotherapy and experimental therapies. Further exploration of HPCs may unveil additional insights and aid decision-making and personalized treatments for colon cancer patients.},
	language = {en},
	number = {1},
	urldate = {2025-04-17},
	journal = {Nature Communications},
	author = {Liu, Bojing and Polack, Meaghan and Coudray, Nicolas and Claudio Quiros, Adalberto and Sakellaropoulos, Theodore and Le, Hortense and Karimkhan, Afreen and Crobach, Augustinus S. L. P. and van Krieken, J. Han J. M. and Yuan, Ke and Tollenaar, Rob A. E. M. and Mesker, Wilma E. and Tsirigos, Aristotelis},
	month = mar,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cancer imaging, Image processing, Machine learning},
	pages = {2328},
}

@article{moshkov2024,
	title = {Learning representations for image-based profiling of perturbations},
	volume = {15},
	copyright = {2024 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-45999-1},
	doi = {10.1038/s41467-024-45999-1},
	abstract = {Measuring the phenotypic effect of treatments on cells through imaging assays is an efficient and powerful way of studying cell biology, and requires computational methods for transforming images into quantitative data. Here, we present an improved strategy for learning representations of treatment effects from high-throughput imaging, following a causal interpretation. We use weakly supervised learning for modeling associations between images and treatments, and show that it encodes both confounding factors and phenotypic features in the learned representation. To facilitate their separation, we constructed a large training dataset with images from five different studies to maximize experimental diversity, following insights from our causal analysis. Training a model with this dataset successfully improves downstream performance, and produces a reusable convolutional network for image-based profiling, which we call Cell Painting CNN. We evaluated our strategy on three publicly available Cell Painting datasets, and observed that the Cell Painting CNN improves performance in downstream analysis up to 30\% with respect to classical features, while also being more computationally efficient.},
	language = {en},
	number = {1},
	urldate = {2025-04-17},
	journal = {Nature Communications},
	author = {Moshkov, Nikita and Bornholdt, Michael and Benoit, Santiago and Smith, Matthew and McQuin, Claire and Goodman, Allen and Senft, Rebecca A. and Han, Yu and Babadi, Mehrtash and Horvath, Peter and Cimini, Beth A. and Carpenter, Anne E. and Singh, Shantanu and Caicedo, Juan C.},
	month = feb,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning, Phenotypic screening},
	pages = {1594},
}

@inproceedings{caicedo2018,
	title = {Weakly {Supervised} {Learning} of {Single}-{Cell} {Feature} {Embeddings}},
	url = {https://ieeexplore.ieee.org/document/8579068},
	doi = {10.1109/CVPR.2018.00970},
	abstract = {We study the problem of learning representations for single cells in microscopy images to discover biological relationships between their experimental conditions. Many new applications in drug discovery and functional genomics require capturing the morphology of individual cells as comprehensively as possible. Deep convolutional neural networks (CNNs) can learn powerful visual representations, but require ground truth for training; this is rarely available in biomedical profiling experiments. While we do not know which experimental treatments produce cells that look alike, we do know that cells exposed to the same experimental treatment should generally look similar. Thus, we explore training CNNs using a weakly supervised approach that uses this information for feature learning. In addition, the training stage is regularized to control for unwanted variations using mixup or RNNs. We conduct experiments on two different datasets; the proposed approach yields single-cell embeddings that are more accurate than the widely adopted classical features, and are competitive with previously proposed transfer learning approaches.},
	urldate = {2025-04-17},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Caicedo, Juan C. and McQuin, Claire and Goodman, Allen and Singh, Shantanu and Carpenter, Anne E.},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {Biology, Compounds, Feature extraction, Microscopy, Sociology, Statistics, Training},
	pages = {9309--9318},
}

@article{archit2025,
	title = {Segment {Anything} for {Microscopy}},
	volume = {22},
	copyright = {2025 The Author(s)},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-024-02580-4},
	doi = {10.1038/s41592-024-02580-4},
	abstract = {Accurate segmentation of objects in microscopy images remains a bottleneck for many researchers despite the number of tools developed for this purpose. Here, we present Segment Anything for Microscopy (μSAM), a tool for segmentation and tracking in multidimensional microscopy data. It is based on Segment Anything, a vision foundation model for image segmentation. We extend it by fine-tuning generalist models for light and electron microscopy that clearly improve segmentation quality for a wide range of imaging conditions. We also implement interactive and automatic segmentation in a napari plugin that can speed up diverse segmentation tasks and provides a unified solution for microscopy annotation across different microscopy modalities. Our work constitutes the application of vision foundation models in microscopy, laying the groundwork for solving image analysis tasks in this domain with a small set of powerful deep learning models.},
	language = {en},
	number = {3},
	urldate = {2025-04-17},
	journal = {Nature Methods},
	author = {Archit, Anwai and Freckmann, Luca and Nair, Sushmita and Khalid, Nabeel and Hilt, Paul and Rajashekar, Vikas and Freitag, Marei and Teuber, Carolin and Buckley, Genevieve and von Haaren, Sebastian and Gupta, Sagnik and Dengel, Andreas and Ahmed, Sheraz and Pape, Constantin},
	month = mar,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Image processing, Software},
	pages = {579--591},
}

@article{li2018,
	title = {{cC}-{GAN}: {A} {Robust} {Transfer}-{Learning} {Framework} for {HEp}-2 {Specimen} {Image} {Segmentation}},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {{cC}-{GAN}},
	url = {https://ieeexplore.ieee.org/document/8301400},
	doi = {10.1109/ACCESS.2018.2808938},
	abstract = {Human epithelial type 2 (HEp-2) cell images play an important role for the detection of antinuclear autoantibodies in autoimmune diseases. As the HEp-2 cell has hundreds of different patterns, none of currently available HEp-2 datasets contain all of the types. Therefore, existing automatic processing systems for HEp-2 cells, e.g., cell segmentation and classification, needs to be transferred between different data sets. However, the performances of transferred system often dramatically decrease, especially when transferring supervised-approaches, e.g., deep learning network, from large dataset to the small but similar ones. In this paper, a novel transfer-learning framework using generative adversarial networks (cC-GAN) is proposed for robust segmentation of different HEp-2 datasets. The proposed cC-GAN tries to solve the overfitting problem of most deep learning networks and improves their transfer-capacity. An improved U-net, so-called Residual U-net (RU-net), is developed to work as the generator for cC-GAN model. The cC-GAN was first trained and tested using I3A dataset and then directly evaluated using MIVIA dataset, which is much smaller than I3A. The segmentation result demonstrates the excellent transferring-capacity of our cC-GAN framework, i.e., a new state-of-the-art segmentation accuracy of 75.27\% was achieved on MIVIA without finetuning.},
	urldate = {2025-04-17},
	journal = {IEEE Access},
	author = {Li, Yuexiang and Shen, Linlin},
	year = {2018},
	keywords = {Cell segmentation, Computer architecture, fully convolutional network, Gallium nitride, generative adversarial networks, Generators, Image segmentation, Machine learning, Microprocessors, Training},
	pages = {14048--14058},
}

@article{morid2021,
	title = {A scoping review of transfer learning research on medical image analysis using {ImageNet}},
	volume = {128},
	issn = {0010-4825},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482520304467},
	doi = {10.1016/j.compbiomed.2020.104115},
	abstract = {Objective
Employing transfer learning (TL) with convolutional neural networks (CNNs), well-trained on non-medical ImageNet dataset, has shown promising results for medical image analysis in recent years. We aimed to conduct a scoping review to identify these studies and summarize their characteristics in terms of the problem description, input, methodology, and outcome.
Materials and methods
To identify relevant studies, MEDLINE, IEEE, and ACM digital library were searched for studies published between June 1st, 2012 and January 2nd, 2020. Two investigators independently reviewed articles to determine eligibility and to extract data according to a study protocol defined a priori.
Results
After screening of 8421 articles, 102 met the inclusion criteria. Of 22 anatomical areas, eye (18\%), breast (14\%), and brain (12\%) were the most commonly studied. Data augmentation was performed in 72\% of fine-tuning TL studies versus 15\% of the feature-extracting TL studies. Inception models were the most commonly used in breast related studies (50\%), while VGGNet was the common in eye (44\%), skin (50\%) and tooth (57\%) studies. AlexNet for brain (42\%) and DenseNet for lung studies (38\%) were the most frequently used models. Inception models were the most frequently used for studies that analyzed ultrasound (55\%), endoscopy (57\%), and skeletal system X-rays (57\%). VGGNet was the most common for fundus (42\%) and optical coherence tomography images (50\%). AlexNet was the most frequent model for brain MRIs (36\%) and breast X-Rays (50\%). 35\% of the studies compared their model with other well-trained CNN models and 33\% of them provided visualization for interpretation.
Discussion
This study identified the most prevalent tracks of implementation in the literature for data preparation, methodology selection and output evaluation for various medical image analysis tasks. Also, we identified several critical research gaps existing in the TL studies on medical image analysis. The findings of this scoping review can be used in future TL studies to guide the selection of appropriate research approaches, as well as identify research gaps and opportunities for innovation.},
	urldate = {2025-04-17},
	journal = {Computers in Biology and Medicine},
	author = {Morid, Mohammad Amin and Borjali, Alireza and Del Fiol, Guilherme},
	month = jan,
	year = {2021},
	keywords = {Convolutional neural network, ImageNet, Medical imaging, Transfer learning},
	pages = {104115},
}

@article{kochetov2024,
	title = {{UNSEG}: unsupervised segmentation of cells and their nuclei in complex tissue samples},
	volume = {7},
	copyright = {2024 The Author(s)},
	issn = {2399-3642},
	shorttitle = {{UNSEG}},
	url = {https://www.nature.com/articles/s42003-024-06714-4},
	doi = {10.1038/s42003-024-06714-4},
	abstract = {Multiplexed imaging technologies have made it possible to interrogate complex tissue microenvironments at sub-cellular resolution within their native spatial context. However, proper quantification of this complexity requires the ability to easily and accurately segment cells into their sub-cellular compartments. Within the supervised learning paradigm, deep learning-based segmentation methods demonstrating human level performance have emerged. However, limited work has been done in developing such generalist methods within the unsupervised context. Here we present an easy-to-use unsupervised segmentation (UNSEG) method that achieves deep learning level performance without requiring any training data via leveraging a Bayesian-like framework, and nucleus and cell membrane markers. We show that UNSEG is internally consistent and better at generalizing to the complexity of tissue morphology than current deep learning methods, allowing it to unambiguously identify the cytoplasmic compartment of a cell, and localize molecules to their correct sub-cellular compartment. We also introduce a perturbed watershed algorithm for stably and automatically segmenting a cluster of cell nuclei into individual nuclei that increases the accuracy of classical watershed. Finally, we demonstrate the efficacy of UNSEG on a high-quality annotated gastrointestinal tissue dataset we have generated, on publicly available datasets, and in a range of practical scenarios.},
	language = {en},
	number = {1},
	urldate = {2025-04-22},
	journal = {Communications Biology},
	author = {Kochetov, Bogdan and Bell, Phoenix D. and Garcia, Paulo S. and Shalaby, Akram S. and Raphael, Rebecca and Raymond, Benjamin and Leibowitz, Brian J. and Schoedel, Karen and Brand, Rhonda M. and Brand, Randall E. and Yu, Jian and Zhang, Lin and Diergaarde, Brenda and Schoen, Robert E. and Singhi, Aatur and Uttam, Shikhar},
	month = aug,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Gastrointestinal system, Image processing},
	pages = {1--14},
}

@misc{chen2020,
	title = {The {Allen} {Cell} and {Structure} {Segmenter}: a new open source toolkit for segmenting {3D} intracellular structures in fluorescence microscopy images},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	shorttitle = {The {Allen} {Cell} and {Structure} {Segmenter}},
	url = {https://www.biorxiv.org/content/10.1101/491035v2},
	doi = {10.1101/491035},
	abstract = {A continuing challenge in quantitative cell biology is the accurate and robust 3D segmentation of structures of interest from fluorescence microscopy images in an automated, reproducible, and widely accessible manner for subsequent interpretable data analysis. We describe the Allen Cell and Structure Segmenter (Segmenter), a Python-based open source toolkit developed for 3D segmentation of cells and intracellular structures in fluorescence microscope images. This toolkit brings together classic image segmentation and iterative deep learning workflows first to generate initial high-quality 3D intracellular structure segmentations and then to easily curate these results to generate the ground truths for building robust and accurate deep learning models. The toolkit takes advantage of the high-replicate 3D live cell image data collected at the Allen Institute for Cell Science of over 30 endogenous fluorescently tagged human induced pluripotent stem cell (hiPSC) lines. Each cell line represents a different intracellular structure with one or more distinct localization patterns within undifferentiated hiPS cells and hiPSC-derived cardiomyocytes. The Segmenter consists of two complementary elements, a classic image segmentation workflow with a restricted set of algorithms and parameters and an iterative deep learning segmentation workflow. We created a collection of 20 classic image segmentation workflows based on 20 distinct and representative intracellular structure localization patterns as a “lookup table” reference and starting point for users. The iterative deep learning workflow can take over when the classic segmentation workflow is insufficient. Two straightforward “human-in-the-loop” curation strategies convert a set of classic image segmentation workflow results into a set of 3D ground truth images for iterative model training without the need for manual painting in 3D. The deep learning model architectures used in this toolkit were designed and tested specifically for 3D fluorescence microscope images and implemented as readable scripts. The Segmenter thus leverages state of the art computer vision algorithms in an accessible way to facilitate their application by the experimental biology researcher.
We include two useful applications to demonstrate how we used the classic image segmentation and iterative deep learning workflows to solve more challenging 3D segmentation tasks. First, we introduce the ‘Training Assay’ approach, a new experimental-computational co-design concept to generate more biologically accurate segmentation ground truths. We combined the iterative deep learning workflow with three Training Assays to develop a robust, scalable cell and nuclear instance segmentation algorithm, which could achieve accurate target segmentation for over 98\% of individual cells and over 80\% of entire fields of view. Second, we demonstrate how to extend the lamin B1 segmentation model built from the iterative deep learning workflow to obtain more biologically accurate lamin B1 segmentation by utilizing multi-channel inputs and combining multiple ML models. The steps and workflows used to develop these algorithms are generalizable to other similar segmentation challenges. More information, including tutorials and code repositories, are available at allencell.org/segmenter.},
	language = {en},
	urldate = {2025-04-22},
	publisher = {bioRxiv},
	author = {Chen, Jianxu and Ding, Liya and Viana, Matheus P. and Lee, HyeonWoo and Sluezwski, M. Filip and Morris, Benjamin and Hendershott, Melissa C. and Yang, Ruian and Mueller, Irina A. and Rafelski, Susanne M.},
	month = dec,
	year = {2020},
	note = {Pages: 491035
Section: New Results},
}

@article{conrad2023,
	title = {Instance segmentation of mitochondria in electron microscopy images with a generalist deep learning model trained on a diverse dataset},
	volume = {14},
	issn = {2405-4712},
	url = {https://www.sciencedirect.com/science/article/pii/S240547122200494X},
	doi = {10.1016/j.cels.2022.12.006},
	abstract = {Mitochondria are extremely pleomorphic organelles. Automatically annotating each one accurately and precisely in any 2D or volume electron microscopy (EM) image is an unsolved computational challenge. Current deep learning-based approaches train models on images that provide limited cellular contexts, precluding generality. To address this, we amassed a highly heterogeneous ∼1.5 × 106 image 2D unlabeled cellular EM dataset and segmented ∼135,000 mitochondrial instances therein. MitoNet, a model trained on these resources, performs well on challenging benchmarks and on previously unseen volume EM datasets containing tens of thousands of mitochondria. We release a Python package and napari plugin, empanada, to rapidly run inference, visualize, and proofread instance segmentations. A record of this paper’s transparent peer review process is included in the supplemental information.},
	number = {1},
	urldate = {2025-04-22},
	journal = {Cell Systems},
	author = {Conrad, Ryan and Narayan, Kedar},
	month = jan,
	year = {2023},
	keywords = {benchmark, crowdsourcing, deep learning, electron microscopy, image dataset, mitochondria, panoptic, segmentation, volume electron miscroscopy, volume EM},
	pages = {58--71.e5},
}

@article{fisch2024,
	title = {Molecular definition of the endogenous {Toll}-like receptor signalling pathways},
	volume = {631},
	copyright = {2024 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07614-7},
	doi = {10.1038/s41586-024-07614-7},
	abstract = {Innate immune pattern recognition receptors, such as the Toll-like receptors (TLRs), are key mediators of the immune response to infection and central to our understanding of health and disease1. After microbial detection, these receptors activate inflammatory signal transduction pathways that involve IκB kinases, mitogen-activated protein kinases, ubiquitin ligases and other adaptor proteins. The mechanisms that connect the proteins in the TLR pathways are poorly defined. To delineate TLR pathway activities, we engineered macrophages to enable microscopy and proteomic analysis of the endogenous myddosome constituent MyD88. We found that myddosomes form transient contacts with activated TLRs and that TLR-free myddosomes are dynamic in size, number and composition over the course of 24 h. Analysis using super-resolution microscopy revealed that, within most myddosomes, MyD88 forms barrel-like structures that function as scaffolds for effector protein recruitment. Proteomic analysis demonstrated that myddosomes contain proteins that act at all stages and regulate all effector responses of the TLR pathways, and genetic analysis defined the epistatic relationship between these effector modules. Myddosome assembly was evident in cells infected with Listeria monocytogenes, but these bacteria evaded myddosome assembly and TLR signalling during cell-to-cell spread. On the basis of these findings, we propose that the entire TLR signalling pathway is executed from within the myddosome.},
	language = {en},
	number = {8021},
	urldate = {2025-04-22},
	journal = {Nature},
	author = {Fisch, Daniel and Zhang, Tian and Sun, He and Ma, Weiyi and Tan, Yunhao and Gygi, Steven P. and Higgins, Darren E. and Kagan, Jonathan C.},
	month = jul,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Innate immune cells, Innate immunity},
	pages = {635--644},
}

@article{bejarano2024,
	title = {Interrogation of endothelial and mural cells in brain metastasis reveals key immune-regulatory mechanisms},
	volume = {42},
	issn = {1535-6108},
	url = {https://www.sciencedirect.com/science/article/pii/S1535610823004464},
	doi = {10.1016/j.ccell.2023.12.018},
	abstract = {Brain metastasis (BrM) is a common malignancy, predominantly originating from lung, melanoma, and breast cancers. The vasculature is a key component of the BrM tumor microenvironment with critical roles in regulating metastatic seeding and progression. However, the heterogeneity of the major BrM vascular components, namely endothelial and mural cells, is still poorly understood. We perform single-cell and bulk RNA-sequencing of sorted vascular cell types and detect multiple subtypes enriched specifically in BrM compared to non-tumor brain, including previously unrecognized immune regulatory subtypes. We integrate the human data with mouse models, creating a platform to interrogate vascular targets for the treatment of BrM. We find that the CD276 immune checkpoint molecule is significantly upregulated in the BrM vasculature, and anti-CD276 blocking antibodies prolonged survival in preclinical trials. This study provides important insights into the complex interactions between the vasculature, immune cells, and cancer cells, with translational relevance for designing therapeutic interventions.},
	number = {3},
	urldate = {2025-04-22},
	journal = {Cancer Cell},
	author = {Bejarano, Leire and Kauzlaric, Annamaria and Lamprou, Eleni and Lourenco, Joao and Fournier, Nadine and Ballabio, Michelle and Colotti, Roberto and Maas, Roeltje and Galland, Sabine and Massara, Matteo and Soukup, Klara and Lilja, Johanna and Brouland, Jean-Philippe and Hottinger, Andreas F. and Daniel, Roy T. and Hegi, Monika E. and Joyce, Johanna A.},
	month = mar,
	year = {2024},
	keywords = {Brain metastasis, endothelial cells, immune regulation, mural cells, single-cell, vasculature},
	pages = {378--395.e10},
}

@article{rangel_dacosta2024,
	title = {A robust synthetic data generation framework for machine learning in high-resolution transmission electron microscopy ({HRTEM})},
	volume = {10},
	copyright = {2024 The Author(s)},
	issn = {2057-3960},
	url = {https://www.nature.com/articles/s41524-024-01336-0},
	doi = {10.1038/s41524-024-01336-0},
	abstract = {Machine learning techniques are attractive options for developing highly-accurate analysis tools for nanomaterials characterization, including high-resolution transmission electron microscopy (HRTEM). However, successfully implementing such machine learning tools can be difficult due to the challenges in procuring sufficiently large, high-quality training datasets from experiments. In this work, we introduce Construction Zone, a Python package for rapid generation of complex nanoscale atomic structures which enables fast, systematic sampling of realistic nanomaterial structures and can be used as a random structure generator for large, diverse synthetic datasets. Using Construction Zone, we develop an end-to-end machine learning workflow for training neural network models to analyze experimental atomic resolution HRTEM images on the task of nanoparticle image segmentation purely with simulated databases. Further, we study the data curation process to understand how various aspects of the curated simulated data—including simulation fidelity, the distribution of atomic structures, and the distribution of imaging conditions—affect model performance across three benchmark experimental HRTEM image datasets. Using our workflow, we are able to achieve state-of-the-art segmentation performance on these experimental benchmarks and, further, we discuss robust strategies for consistently achieving high performance with machine learning in experimental settings using purely synthetic data. Construction Zone and its documentation are available at https://github.com/lerandc/construction\_zone.},
	language = {en},
	number = {1},
	urldate = {2025-04-22},
	journal = {npj Computational Materials},
	author = {Rangel DaCosta, Luis and Sytwu, Katherine and Groschner, C. K. and Scott, M. C.},
	month = jul,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Atomistic models, Nanoparticles, Transmission electron microscopy},
	pages = {1--11},
}

@article{lin2022,
	title = {A deep learned nanowire segmentation model using synthetic data augmentation},
	volume = {8},
	copyright = {2022 The Author(s)},
	issn = {2057-3960},
	url = {https://www.nature.com/articles/s41524-022-00767-x},
	doi = {10.1038/s41524-022-00767-x},
	abstract = {Automated particle segmentation and feature analysis of experimental image data are indispensable for data-driven material science. Deep learning-based image segmentation algorithms are promising techniques to achieve this goal but are challenging to use due to the acquisition of a large number of training images. In the present work, synthetic images are applied, resembling the experimental images in terms of geometrical and visual features, to train the state-of-art Mask region-based convolutional neural networks to segment vanadium pentoxide nanowires, a cathode material within optical density-based images acquired using spectromicroscopy. The results demonstrate the instance segmentation power in real optical intensity-based spectromicroscopy images of complex nanowires in overlapped networks and provide reliable statistical information. The model can further be used to segment nanowires in scanning electron microscopy images, which are fundamentally different from the training dataset known to the model. The proposed methodology can be extended to any optical intensity-based images of variable particle morphology, material class, and beyond.},
	language = {en},
	number = {1},
	urldate = {2025-04-22},
	journal = {npj Computational Materials},
	author = {Lin, Binbin and Emami, Nima and Santos, David A. and Luo, Yuting and Banerjee, Sarbajit and Xu, Bai-Xiang},
	month = apr,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Characterization and analytical techniques, Imaging techniques, Optical spectroscopy},
	pages = {1--12},
}

@article{shorten2019,
	title = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-019-0197-0},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	number = {1},
	urldate = {2025-04-22},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	month = jul,
	year = {2019},
	keywords = {Big data, Data Augmentation, Deep Learning, GANs, Image data},
	pages = {60},
}

@article{lecun1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	url = {https://ieeexplore.ieee.org/document/726791},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	urldate = {2025-04-22},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	keywords = {Character recognition, Feature extraction, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis},
	pages = {2278--2324},
}

@inproceedings{alibrahim2021,
  author={Alibrahim, Hussain and Ludwig, Simone A.},
  booktitle={2021 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Hyperparameter Optimization: Comparing Genetic Algorithm against Grid Search and Bayesian Optimization}, 
  year={2021},
  volume={},
  number={},
  pages={1551-1559},
  keywords={Training;Machine learning algorithms;Neural networks;Prediction algorithms;Search problems;Time measurement;Bayes methods;Hyperparmeter optimization;Grid Search;Bayesian;Genetic Algorithm},
  doi={10.1109/CEC45853.2021.9504761}
}

@article{ilievski2017, 
  title={Efficient Hyperparameter Optimization for Deep Learning Algorithms Using Deterministic RBF Surrogates}, 
  volume={31}, 
  url={https://ojs.aaai.org/index.php/AAAI/article/view/10647}, 
  DOI={10.1609/aaai.v31i1.10647}, 
  abstractNote={ &lt;p&gt; Automatically searching for optimal hyperparameter configurations is of crucial importance for applying deep learning algorithms in practice. Recently, Bayesian optimization has been proposed for optimizing hyperparameters of various machine learning algorithms. Those methods adopt probabilistic surrogate models like Gaussian processes to approximate and minimize the validation error function of hyperparameter values. However, probabilistic surrogates require accurate estimates of sufficient statistics (e.g., covariance) of the error distribution and thus need many function evaluations with a sizeable number of hyperparameters. This makes them inefficient for optimizing hyperparameters of deep learning algorithms, which are highly expensive to evaluate. In this work, we propose a new deterministic and efficient hyperparameter optimization method that employs radial basis functions as error surrogates. The proposed mixed integer algorithm, called HORD, searches the surrogate for the most promising hyperparameter values through dynamic coordinate search and requires many fewer function evaluations. HORD does well in low dimensions but it is exceptionally better in higher dimensions. Extensive evaluations on MNIST and CIFAR-10 for four deep neural networks demonstrate HORD significantly outperforms the well-established Bayesian optimization methods such as GP, SMAC, and TPE. For instance, on average, HORD is more than 6 times faster than GP-EI in obtaining the best configuration of 19 hyperparameters. &lt;/p&gt; }, 
  number={1}, 
  journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
  author={Ilievski, Ilija and Akhtar, Taimoor and Feng, Jiashi and Shoemaker, Christine}, 
  year={2017}, 
  month={Feb.} 
}


