---
author: 
  - name: Guillaume Jacquemet
    orcid: 0000-0002-9286-920X
    affiliations:
      - name: Turku Bioscience Centre, University of Turku and Åbo Akademi University, FI-20520 Turku, FI
        country: Finland
      - name: Faculty of Science and Engineering, Cell Biology, Åbo Akademi University, FI-20520 Turku, FI
        country: Finland
      - name: InFLAMES Research Flagship Center, University of Turku, FI- 20520, Turku, FI
        country: Finland
  - name: Joanna Pylvänäinen 
    orcid: 0000-0002-3540-5150
    affiliations:
      - name: Turku Bioscience Centre, University of Turku and Åbo Akademi University, FI-20520 Turku, FI
        country: Finland
      - name: Faculty of Science and Engineering, Cell Biology, Åbo Akademi University, FI-20520 Turku, FI
        country: Finland
      - name: InFLAMES Research Flagship Center, University of Turku, FI- 20520, Turku, FI
        country: Finland
  - name: Iván Hidalgo-Cenalmor
    orcid: 0009-0000-8923-568X
    affiliations:
      - name: Turku Bioscience Centre, University of Turku and Åbo Akademi University, FI-20520 Turku, FI
        country: Finland
      - name: Faculty of Science and Engineering, Cell Biology, Åbo Akademi University, FI-20520 Turku, FI
        country: Finland
      - name: InFLAMES Research Flagship Center, University of Turku, FI- 20520, Turku, FI
        country: Finland
      - name: Turku Bioimaging, University of Turku and Åbo Akademi University, FI- 20520 Turku, FI
        country: Finland

subtitle: optionally add a subtitle
---
# Chapter 9: How to Train and Use Deep Learning Models in Microscopy

**Practical considerations to train and use deep learning models in microscopy**

## 1. Introduction 

* The landscape evolves constantly; here, we provide some general principles  
* When to use DL or not?  
* Finding trained models  
* Why and when train your own model?

Table   
What do you want to do \- what can you do \- what do you have to run

## 2. Assessing the performance of a model 

* With the growing availability of trained models, it is essential to start by discussing how to assess the quality of the tools  
* Defining the objective of the image analysis (Image segmentation vs. classification, restoration..) and scope (dataset specific vs general model)  
* Visual inspection and Quality metrics	

## 3. Training a segmentation model 

Choosing a model architecture based on the type of segmentation (Binary, semantic, instance) and dimensionality (2D vs 3D segmentation). Depending on the available computing power these training speed and architecture are something that should be taken into account.

Possible image here to showcase common tasks solved with DL, if not covered in other chapters.

### 3.1 Preparing a training dataset

* Training dataset size (dataset specific vs a more general model)  
* Number of images for training \- Need to have enough labels / masks  
* Manual annotations  
* DL/AI to accelerate and curate training datasets (micro-sam, Cellpose, manual curation)  
* Data augmentation and synthetic data

### 3.2 Training a model from scratch 

* Dataset split (train and validation) \+ data for quality control)  
* Training is an iterative process  
* Key Training parameters  
  * Batch size, epochs, and optimizer selection, learning rate, field of view of the network  
* Understanding the training and validation loss  
  * Overfitting & Underfitting

### 3.3 Fine Tuning an existing model and iterative training 

* Transfer learning  
* Classic fine-tuning  
* Human-in-the-loop fine-tuning

With 3.2 and 3.3 would be nice to include a figure about different ways to train a model.

### 3.4 Troubleshooting common issues 

## 4. Getting started 

### 4.1 Where to use and train DL models 

- Computing power  
- Cloud-based resources  
- Local resources  
- Servers

Figure X: Here we introduce different ways of training and deploying DL models using cloud and local resources. We also give some examples of current tools that could be used here. Image similar to what was in von Chamier, L., Laine, R.F., Jukkala, J. et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nat Commun 12, 2276 (2021). [https://doi.org/10.1038/s41467-021-22518-0](https://doi.org/10.1038/s41467-021-22518-0):

### 4.2. Tools available to use and train DL models

* Tool for Deep learning depending on your comfort zone:  
  * Code  
    * Python libraries  
    * TensorFlow/Keras and PyTorch  
    * Matlab, etc  
  * Notebooks  
    * Biapy  
    * DL4MicEverywhere for microscopy-specific training.  
    * ZeroCostDL4Mic  
  * Graphical interface  
    * CellPose GUI  
    * DeepImageJ

### 4.3. Toward reproducibility in DL 

* A DL model is: a dataset \+ code \+ underlying libraries  
* Docker containers

# 5. Practical Guide: Training a Segmentation Model 

Step-by-step walkthrough of training a deep learning model for segmentation (Stardist or Cellpose). Here we will also have a figure that shows the steps. 

Figure X: Steps of training a segmentation model

### Step 0: Defining the task at hand 

What do you want the DL to do?  
What model would be the best to use?  
Are there existing models?

### Step 1: Preparing Data

Dataset selection (open datasets and in-house data).  
Training data preparation.  
Splitting into training, validation, and test data.

### Step 2: Setting Up the Training Environment

Installing necessary libraries (TensorFlow/PyTorch, OpenCV, Albumentations).  
Running models on Google Colab or Docker for easy setup.

### Step 3: Training and validating the Model

Running training.  
Monitoring loss curves and performance metrics.

### Step 4: Evaluating the Model

Using IoU, F1-score for segmentation assessment for test dataset.  
Fine-tuning for better performance if needed

### Step 5: Applying the model to own data

Applying the trained model to new microscopy images. Assessing the need for retraining  
(Post-processing predictions)

# 6. Summary & Outlook

Key takeaways from training deep learning models in microscopy.  
Future directions in AI-driven microscopy training.  
Additional resources for hands-on learning.

---

# Training and Using Your Own Models
<!--Your first header will be the chapter's upper-level table of contents title.-->
<!--If you'd like to have a subtitle, include it in the Quarto header above -->

Under your first header, include a brief introduction to your chapter.

Starting prompt for this chapter: Chapter 9 discusses the considerations (e.g., over/under-fitting, parameter choices) for training a new model and tools to help getting started (e.g. DL4MicEverywhere). It should include a primer on cloud-based computing tools such as Docker, Google Colab, etc. to highlight the potential for training models without bespoke hardware in house. This chapter should demonstrate a walk-through of training a model for segmentation, following the throughline of the book.

## Include section headers as appropriate

Use markdown heading level two for section headers. You can use standard markdown formatting, for example _emphasize the end of this sentence_.

This is a new paragraph with more text. Your paragraphs can cross reference other items, such as @fig-simple. Use `fig` to reference figures, and `eq` to reference equations, such as @eq-stddev.

###  Sub-subsection headers are also available

To make your sections cross reference-able throughout the book, include a section reference, as shown in the header for @sec-equation.

## Bibliography and Citations

To cite a research article, add it to references.bib and then refer to the citation key. For example, reference @stringer2021 refers to CellPose and reference @von_chamier2021 refers to ZeroCostDL4Mic.

## Adding to the Glossary

We are using the extension [Quarto-glossary](https://debruine.github.io/quarto-glossary/#styles) to create a glossary for this book. To add a definition, edit the glossary.yml file. To reference the glossary, enclose the word as in these examples: LLMs suffer from {{< glossary hallucinations >}}. It is important to understand the underlying {{< glossary "training data" >}} to interpret your results. Clicking on the word will reveal its definition. The complete glossary for the book will be listed in the [Glossary](glossary.qmd).

## Code and Equations {#sec-equation}

This is an example of including a python snippet that generates a figure

```{python}
#| label: fig-simple
#| fig-cap: "Simple Plot"
import matplotlib.pyplot as plt
plt.plot([1,23,2,4])
plt.show()
```


In some cases, you may want to include a code-block that is not executed when the book is compiled. Use the `eval: false` option for this.

```{python}
#| eval: false
import matplotlib.pyplot as plt
plt.plot([1,23,2,4])
plt.show()
```


Figures can also be generated that do not show the code by using the option for `code-fold: true`.

```{python}
#| code-fold: true
#| label: fig-polar
#| fig-cap: "A spiral on a polar axis"
#| fig-alt: "A line plot on a polar axis. The line spirals out from a value of zero to a value of 2."

import numpy as np
import matplotlib.pyplot as plt

r = np.arange(0, 2, 0.01)
theta = 2 * np.pi * r
fig, ax = plt.subplots(
  subplot_kw = {'projection': 'polar'} 
)
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)
plt.show()
```

Here is an example equation.

$$
s = \sqrt{\frac{1}{N-1} \sum_{i=1}^N (x_i - \overline{x})^2}
$$ {#eq-stddev}

### Embedding Figures

You can also embed figures from other notebooks in the repo as shown in the following embed example.


{{< embed ../notebooks/test.ipynb#fig-test-fig echo-true >}}

When embedding notebooks, please store the .ipynb file in the notebook directory. Include the chapter in the name of your file. For example, `chapter4_example_u-net.ipynb`. This is how we will handle chapter- or example-specific environments. We will host notebooks on Google Colab so that any required packages for the code--but not for rendering the book at large--will be installed there. That way, we will not need to handle a global environment across the book.

## Quarto has additional features.

You can learn more about markdown options and additional Quarto features in the [Quarto documentation](https://quarto.org/docs/authoring/markdown-basics.html).  One example that you might find interesting is the option to include callouts in your text. These callouts can be used to highlight potential pitfalls or provide additional optional exercises that the reader might find helpful. Below are examples of the types of callouts available in Quarto.

::: {.callout-note}
Note that there are five types of callouts, including:
`note`, `tip`, `warning`, `caution`, and `important`. They can default to open (like this example) or collapsed (example below).
:::

::: {.callout-tip collapse="true"}
These could be good for extra material or exercises.
:::

::: {.callout-caution}
There are caveats when applying these tools. Expand the code below to learn more.

```{python}
#| code-fold: true
r = np.arange(0, 2, 0.01)
theta = 2 * np.pi * r
```
:::

::: {.callout-warning}
Be careful to avoid hallucinations.
:::

::: {.callout-important}
This is key information.
:::
