[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI in Microscopy: A BioImaging Guide",
    "section": "",
    "text": "Welcome\nThis is an initial outline for the welcome page:\n\nInclude a very brief introduction of the book (this is not the introduction chapter).\nExplain how to interact with the book. For example, explain how code snippets work for the reader, links to figures, glossary terms, etc.\nState the licensing and use restrictions.\nHow to cite the book.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "1-intro.html",
    "href": "1-intro.html",
    "title": "1  Preface",
    "section": "",
    "text": "1.1 Introduction\nThe rapid advancement of microscopy, further fueled by parallel development in other related technologies such as probe chemistry, molecular biology, image analysis methods, has propelled bioimaging to the forefront of life science studies1. In fact, many life scientists now fully expect to visualize intricate biological processes in the context of a whole, living organism – a prospect deemed impossible just a mere decade ago.\nThis seemingly limitless possibility has greatly amplified the complexity of the questions that can be probed. Likewise, it demands a significant degree of forethought in the design of biological experiments2,3 to ensure that the maximal amount of biological information can be extracted from the acquired data. On the other hand, the wealth of information contained in modern bioimage data can now often exceed the grasp of a human observer and thus require computational means to aid in data interpretation. In other words, the need for the ever-expanding and multifaceted field of artificial intelligence (AI) can no longer be separated from bioimaging. Yet the absence of a comprehensive resource to systematically explore how AI can enhance bioimaging prevents many in the community from taking advantage of this indispensable technology. This textbook therefore aims to demystify AI in its many forms and outline proper use cases thereof to facilitate biological discovery. We have specifically contextualized the textbook for our two target audiences: microscopists and biologists. The scientists developing new AI and ML tools often fail to tailor explanations and tutorials for these users who will ultimately facilitate biological discovery. We hope this textbook will bridge the gap.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "1-intro.html#textbook-outline",
    "href": "1-intro.html#textbook-outline",
    "title": "1  Preface",
    "section": "1.2 Textbook Outline",
    "text": "1.2 Textbook Outline\nAs the chapters of this book will collectively demonstrate, AI now plays an indispensable role in the four main stages of the microscopy experimental workflow (Figure 1.1): Project design, tool-building, execution of experiments, and data modeling. To address each part of this workflow, this textbook is divided into three distinct sections.\n\n\n\n\n\n\nFigure 1.1: AI impacts every aspect of the microscopy workflow. The experimental process represents a cycle through four distinct phases: experimental design, constructing systems and workflows, testing hypotheses using these workflows, and analyzing their outputs to develop new models which inform new exeperiments. In the current age, AI holds the capacity to impact each phase of this cycle.\n\n\n\nThe first focuses on introducing the core concepts of AI, large language models (LLMs) and the computational architectures that underpin these technologies. Chapter 2 will introduce the basic principles behind artificial intelligence and machine learning (ML) as a gateway for those less familiar with this topic. In the experimental workflow, modern hypotheses can now drive a project design so complex that artificial intelligence may be required to conceptualize and build the dedicated microscopy tool needed to tackle the experimental challenge. This can be accomplished by using large language models (LLMs) as assistants, as discussed in Chapter 3. Chapter 4 will further explain the construction of AI models and how the accuracy of their outputs can be validated.\nThe second section focuses on extending microscopy hardware with AI. To render a microscope “smart”, i.e., for it to perform an experiment with minimal or no human intervention, ML software first needs to be trained on extensive and high-quality training data. The collection and design of training data is covered in Chapter 5. Chapter 6 will discuss the implementation of AI and ML to enhance the capabilities of existing microscopes through resolution improvement and denoising. Likewise, AI and ML models can be designed to control event-driven image acquisition to visualize biological events that are rare and/or photosensitive (Chapter 7).\nFinally, the third section addresses the use of AI and ML to process and analyze microscopy data in order to extract biological information beyond what is achievable through human observation alone. Chapter 8 will focus on helping readers find and use available open-access tools. When those existing tools are yet insufficient, Chapter 9 seeks to further widen the horizon by discussing the process of training new models for specific image processing and analysis tasks. Finally, Chapter 10 will address the question of “when is my model performing well enough?”. This important step pinpoints potential weaknesses of any model, thereby placing results in meaningful context.\nWe conclude with a chapter that summarizes the core concepts and hopes to inspire readers to begin taking full advantage of the promise of the new era of AI and ML. To close the loop, we also hope that by equipping readers with the proper vocabulary, the book will facilitate more effective communication between biologists and AI tool developers.\n\n\n\n\n\n1. Balasubramanian, H., Hobson, C. M., Chew, T.-L. & Aaron, J. S. Imagining the future of optical microscopy: Everything, everywhere, all at once. Communications Biology 6, 1096 (2023).\n\n\n2. Wait, E. C., Reiche, M. A. & Chew, T.-L. Hypothesis-driven quantitative fluorescence microscopy – the importance of reverse-thinking in experimental design. Journal of Cell Science 133, jcs250027 (2020).\n\n\n3. Lee, R. M., Eisenman, L. R., Khuon, S., Aaron, J. S. & Chew, T.-L. Believing is seeing – the deceptive influence of bias in quantitative microscopy. Journal of Cell Science 137, jcs261567 (2024).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "2-primer.html",
    "href": "2-primer.html",
    "title": "2  AI Primer",
    "section": "",
    "text": "2.1 Include section headers as appropriate\nUnder your first header, include a brief introduction to your chapter.\nStarting prompt for this chapter: Chapter 2 demystifies Artificial Intelligence for microscopy users. It should define terms (e.g., machine/deep learning, supervised/unsupervised learning) without programming details such that an educated scientist without AI experience can understand how these concepts apply to microscopy in life sciences. The use-cases and strengths of different approaches for different applications should be discussed (e.g., contrasting unsupervised clustering vs supervised segmentation). This chapter should broadly introduce image restoration and segmentation, as they will be themes throughout.\nSuggestion from authors’ meetings: This chapter can draw on the outlines from other chapters to introduce key topics for the following chapters.\nUse markdown heading level two for section headers. You can use standard markdown formatting, for example emphasize the end of this sentence.\nThis is a new paragraph with more text. Your paragraphs can cross reference other items, such as Figure 11.1. Use fig to reference figures, and eq to reference equations, such as Equation 11.1.",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI Primer</span>"
    ]
  },
  {
    "objectID": "2-primer.html#include-section-headers-as-appropriate",
    "href": "2-primer.html#include-section-headers-as-appropriate",
    "title": "2  AI Primer",
    "section": "",
    "text": "2.1.1 Sub-subsection headers are also available\nTo make your sections cross reference-able throughout the book, include a section reference, as shown in the header for Section 11.4.",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI Primer</span>"
    ]
  },
  {
    "objectID": "2-primer.html#bibliography-and-citations",
    "href": "2-primer.html#bibliography-and-citations",
    "title": "2  AI Primer",
    "section": "2.2 Bibliography and Citations",
    "text": "2.2 Bibliography and Citations\nTo cite a research article, add it to references.bib and then refer to the citation key. For example, reference1 refers to CellPose and reference2 refers to ZeroCostDL4Mic.",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI Primer</span>"
    ]
  },
  {
    "objectID": "2-primer.html#adding-to-the-glossary",
    "href": "2-primer.html#adding-to-the-glossary",
    "title": "2  AI Primer",
    "section": "2.3 Adding to the Glossary",
    "text": "2.3 Adding to the Glossary\nWe are using R code to create a glossary for this book. To add a definition, edit the glossary.yml file. To reference the glossary, enclose the word as in these examples: LLMs suffer from hallucinations. It is important to understand the underlying training data, validation data and false positives to interpret your results. Clicking on the word will reveal its definition by taking you to the entry on the Glossary page. Pressing back in your browser will return you to your previous place in the textbook.",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI Primer</span>"
    ]
  },
  {
    "objectID": "2-primer.html#sec-equation",
    "href": "2-primer.html#sec-equation",
    "title": "2  AI Primer",
    "section": "2.4 Code and Equations",
    "text": "2.4 Code and Equations\nThis is an example of including a python snippet that generates a figure\n\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\n\n\n\n\n\n\nFigure 2.1: Simple Plot\n\n\n\n\n\nIn some cases, you may want to include a code-block that is not executed when the book is compiled. Use the eval: false option for this.\n\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\nFigures can also be generated that do not show the code by using the option for code-fold: true.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.2: A spiral on a polar axis\n\n\n\n\n\nHere is an example equation.\n\\[\ns = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\overline{x})^2}\n\\tag{2.1}\\]\n\n2.4.1 Embedding Figures\nYou can also embed figures from other notebooks in the repo as shown in the following embed example.\n\n\n\n\n\n\n\n\n\nFigure 2.3: Polar plot of circles of random areas at random coords\n\n\n\n\n\n\nWhen embedding notebooks, please store the .ipynb file in the notebook directory. Include the chapter in the name of your file. For example, chapter4_example_u-net.ipynb. This is how we will handle chapter- or example-specific environments. We will host notebooks on Google Colab so that any required packages for the code–but not for rendering the book at large–will be installed there. That way, we will not need to handle a global environment across the book.",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI Primer</span>"
    ]
  },
  {
    "objectID": "2-primer.html#quarto-has-additional-features.",
    "href": "2-primer.html#quarto-has-additional-features.",
    "title": "2  AI Primer",
    "section": "2.5 Quarto has additional features.",
    "text": "2.5 Quarto has additional features.\nYou can learn more about markdown options and additional Quarto features in the Quarto documentation. One example that you might find interesting is the option to include callouts in your text. These callouts can be used to highlight potential pitfalls or provide additional optional exercises that the reader might find helpful. Below are examples of the types of callouts available in Quarto.\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important. They can default to open (like this example) or collapsed (example below).\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThese could be good for extra material or exercises.\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThere are caveats when applying these tools. Expand the code below to learn more.\n\n\nCode\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe careful to avoid hallucinations.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis is key information.\n\n\n\n\n\n\n1. Stringer, C., Wang, T., Michaelos, M. & Pachitariu, M. Cellpose: A generalist algorithm for cellular segmentation. Nature Methods 18, 100–106 (2021).\n\n\n2. Chamier, L. von et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nature Communications 12, 2276 (2021).",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI Primer</span>"
    ]
  },
  {
    "objectID": "3-llms.html",
    "href": "3-llms.html",
    "title": "3  Large Language Models (LLMs)",
    "section": "",
    "text": "3.1 Foundations of Large Language Models\nIn recent years, large language models (LLMs) have revolutionized how we interact with technology, bringing unprecedented capabilities to scientific research including microscopy. This chapter explores how microscopists can leverage these powerful AI tools to enhance their workflow, from learning concepts to automating analysis tasks. We’ll discuss both general-purpose and microscopy-specific tools while highlighting practical applications and potential pitfalls.\nThis section introduces the fundamental concepts behind modern language models, focusing on transformer architectures that power tools like ChatGPT. We’ll explain how these models function, their capabilities for understanding scientific text, and their emerging role in generating code for image analysis tasks. We’ll demonstrate how microscopists can effectively use LLMs to learn new concepts, troubleshoot methods, and generate starting points for analysis scripts.",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models (LLMs)</span>"
    ]
  },
  {
    "objectID": "3-llms.html#sec-multimodal-ai",
    "href": "3-llms.html#sec-multimodal-ai",
    "title": "3  Large Language Models (LLMs)",
    "section": "3.2 Multi-modal AI: Vision-Language Models and Generative AI",
    "text": "3.2 Multi-modal AI: Vision-Language Models and Generative AI\nMoving beyond text-only interfaces, multi-modal models combine language understanding with visual processing capabilities. This section explores how Vision-Language Models (VLMs) like GPT-4o can “see” and interpret microscopy images, assist with image annotation, and even aid in experimental design. We’ll also cover generative AI technologies including diffusion models that can create synthetic training data, perform style transfer, or convert microscopy images into vector graphics for publications.",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models (LLMs)</span>"
    ]
  },
  {
    "objectID": "3-llms.html#sec-ai-agents",
    "href": "3-llms.html#sec-ai-agents",
    "title": "3  Large Language Models (LLMs)",
    "section": "3.3 AI Agents for Microscopy Workflows",
    "text": "3.3 AI Agents for Microscopy Workflows\nAI agents represent the next evolution - autonomous systems that combine language understanding with specialized scientific knowledge and the ability to execute actions. We’ll examine microscopy-specific tools like Omega and the BioImage.io chatbot that can perform complex bioimage analysis workflows through natural language instructions. This section will explore chain-of-thought reasoning, code generation and execution capabilities, and how these agents use visual feedback to iteratively improve results.",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models (LLMs)</span>"
    ]
  },
  {
    "objectID": "3-llms.html#sec-challenges",
    "href": "3-llms.html#sec-challenges",
    "title": "3  Large Language Models (LLMs)",
    "section": "3.4 Challenges and Limitations",
    "text": "3.4 Challenges and Limitations\nWhile powerful, AI assistants come with significant limitations that microscopists must understand. This section addresses critical challenges including: - hallucinations and factual errors in generated content - The “black box” nature of models and concerns about reproducibility - Alignment problems when tools lack domain-specific knowledge - The need for human validation and the dangers of overreliance - Practical strategies for steering models toward scientifically valid outputs",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models (LLMs)</span>"
    ]
  },
  {
    "objectID": "3-llms.html#sec-future",
    "href": "3-llms.html#sec-future",
    "title": "3  Large Language Models (LLMs)",
    "section": "3.5 Future Directions",
    "text": "3.5 Future Directions\nThe intersection of LLMs and microscopy is rapidly evolving. This final section examines emerging capabilities and future possibilities, including: - Generalist vision-language models capable of performing diverse analysis tasks - Models that can directly transform input images into processed outputs - The integration of AI agents with microscope hardware for fully autonomous imaging - Smart microscopy systems that adapt acquisition parameters based on real-time image understanding - Ethical considerations and best practices for responsible AI adoption in biological research",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models (LLMs)</span>"
    ]
  },
  {
    "objectID": "3-llms.html#sec-practical-guide",
    "href": "3-llms.html#sec-practical-guide",
    "title": "3  Large Language Models (LLMs)",
    "section": "3.6 Practical Guide: Getting Started with LLMs for Microscopy",
    "text": "3.6 Practical Guide: Getting Started with LLMs for Microscopy\nThis hands-on section provides step-by-step guidance for microscopists to begin leveraging LLMs effectively, including: - Crafting effective prompts that produce reliable, scientific outputs - Using ChatGPT and similar tools to learn imaging concepts and generate analysis code - Getting started with BioImage.io tools and microscopy-specific AI agents - Strategies for validating and verifying AI-generated solutions - Example workflows demonstrating LLM integration into real microscopy analysis tasks",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Large Language Models (LLMs)</span>"
    ]
  },
  {
    "objectID": "4-architectures.html",
    "href": "4-architectures.html",
    "title": "4  Architectures and Loss Models",
    "section": "",
    "text": "4.1 Include section headers as appropriate\nUnder your first header, include a brief introduction to your chapter.\nStarting prompt for this chapter: Chapter 4 introduces architectures and loss models, defining them and providing examples through two practical case studies: image restoration and segmentation. Although this chapter will include code snippets/exercises, the presentation of essential concepts should communicate the philosophy behind the choice of a model for non-programmers.\nUse markdown heading level two for section headers. You can use standard markdown formatting, for example emphasize the end of this sentence.\nThis is a new paragraph with more text. Your paragraphs can cross reference other items, such as Figure 11.1. Use fig to reference figures, and eq to reference equations, such as Equation 11.1.",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Architectures and Loss Models</span>"
    ]
  },
  {
    "objectID": "4-architectures.html#include-section-headers-as-appropriate",
    "href": "4-architectures.html#include-section-headers-as-appropriate",
    "title": "4  Architectures and Loss Models",
    "section": "",
    "text": "4.1.1 Sub-subsection headers are also available\nTo make your sections cross reference-able throughout the book, include a section reference, as shown in the header for Section 11.4.",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Architectures and Loss Models</span>"
    ]
  },
  {
    "objectID": "4-architectures.html#bibliography-and-citations",
    "href": "4-architectures.html#bibliography-and-citations",
    "title": "4  Architectures and Loss Models",
    "section": "4.2 Bibliography and Citations",
    "text": "4.2 Bibliography and Citations\nTo cite a research article, add it to references.bib and then refer to the citation key. For example, reference1 refers to CellPose and reference2 refers to ZeroCostDL4Mic.",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Architectures and Loss Models</span>"
    ]
  },
  {
    "objectID": "4-architectures.html#adding-to-the-glossary",
    "href": "4-architectures.html#adding-to-the-glossary",
    "title": "4  Architectures and Loss Models",
    "section": "4.3 Adding to the Glossary",
    "text": "4.3 Adding to the Glossary\nWe are using R code to create a glossary for this book. To add a definition, edit the glossary.yml file. To reference the glossary, enclose the word as in these examples: LLMs suffer from hallucinations. It is important to understand the underlying training data, validation data and false positives to interpret your results. Clicking on the word will reveal its definition by taking you to the entry on the Glossary page. Pressing back in your browser will return you to your previous place in the textbook.",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Architectures and Loss Models</span>"
    ]
  },
  {
    "objectID": "4-architectures.html#sec-equation",
    "href": "4-architectures.html#sec-equation",
    "title": "4  Architectures and Loss Models",
    "section": "4.4 Code and Equations",
    "text": "4.4 Code and Equations\nThis is an example of including a python snippet that generates a figure\n\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\n\n\n\n\n\n\nFigure 4.1: Simple Plot\n\n\n\n\n\nIn some cases, you may want to include a code-block that is not executed when the book is compiled. Use the eval: false option for this.\n\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\nFigures can also be generated that do not show the code by using the option for code-fold: true.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4.2: A spiral on a polar axis\n\n\n\n\n\nHere is an example equation.\n\\[\ns = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\overline{x})^2}\n\\tag{4.1}\\]\n\n4.4.1 Embedding Figures\nYou can also embed figures from other notebooks in the repo as shown in the following embed example.\n\n\n\n\n\n\n\n\n\nFigure 4.3: Polar plot of circles of random areas at random coords\n\n\n\n\n\n\nWhen embedding notebooks, please store the .ipynb file in the notebook directory. Include the chapter in the name of your file. For example, chapter4_example_u-net.ipynb. This is how we will handle chapter- or example-specific environments. We will host notebooks on Google Colab so that any required packages for the code–but not for rendering the book at large–will be installed there. That way, we will not need to handle a global environment across the book.",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Architectures and Loss Models</span>"
    ]
  },
  {
    "objectID": "4-architectures.html#quarto-has-additional-features.",
    "href": "4-architectures.html#quarto-has-additional-features.",
    "title": "4  Architectures and Loss Models",
    "section": "4.5 Quarto has additional features.",
    "text": "4.5 Quarto has additional features.\nYou can learn more about markdown options and additional Quarto features in the Quarto documentation. One example that you might find interesting is the option to include callouts in your text. These callouts can be used to highlight potential pitfalls or provide additional optional exercises that the reader might find helpful. Below are examples of the types of callouts available in Quarto.\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important. They can default to open (like this example) or collapsed (example below).\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThese could be good for extra material or exercises.\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThere are caveats when applying these tools. Expand the code below to learn more.\n\n\nCode\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe careful to avoid hallucinations.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis is key information.\n\n\n\n\n\n\n1. Stringer, C., Wang, T., Michaelos, M. & Pachitariu, M. Cellpose: A generalist algorithm for cellular segmentation. Nature Methods 18, 100–106 (2021).\n\n\n2. Chamier, L. von et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nature Communications 12, 2276 (2021).",
    "crumbs": [
      "Getting Started with AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Architectures and Loss Models</span>"
    ]
  },
  {
    "objectID": "5-training-data.html",
    "href": "5-training-data.html",
    "title": "5  Collecting Training Data",
    "section": "",
    "text": "5.1 Include section headers as appropriate\nUnder your first header, include a brief introduction to your chapter.\nStarting prompt for this chapter: Chapter 5 discusses collecting, annotating and validating training data. It should highlight potential pitfalls such as balanced data sets, out-of-distribution problems, etc. It should also address the question: how do you collect training data on your microscope? For example, this chapter should discuss collecting low/high-laser power pairs for the purpose of training an image restoration model.\nUse markdown heading level two for section headers. You can use standard markdown formatting, for example emphasize the end of this sentence.\nThis is a new paragraph with more text. Your paragraphs can cross reference other items, such as Figure 11.1. Use fig to reference figures, and eq to reference equations, such as Equation 11.1.",
    "crumbs": [
      "Image Acquisition",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Training Data</span>"
    ]
  },
  {
    "objectID": "5-training-data.html#include-section-headers-as-appropriate",
    "href": "5-training-data.html#include-section-headers-as-appropriate",
    "title": "5  Collecting Training Data",
    "section": "",
    "text": "5.1.1 Sub-subsection headers are also available\nTo make your sections cross reference-able throughout the book, include a section reference, as shown in the header for Section 11.4.",
    "crumbs": [
      "Image Acquisition",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Training Data</span>"
    ]
  },
  {
    "objectID": "5-training-data.html#bibliography-and-citations",
    "href": "5-training-data.html#bibliography-and-citations",
    "title": "5  Collecting Training Data",
    "section": "5.2 Bibliography and Citations",
    "text": "5.2 Bibliography and Citations\nTo cite a research article, add it to references.bib and then refer to the citation key. For example, reference1 refers to CellPose and reference2 refers to ZeroCostDL4Mic.",
    "crumbs": [
      "Image Acquisition",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Training Data</span>"
    ]
  },
  {
    "objectID": "5-training-data.html#adding-to-the-glossary",
    "href": "5-training-data.html#adding-to-the-glossary",
    "title": "5  Collecting Training Data",
    "section": "5.3 Adding to the Glossary",
    "text": "5.3 Adding to the Glossary\nWe are using R code to create a glossary for this book. To add a definition, edit the glossary.yml file. To reference the glossary, enclose the word as in these examples: LLMs suffer from hallucinations. It is important to understand the underlying training data, validation data and false positives to interpret your results. Clicking on the word will reveal its definition by taking you to the entry on the Glossary page. Pressing back in your browser will return you to your previous place in the textbook.",
    "crumbs": [
      "Image Acquisition",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Training Data</span>"
    ]
  },
  {
    "objectID": "5-training-data.html#sec-equation",
    "href": "5-training-data.html#sec-equation",
    "title": "5  Collecting Training Data",
    "section": "5.4 Code and Equations",
    "text": "5.4 Code and Equations\nThis is an example of including a python snippet that generates a figure\n\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\n\n\n\n\n\n\nFigure 5.1: Simple Plot\n\n\n\n\n\nIn some cases, you may want to include a code-block that is not executed when the book is compiled. Use the eval: false option for this.\n\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\nFigures can also be generated that do not show the code by using the option for code-fold: true.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.2: A spiral on a polar axis\n\n\n\n\n\nHere is an example equation.\n\\[\ns = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\overline{x})^2}\n\\tag{5.1}\\]\n\n5.4.1 Embedding Figures\nYou can also embed figures from other notebooks in the repo as shown in the following embed example.\n\n\n\n\n\n\n\n\n\nFigure 5.3: Polar plot of circles of random areas at random coords\n\n\n\n\n\n\nWhen embedding notebooks, please store the .ipynb file in the notebook directory. Include the chapter in the name of your file. For example, chapter4_example_u-net.ipynb. This is how we will handle chapter- or example-specific environments. We will host notebooks on Google Colab so that any required packages for the code–but not for rendering the book at large–will be installed there. That way, we will not need to handle a global environment across the book.",
    "crumbs": [
      "Image Acquisition",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Training Data</span>"
    ]
  },
  {
    "objectID": "5-training-data.html#quarto-has-additional-features.",
    "href": "5-training-data.html#quarto-has-additional-features.",
    "title": "5  Collecting Training Data",
    "section": "5.5 Quarto has additional features.",
    "text": "5.5 Quarto has additional features.\nYou can learn more about markdown options and additional Quarto features in the Quarto documentation. One example that you might find interesting is the option to include callouts in your text. These callouts can be used to highlight potential pitfalls or provide additional optional exercises that the reader might find helpful. Below are examples of the types of callouts available in Quarto.\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important. They can default to open (like this example) or collapsed (example below).\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThese could be good for extra material or exercises.\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThere are caveats when applying these tools. Expand the code below to learn more.\n\n\nCode\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe careful to avoid hallucinations.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis is key information.\n\n\n\n\n\n\n1. Stringer, C., Wang, T., Michaelos, M. & Pachitariu, M. Cellpose: A generalist algorithm for cellular segmentation. Nature Methods 18, 100–106 (2021).\n\n\n2. Chamier, L. von et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nature Communications 12, 2276 (2021).",
    "crumbs": [
      "Image Acquisition",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collecting Training Data</span>"
    ]
  },
  {
    "objectID": "6-image-restoration.html",
    "href": "6-image-restoration.html",
    "title": "6  Image Restoration",
    "section": "",
    "text": "6.1 General Concepts in Image Restoration\nFluorescence microscopy is central to biological discovery, enabling the visualization of cellular and subcellular structures with exquisite detail. From observing dynamic intracellular processes to mapping entire tissues, microscopy is indispensable for understanding biological systems1. However, the quality of microscopic images is often compromised due to intrinsic limitations such as noise, optical aberrations, diffraction, and limited signal2. These factors hinder analysis and interpretation, particularly when studying fine biological structures or dynamic processes. For example, by quantifying the intensity of fluorescence signals, researchers infer the abundance or expression levels of specific molecules (e.g. proteins tagged with a genetically expressed marker). Using image segmentation, researchers can analyze the size, shape, distribution and number of specific objects within a defined region3. By tracking the movement of fluorescently labeled molecules or cells over time, researchers can study processes including endocytosis, intracellular transport, and cellular signaling4. In these scenarios, noise and low SNR can make it difficult to distinguish between the actual signal and background; and spatial blurring causes fluorescence signal to spread out, confounding the ability to accurately assign pixels to specific regions or structures.\nMany of these problems can be overcome with suitable hardware or by using advanced imaging techniques. To improve spatial resolution, super-resolution techniques (stimulated emission depletion (STED), structured illumination microscopy (SIM), or single-molecule localization microscopy (SMLM)) may be employed5. To suppress noise and maximize the collection of useful signal, highly sensitive detectors, such as cooled charge-coupled devices (CCDs) or complementary metal-oxide-semiconductor (CMOS) sensors can be used. To correct optical distortions, advanced microscopes integrate adaptive optics (AO)6,7, which use real-time feedback to dynamically adjust the focus and compensate for aberrations. However, these technologies often come with high cost, require complex operation, or need extensive maintenance. Additionally, improving one attribute of the image (e.g. spatial resolution) often results in a compromise in another (e.g., temporal resolution), requiring careful consideration of the specific needs of a study and the resources available8.\nAnother way to address these limitations is to develop image restoration techniques that enhance the quality of microscopic images, often without expensive instrumentation. Traditionally, these techniques relied on mathematical algorithms and physical models of the imaging process. However, recent advances in artificial intelligence (AI), particularly deep learning, have transformed image restoration by enabling data-driven approaches that offer improved performance and flexibility. Manufacturers have increasingly integrated image restoration technologies as an essential component of their products to ensure their users can obtain clearer images directly from the microscope, e.g., Leica THUNDER Imager, Olympus cellSens, Andor iQ, ZEISS arivis, and Nikon NIS-Elements.\nIn this chapter, we provide a practical overview of image restoration for applications in fluorescence microscopy. We begin by introducing the general concept of image restoration, then describe traditional and deep learning-based approaches. Next, we review key technologies and advances by categorizing restoration into four major areas: denoising, deconvolution, deaberration, and resolution enhancement. Finally, we provide practical guidelines for implementing image restoration, including step-by-step workflows and test datasets for interested readers to practice applying these methods.\nImage restoration is the application of mathematical and computational techniques aimed at improving the quality of an image by reversing or reducing the effects of degradation that may occur during the imaging process. These degradations stem from noise, blur, aberrations, and other artifacts. The goal of the restoration is to recover or estimate the latent (true) image from the degraded raw data. Image restoration has the potential not only to improve the precision of biological analyses but also to expand the capabilities of existing microscopes, enabling researchers using basic hardware to achieve advanced imaging quality.",
    "crumbs": [
      "Image Acquisition",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Image Restoration</span>"
    ]
  },
  {
    "objectID": "6-image-restoration.html#general-concepts-in-image-restoration",
    "href": "6-image-restoration.html#general-concepts-in-image-restoration",
    "title": "6  Image Restoration",
    "section": "",
    "text": "6.1.1 Image Degradation Model\nUnderstanding the types of distortions and the causes behind them is the first step in image restoration. This could involve a mathematical model that characterizes how an image is corrupted, using the concept that any real-world imaging system cannot capture an object perfectly. Instead, the imaging system or sample introduces imperfections, which distort the image. In fluorescence microscopy, a general image degradation model can be written as:\n\\[\ni = o \\otimes f ( \\phi ) + n\n\\tag{6.1}\\]\nWhere \\(i\\) is the image acquired by the microscope; \\(o\\) is the intensity distribution of the biological sample; \\(f(\\phi)\\) is the point spread function (PSF) of the system; \\(\\phi\\) is the phase aberration or wavefront distortion (when \\(\\phi=0\\), the wavefront is ‘flat’ or aberration-free and \\(f\\) is the ideal PSF predicted by theory); \\(\\bigotimes\\) is the convolution operator; and \\(n\\) models noise contamination. Two common types of noise are Gaussian noise and Poisson noise. Gaussian noise is caused by random fluctuations in the image due to imperfections in the camera sensor or electronic interference. Poisson noise arises from the random nature of photon detection, meaning that the number of photons detected can vary from one measurement to the next, especially at low light levels. Equation 6.1 is described schematically in Figure 6.1.\n\n\n\n\n\n\nFigure 6.1: Schematic of image degradation model for fluorescence microscopy. Object information passes through the imaging system with degradation due to blurring from the point spread function (PSF, here denoted by the convolution kernel \\(f\\)) and noise. In an ideal case, there is no distortion, and the wavefront associated with a point source maintains a flat phase at the back focal plane of the microscope objective. In any real situation, wavefront distortion due to optical imperfections or heterogeneity within the biological sample results in an aberrated wavefront (indicated by \\(\\phi\\)) and PSF, which results in additional blurring. If spatial resolution is enhanced, e.g., by super-resolution methods (SR), the PSF becomes smaller, resulting in sharper images. Super-resolution imaging is also (typically more) prone to aberrations than conventional diffraction-limited imaging, and noise is inherent to any imaging process.\n\n\n\n\n\n6.1.2 Image Restoration with Traditional Approaches\nTraditional microscopy image restoration techniques use mathematical modeling of the imaging model and statistical analyses that attempt to reverse or mitigate image degradations, and/or additional microscope hardware. We find it helpful to categorize image restoration into four main areas: denoising, deconvolution, deaberration, and resolution enhancement.\n\n6.1.2.1 Denoising\nTraditional denoising methods try to remove noise while preserving important features such as edges and fine structures. Many denoising algorithms assume Gaussian noise, for computational tractability. Gaussian noise removal can be achieved by filtering-based methods in the spatial or frequency domain, such as Gaussian filtering, mean filtering, wavelet filtering, bilateral filtering9, and non-local-based BM3D10. Simple linear filters are easy to implement, but they cause a loss in high-frequency information. Complex denoising methods require careful parameter design and are usually computationally intensive. In many cases, the Gaussian noise model provides a good approximation, but Poisson noise is also a key source of noise for fluorescence microscopy given the quantized nature of fluorescence emission, especially under low signal conditions when detector noise is minimal. One method for dealing with Poisson noise11 directly incorporates its statistics, e.g. the PURE-LET method12. Alternatively, a nonlinear variance-stabilizing transformation (VST)13 can be used to convert the Poisson denoising problem into a Gaussian denoising problem. There are several free and open-source resources available for microscopy image denoising, including filters and the PureDeNoise14 plugin in ImageJ/Fiji, and the scikit-image library in Python15.\n\n\n6.1.2.2 Deconvolution\nDeconvolution is the process of reversing optical blur introduced by the PSF of the microscope (\\(f(\\phi)\\) in Figure 6.1) and is most often implemented without considering optical aberrations. It improves effective contrast and resolution by accounting for such blur and reassigning the relevant signal to its most likely location, given the blurring and noise models. Traditional deconvolution methods include frequency-domain algorithms like naive inverse filtering and Wiener filtering16,17, optimization-based algorithms like Tikhonov regularization18, and iterative deconvolution methods like the Tikhonov-Miller algorithm19, fast iterative soft-thresholding algorithm20, and Richardson-Lucy deconvolution21,22. Traditional deconvolution methods can require significant computational resources, especially for large datasets with complex blurring functions, and also can amplify noise23–25. Commercial deconvolution software includes Huygens, DeltaVison Deconvolution, and AutoQuant. There are also open-source deconvolution plugins integrated into Fiji26 (e.g., DeconvolutionLab227) or MIPAV28 (Medical Image Processing, Analysis, and Visualization, https://mipav.cit.nih.gov/) programs.\n\n\n6.1.2.3 Deaberration\nAberrations refer to the optical imperfections or distortions that occur during image acquisition, degrading the quality of the captured image. Such aberrations can arise due to optical path length differences introduced anywhere in the imaging path, including instrument misalignment, optical imperfections, or differences in refractive index between the heterogeneous and refractile sample, immersion media, and/or objective immersion oil. Aberrations significantly alter the wavefront of light. When the wavefront is distorted, the light rays that are focused by the microscope do not converge as they ideally should. This distortion can lead to various image quality issues, such as loss of resolution, and distortion of fine features. Adaptive optics6,29 can mitigate aberrations by measuring wavefront distortions and subsequently compensating for them using a deformable mirror or other optical elements. However, implementing AO is nontrivial, often requiring additional control algorithms and new hardware, adding considerable expense to the underlying microscope.\n\n\n6.1.2.4 Resolution Enhancement\nOptical super-resolution techniques, like STED, PALM, SIM, etc., bypass the diffraction limit by utilizing on/off fluorophore state transitions30, sophisticated hardware designs, and/or image reconstruction algorithms4,31,32. Alternatively, physical expansion of the sample can be used with conventional microscopes33,34. While effective, all these methods present some tradeoff for the gain in spatial resolution, including increased acquisition time, additional illumination dose, specially designed probes, or more complex instrumentation35,36.\n\n\n6.1.2.5 Drawbacks\nAlthough traditional image restoration algorithms are used extensively in all these categories, they suffer several drawbacks. For example, many methods require careful and manual parameter tuning, and they often perform poorly in challenging conditions, such as in the presence of high noise, low contrast, defocus, or complex background. This is often because they are based on idealized assumptions (Equation 6.1), which are often not met in practice. Another limitation of traditional restoration methods is that they are generally ‘content unaware’ and do not use sample-specific prior information (e.g., shape, size, intensity distributions). On the one hand, this means that traditional algorithms generalize well, but on the other hand, they are not as performant as newer deep learning methods. Deep learning methods can learn a task such as denoising from the data themselves or provide a sample-specific prior37, and thus can outperform traditional methods in many cases.\n\n\n\n6.1.3 Image Restoration with Deep Learning-Based Approaches\nArtificial intelligence (AI), particularly deep learning, has achieved remarkable progress in recent years, leading to breakthroughs in image restoration tasks. Deep learning uses artificial neural networks with multiple layers to model and learn complex patterns in data through end-to-end training, eliminating the need for handcrafted features or manual parameter tuning. By leveraging large datasets and computational power, deep learning can capture non-linear relationships and subtle details within image data, making it particularly well-suited for image restoration. Techniques such as convolutional neural networks (CNNs), generative adversarial networks (GANs), autoencoders, and transfer learning have been shown to tackle applications in denoising, deblurring, super-resolution, and deaberration. Recent advances, such as content-aware restoration (CARE)38, residual channel attention networks (RCANs)39, and deep Fourier-based models40, have demonstrated improvements in image quality while reducing phototoxicity and photobleaching during acquisition. These models offer several advantages over more traditional methods of image restoration:\n\nAutomatic Feature Learning: Traditional image restoration methods rely heavily on manually designed features (e.g., regularization selection) and parameter tuning, which often require fine adjustments for different types of images. Deep learning models, on the other hand, can automatically learn features from large datasets, eliminating the need for manual intervention. This significantly reduces the complexity of parameter tuning and makes the models more adaptable to complex samples or tasks.\nStrong Non-Linear Modeling Capabilities: Traditional algorithms are often based on linear image degradation models (Equation 6.1), which ignore sample-specific information and perform poorly in the presence of complex image distortions or noise. However, recovering the object structure from the acquired image is an inherently ill-posed and nonlinear problem due to noise and blurring. Deep learning uses nonlinear activation functions (ReLU, sigmoid) and hierarchical layers to approximate complex relationships in the image data. Thus, deep learning can better model the global context of an image, allowing for more accurate restoration of image details under challenging and suboptimal imaging conditions. This can extend the use of existing hardware beyond its original use to new biological questions that were previously inaccessible.\nEfficient Handling of Complex Scenes: Deep learning models are typically trained on large datasets and are capable of handling complex scenes (e.g., images contaminated with noise and aberrations41) and large-scale data (e.g., GB- or TB-scale time-lapse data). Once a model is trained, it can automatically adapt to new input data with similar acquisition parameters, which makes it much more efficient for large-scale image restoration tasks.\n\nAs deep learning models continue to evolve and integrate with advanced microscopy hardware and large datasets, they are expected to further push the boundaries of biological imaging and discovery. In the next section, we will introduce the application of deep learning methods in different image restoration tasks.",
    "crumbs": [
      "Image Acquisition",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Image Restoration</span>"
    ]
  },
  {
    "objectID": "6-image-restoration.html#deep-learning-based-techniques-for-image-restoration",
    "href": "6-image-restoration.html#deep-learning-based-techniques-for-image-restoration",
    "title": "6  Image Restoration",
    "section": "6.2 Deep Learning-Based Techniques for Image Restoration",
    "text": "6.2 Deep Learning-Based Techniques for Image Restoration\nFollowing the convention from Section 6.1.2, we group AI-based restoration into four broad categories.\n\n6.2.1 Denoising\nTraditional denoising techniques struggle to preserve fine details while removing noise40,42,43. Deep learning-based methods, on the other hand, can learn to predict what fine details look like even in the presence of noise. Such methods can be generally divided into supervised learning and self-supervised learning (Figure 6.2). Supervised learning methods require a large number of paired datasets of noisy and clean images for training. These datasets can be obtained by tuning illumination intensity and/or exposure time during image acquisition: when imaging with high intensity or longer exposure time, high-SNR clean images can be collected; otherwise, noisy images are collected. Training a supervised deep learning method on these data results in a mapping between noisy and clean images. When sufficient training data is available, these methods can provide very high-quality restoration on noisy data unseen by the network in the training process. However, paired datasets can be difficult to obtain in some microscopy settings (e.g., live-cell imaging). Typical denoising networks include U-Net-based Content-aware image restoration (CARE)38 and attention-based residual channel attention networks (RCAN)39.\nSelf-supervised learning methods do not require paired noisy and clean images. Instead, only noisy images are needed for training. They exploit the inherent structure of the data, learning to predict parts of the image using information gleaned from other parts. Typical self-supervised methods include Noise2Void (N2V)44 and Noise2Self (N2S)45. The key idea underlying N2V and N2S is that pixels from clean images are often highly correlated, i.e., nearby pixels usually look similar and follow predictable patterns. For example, neighboring pixels could show smooth gradients in image texture. However, many noise sources are are randomly correlated in space, particularly across large areas. So by selecting a suitable mechanism, noise and signal can be separated properly. In N2V, missing pixels are randomly masked in the noisy input image and the network is then trained to predict the value of the missing pixel using the surrounding noisy pixels. In N2S, instead of masking the noisy image entirely, local patches are used, and the network is trained to predict the value of the noisy pixel from its neighbors. While self-supervised methods can be effective, they often do not perform as well as supervised methods when the sample or noise distribution is complex. These methods nevertheless are quite useful when paired noisy-clean images are difficult or impossible to obtain.\nNoise2Noise (N2N)46 represents another kind of supervised training. It trains the model using two noisy images (A and B), where both images contain the same underlying clean signal. Essentially, it learns to map noisy image A to noisy image B. Since both images share the same clean signal beneath the noise, the network can learn to recover the underlying clean image. This method still relies on paired data as supervision, but the supervision signal comes from another noisy image rather than a traditional clean image.\n\n\n\n\n\n\nFigure 6.2: Different denoising methods for microscopy images. (a) Supervised denoising methods38,39, train a neural network using paired noisy and high-SNR images. (b) In self-supervised methods44,45, a portion of the image is masked (blind spots or black pixels in yellow and red subregions) for training, removing the need for explicit high-SNR images. (c) The Noise2Noise network46 is another form of supervised training, whereby the network is trained with a set of noisy images of the same underlying sample.\n\n\n\n\n\n6.2.2 Deconvolution\nDeconvolution has long relied on iterative algorithms like the Richardson-Lucy method. Recent advances in deep learning (DL) have revolutionized this task, offering high speed and in some cases even more accurate predictions than traditional methods47. Deep learning deconvolution methods can be divided into two categories: purely data-driven learning and physics-informed learning (Figure 6.3).\nIn the data-driven approach, training is conducted similarly to supervised denoising methods. The raw images are the low-quality blurred acquisitions; the high-quality reference can be the results of traditional restoration (e.g., multi-view jointly deconvolved results, or high SNR deconvolved results). Models like CARE, RCAN, and DenseDeconNet25 can be used for deconvolution. Once trained, such data-driven deconvolution generally enables more rapid deconvolution than traditional methods. However, these methods are content-aware and depend heavily on high-quality paired training data. This criterion may introduce artifacts and affect how well the network generalizes.\nPhysics-informed methods integrate domain knowledge—such as the microscope’s point spread function (PSF), and image formation models—into the network architecture or loss functions. For example, the Richardson-Lucy Network (RLN)47 embeds Richardson-Lucy iterations within a convolutional network, creating a hybrid method that leverages classical knowledge and data-driven deep learning. MultiWienerNet uses multiple differentiable Wiener filters paired with a convolutional neural network, exploiting the knowledge of the system’s spatially varying PSF to quickly perform 2D and 3D reconstruction, achieving good results on spatially varying deconvolution tasks48. Physics-informed methods can also improve interpretability and reliability and may reduce the demand for experimental training data by leveraging synthetic data generated from optical models.\n\n\n\n\n\n\nFigure 6.3: Different Deep Learning-Based Methods for Deconvolution in Microscopy Images. (a) Data-driven based deconvolution methods25,38, where a neural network is trained using only a large-set of paired raw data and ground truth (GT, e.g., joint deconvolution of multiview data) . (b) Physics-informed deconvolution47,48, whereby a neural network is trained using not only paired datasets but also incorporatesprior physical knowledge, such as embedding the image formation function in the loss function and/or the Richardson-Lucy framework or Wiener filtering in the network architecture.\n\n\n\n\n\n6.2.3 Deaberration\nDeaberration addresses image degradation caused by optical distortions due to refractive index mismatches in biological samples or imperfections in the optical system. These aberrations degrade image quality, particularly in thick or scattering samples. Recent advances in deep learning have introduced powerful alternatives to traditional AO for both explicit wavefront estimation and the prediction of cleaner images in which aberrations are suppressed (Figure 6.4).\nInitial efforts focused on estimating the distorted wavefronts with AI, followed by explicit wavefront correction with hardware (e.g., a deformable mirror, or SLM) to obtain a clean image. The wavefront aberration can be decomposed as a sum of Zernike polynomials. The problem of wavefront estimation then becomes inferring the amplitudes of different Zernike polynomials49. This approach combines the strengths of traditional AO with artificial intelligence50–52. Following the concept of deconvolution, researchers have also used deep learning to predict the aberrated wavefronts or blurring kernels (aberrated PSFs), also combining this information with post-processing deconvolution algorithms for image restoration53,54.\nIn contrast to explicit wavefront prediction, other efforts have trained neural networks directly on paired aberrated and corrected images55,56. These models are designed to learn the mapping between aberrated and corrected images, enabling aberration correction as a purely computational post-processing step. This approach eliminates the need for hardware-based wavefront sensing or correction, making it a practical and scalable solution for many microscopy applications. More recently, a unified deep learning framework for simultaneous denoising and deaberration in fluorescence microscopy has also been developed41. By addressing both noise and aberrations in a single model, this approach simplifies the image restoration pipeline while delivering high-quality results, highlighting the potential of multitask models to streamline microscopy workflows.\n\n\n\n\n\n\nFigure 6.4: Different Deep Learning-Based Methods for Aberration Correction in Microscopy Images. (a) A method where the network is configured to learn the distorted wavefront (using a sparse representation of Zernike coefficients)50–52. The trained network is then used to predict wavefront distortions (in terms of Zernike coefficients) for a new dataset, followed by hardware-based correction using an adaptive optics (AO) system (e.g., a deformable mirror). (b) A method where the network is configured to learn the aberrated Point Spread Function (PSF)53,54. The trained network is then used to predict the aberrated PSF corresponding to a new dataset, and a second step of image deconvolution is applied with the predicted PSF to remove aberrations from raw images. (c) A method where a neural network learns the mapping between aberrated and corrected images directly for post-processing aberration correction55,56. The trained network is then used to directly predict the deaberrated image corresponding to a new dataset.\n\n\n\n\n\n6.2.4 Resolution Enhancement\nThe spatial resolution of optical microscopy is fundamentally limited by the diffraction of light, a barrier described by Abbe’s law. Deep learning methods can learn to predict what a higher resolution image might look like, given lower resolution input data (Figure 6.5). Such methods use artificial neural networks to infer high-resolution (HR) details from low-resolution (LR) fluorescence images, which can offer a sharper image without the need for super-resolution microscopes. Cross-modality super-resolution (CMSR) refers to a class of techniques that use information from different imaging modalities to enhance resolution. Models such as CNNs and GANs are trained to learn the mapping between these modalities. For example, researchers can use super-resolution (e.g., STED) images to enhance diffraction-limited (e.g., confocal) images57; or use models trained on expansion microscopy data to predict super-resolution images from diffraction-limited inputs. Fourier-based approaches (e.g. DFCAN40) leverage the frequency content difference across distinct features in the Fourier domain to enable the networks to learn the hierarchical representations of high-frequency information efficiently. While powerful, we note that these resolution enhancement methods offer a prediction at best and cannot truly retrieve higher resolution information absent in the low-resolution input data.\nAnother resolution enhancement task is the prediction of isotropic resolution from blurry axial views. The majority of fluorescence microscopy techniques, including most widefield, confocal, and light-sheet microscopes, suffer from unmatched resolution in the lateral and axial directions (i.e., resolution anisotropy), which severely deteriorates the quality, reconstruction, and analysis of 3D images. Deep learning models can be trained to predict and fill in the missing information in the z-direction using the higher lateral resolution view and knowledge of the anisotropic PSF, effectively making the resolution more uniform across the entire image58,59.\n\n\n\n\n\n\nFigure 6.5: Schematic of resolution enhancement with cross-domain networks. (a) Cross-modality super resolution39,57. Top row: the method takes diffraction-limited data (e.g., confocal microscopy data) as input and super-resolved data (e.g., STED microscopy data or expansion microscopy data) as ground truth. By training a neural network with paired datasets, the network parameters are updated based on the loss between the network’s output and the ground truth, thereby achieving a mapping from the low-resolution domain to the high-resolution domain. Bottom row: similar cross-modality learning can also be adapted using expansion microscopy as the ground truth. The red and yellow arrows highlight improvements in lateral and axial slices due to the restoration process. By ‘digital expansion’, we mean the super-resolution prediction based on expansion microscopy ground truth. (b) Axial resolution enhancement58,59. The method uses datasets composed of low-resolution axial slices (sometimes synthesized from higher resolution lateral views38) and high-resolution lateral slices, employing a one-step58 or two-step59 framework to learn the mapping from the low-resolution domain to the high-resolution domain, thus achieving axial resolution enhancement for fluorescence microscopy 3D stack data.",
    "crumbs": [
      "Image Acquisition",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Image Restoration</span>"
    ]
  },
  {
    "objectID": "6-image-restoration.html#practical-guidelines-for-image-restoration",
    "href": "6-image-restoration.html#practical-guidelines-for-image-restoration",
    "title": "6  Image Restoration",
    "section": "6.3 Practical Guidelines for Image Restoration",
    "text": "6.3 Practical Guidelines for Image Restoration\nFor researchers new to deep learning-based image restoration, successfully applying deep learning to image restoration requires careful preparation and a clear understanding of basic concepts. Here we briefly describe practical guidelines to help you get started with using deep learning for image restoration, from data preparation to model deployment. A step-by-step manual for different image restoration tasks (i.e., denoising, deconvolution, deaberration, and resolution enhancement) using RCAN is provided in Appendix A.\n\n6.3.1 Considerations for Dataset Preparation\nMicroscope Compatibility: Ensure images are captured with consistent settings (e.g., exposure time, magnification). Common sources include confocal, widefield, light-sheet, or super-resolution microscopes.\nPaired vs. Unpaired Data: Paired data (low-quality – high-quality pairs) is ideal for supervised learning. Acquire paired datasets of degraded and high-quality images for supervised training. In contrast, unpaired data is typically used for for self-supervised methods (e.g., Noise2Noise). For unsupervised methods, collecting degraded images with varying conditions (e.g, different noise levels) helps the model generalize better across different conditions.\nTraining Data Size: According to the size of the acquired data, start with at least dozens of datasets of size 128×128 for 2D and 128×128×128 for 3D. Augmentation with rotations, flips, and noise injections can expand your training dataset (see also Chapter 5).\n\n\n6.3.2 Choosing a Network Architecture\nThe choice of network architecture can play an important role when performing image restoration (see also Chapter 4). Here are some popular architectures used for this purpose:\n\nConvolutional Neural Networks (CNNs): CNNs are currently commonly used because they are highly effective at capturing spatial features in images. For example, you can use U-Net60, a popular CNN architecture designed for image segmentation that is also well-suited for restoration tasks.\nGenerative Adversarial Networks (GANs): GANs can be used for tasks like image generation or synthesis (e.g., resolution enhancement), where the goal is to generate realistic images from degraded inputs. GANs consist of two networks: a generator (which creates images) and a discriminator (which evaluates the quality of the generated images).\nAutoencoders: These are unsupervised learning models that can be used for denoising, inpainting, and compression. Autoencoders consist of an encoder and a decoder that map input data into a lower-dimensional representation and then reconstruct it.\nChoose or design networks (e.g., CARE, Noise2Void), or even start with pre-trained models for faster deployment.\n\n\n\n6.3.3 Training\nOnce you have your data, hardware, and software environment and have chosen a model architecture, it’s time to train the model (see also Chapter 9). Training involves feeding the model the degraded images and their corresponding high-quality ground truth and then updating the model weights to minimize the difference between the restored and the ground truth images.\nLoss Functions: Common loss functions for image restoration include mean squared error (MSE), structural similarity index (SSIM), and perceptual loss, which helps preserve perceptually important image details.\nOptimization: Use optimizers like Adam or SGD (Stochastic Gradient Descent) to update the model’s parameters during training.\nTraining deep learning models can take a significant amount of time, depending on the size of the dataset, the complexity of the model, and the hardware you use. You can use techniques like early stopping61 to avoid overfitting and ensure the model is trained for an optimal number of epochs62,63.\n\n\n6.3.4 Validation and Deployment\nAfter training, it’s important to evaluate your model’s performance (see also Chapter 10). This can be done by testing it on a separate validation set (with ground truth reference) or test set (without ground truth) that wasn’t used during training.\nCommon evaluation metrics for image restoration include peak signal-to-noise ratio (PSNR), SSIM, and Visual Quality Assessment. These metrics help assess how close the restored images are to the ground truth in terms of quality and structure. Perform biological validation to ensure restored images are artifact-free, such as expert evaluation, cross-validation with alternative techniques, or repeat experiments to evaluate uncertainty.\nIf the initial results are not satisfactory, you may need to fine-tune your model. This can involve: adjusting the model architecture (e.g., adding more layers or changing the number of filters); using a different loss function; and fine-tuning hyperparameters like the learning rate.\nOnce your model has been trained and evaluated, the next step is deploying it. Depending on the use case, you might deploy it for real-time image restoration or integrate it into a larger image processing pipeline.",
    "crumbs": [
      "Image Acquisition",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Image Restoration</span>"
    ]
  },
  {
    "objectID": "6-image-restoration.html#limitations-and-future-perspectives",
    "href": "6-image-restoration.html#limitations-and-future-perspectives",
    "title": "6  Image Restoration",
    "section": "6.4 Limitations and Future Perspectives",
    "text": "6.4 Limitations and Future Perspectives\nThe integration of artificial intelligence (AI) into fluorescence microscopy image restoration has opened new avenues for biological discovery. However, as the field evolves, critical challenges and opportunities must be navigated carefully. Below, we outline key failure modes, caveats, and emerging trends, including the role of transformers and large models, to guide future research.\n\n6.4.1 Caveats of AI-based image restoration\nWhile AI has made remarkable strides in fluorescence image restoration, there are still failure modes that researchers must be aware of. One potential issue is overfitting, where a model becomes too specialized to the training data and struggles to generalize to unseen data. This can result in poor restoration performance on images that differ from the dataset used for training.\nAdditionally, loss of fine details40,64 and artifact generation11,65 remain a concern. Restored images may exhibit blurring, distorted textures, or unrealistic artifacts66, especially in high-resolution regions or when attempting to restore challenging image regions (e.g., dense and overlapping structures67 or highly degraded structures41). For example, while doing confocal-to-STED microscopy restoration with RCAN, certain microtubules evident in the STED remain unresolved in the RCAN result, and RCAN prediction for nuclear pores revealed slight differences in pore placement relative to STED ground truth39. While denoising fly wing data with CARE, the same networks predicted obviously dissimilar solutions across multiple trained models38. During denoising, conflicts between restoring local details and enhancing global smoothing can arise68,69. Due to AI’s inability to achieve a global understanding of semantic information, stitching artifacts may arise during large-scale data processing involving tiling operations70, resulting in inconsistent intensity, geometry, and textures. These artifacts may not always be immediately noticeable but can significantly affect subsequent analysis and interpretation. Ensuring algorithm robustness across varied datasets and imaging conditions is key to minimizing these issues.\nDeep learning models often operate in a “black-box” manner. This means that while these models can produce useful predictions, the reasoning behind their predictions is not easily interpretable by humans. This lack of transparency and interpretability means researchers might not know exactly how or why certain features in an image are being restored in specific ways. This issue can undermine trust in the model’s results and limit its acceptance in critical applications.\nAI-based image restoration techniques are highly dependent on the quality of training data. For fluorescence images, obtaining high-quality ground truth data can be challenging, especially when using highly specific imaging conditions or performing imaging on rare biological samples. The availability and variability of the resulting data can limit the model’s ability to generalize to diverse datasets.\nDeep learning models can be computationally expensive during training and require powerful hardware for training and inference. This can be a significant barrier for some research labs or organizations with limited access to high-performance computing resources. Researchers need to consider these limitations when planning AI-based restoration projects and balance trade-offs between model complexity, data availability, and computational requirements.\n\n\n6.4.2 Outlook for the Future\nWith advances in transfer learning and self-supervised learning, AI models are likely to become more efficient, requiring less annotated data and computational power. As datasets grow and become more diverse, the dependency on highly specialized ground truth data will be reduced. Moreover, the integration of AI with microscopy platforms in real-time will enhance the ability to process images on the fly, providing immediate feedback to researchers and enabling dynamic imaging of live cells and tissues.\nNewer methods of AI will be developed. Transformer models and large-scale models have shown remarkable success in other domains like natural language processing and computer vision. Transformer architectures, known for their ability to capture long-range dependencies and global context, could significantly improve image restoration tasks by better handling complex, large-scale image structures. Large models—trained on massive datasets—are likely to offer even greater performance, able to generalize across different imaging modalities and restoration tasks. As computational resources continue to expand and more sophisticated models are developed, we can expect these methods to further push the boundaries of image restoration, achieving even finer levels of detail and more accurate reconstruction.\nIn conclusion, the future of AI in fluorescence microscopic image restoration is bright. While challenges such as interpretability, data quality, overfitting, and computational demands remain, the field is poised for rapid advances with new architectures and training techniques.\n\n\n\n\n1. Wu, Y. & Shroff, H. Multiscale fluorescence imaging of living samples. Histochemistry and Cell Biology 158, 301–323 (2022).\n\n\n2. Schermelleh, L., Heintzmann, R. & Leonhardt, H. A guide to super-resolution fluorescence microscopy. Journal of Cell Biology 190, 165–175 (2010).\n\n\n3. Archit, A. et al. Segment anything for microscopy. Nature Methods 22, 579–591 (2025).\n\n\n4. Sahl, S. J., Hell, S. W. & Jakobs, S. Fluorescence nanoscopy in cell biology. Nature Reviews Molecular Cell Biology 18, 685–701 (2017).\n\n\n5. Schermelleh, L. et al. Super-resolution microscopy demystified. Nature Cell Biology 21, 72–84 (2019).\n\n\n6. Ji, N. Adaptive optical fluorescence microscopy. Nature Methods 14, 374–380 (2017).\n\n\n7. Hampson, K. M. et al. Adaptive optics for high-resolution imaging. Nature Reviews Methods Primers 1, 68 (2021).\n\n\n8. Shroff, H., Testa, I., Jug, F. & Manley, S. Live-cell imaging powered by computation. Nature Reviews Molecular Cell Biology 25, 443–463 (2024).\n\n\n9. Venkatesh, M., Mohan, K. & Seelamantula, C. S. Directional bilateral filters for smoothing fluorescence microscopy images. AIP Advances 5, 084805 (2015).\n\n\n10. Danielyan, A., Wu, Y.-W., Shih, P.-Y., Dembitskaya, Y. & Semyanov, A. Denoising of two-photon fluorescence images with block-matching 3D filtering. Methods 68, 308–316 (2014).\n\n\n11. Zhang, Y. et al. A poisson-gaussian denoising dataset with real fluorescence microscopy images. in 2019 IEEE/CVF conference on computer vision and pattern recognition (CVPR) 11702–11710 (Optica Publishing Group, 2019). doi:10.1109/CVPR.2019.01198.\n\n\n12. Li, J., Luisier, F. & Blu, T. Pure-let deconvolution of 3D fluorescence microscopy images. in 2017 IEEE 14th international symposium on biomedical imaging (ISBI 2017) 723–727 (2017). doi:10.1109/ISBI.2017.7950621.\n\n\n13. Makitalo, M. & Foi, A. Optimal inversion of the generalized anscombe transformation for poisson-gaussian noise. IEEE Transactions on Image Processing 22, 91–103 (2013).\n\n\n14. Luisier, F., Vonesch, C., Blu, T. & Unser, M. Fast interscale wavelet denoising of poisson-corrupted images. Signal Processing 90, 415–427 (2010).\n\n\n15. Walt, S. van der et al. Scikit-image: Image processing in python. PeerJ 2, e453 (2014).\n\n\n16. Wiener, N. Extrapolation, Interpolation, and Smoothing of Stationary Time Series: With Engineering Applications. (The MIT Press, 1949). doi:10.7551/mitpress/2946.001.0001.\n\n\n17. Gonzalez, R. C. & Woods, R. E. Digital Image Processing. (Prentice Hall, 2008).\n\n\n18. Tikhonov, A. N. Solution of incorrectly formulated problems and the regularization method. Soviet Math. Dokl. 4, 1035–1038 (1963).\n\n\n19. Miller, K. Least squares methods for ill-posed problems with a prescribed bound. SIAM Journal on Mathematical Analysis 1, 52–74 (1970).\n\n\n20. Beck, A. & Teboulle, M. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences 2, 183–202 (2009).\n\n\n21. Lucy, L. B. An iterative technique for the rectification of observed distributions. Astronomical Journal 79, 745 (1974).\n\n\n22. Richardson, W. H. Bayesian-based iterative method of image restoration\\(\\ast\\). J. Opt. Soc. Am. 62, 55–59 (1972).\n\n\n23. Sarder, P. & Nehorai, A. Deconvolution methods for 3-d fluorescence microscopy images. IEEE Signal Processing Magazine 23, 32–45 (2006).\n\n\n24. Goodwin, P. C. Chapter 10 - quantitative deconvolution microscopy. in Quantitative imaging in cell biology (eds. Waters, J. C. & Wittman, T.) vol. 123 177–192 (Academic Press, 2014).\n\n\n25. Guo, M. et al. Rapid image deconvolution and multiview fusion for optical microscopy. Nature Biotechnology 38, 1337–1346 (2020).\n\n\n26. Schindelin, J. et al. Fiji: An open-source platform for biological-image analysis. Nature Methods 9, 676–682 (2012).\n\n\n27. Sage, D. et al. DeconvolutionLab2: An open-source software for deconvolution microscopy. Methods 115, 28–41 (2017).\n\n\n28. Bazin, P.-L. et al. Volumetric neuroimage analysis extensions for the MIPAV software package. Journal of Neuroscience Methods 165, 111–121 (2007).\n\n\n29. Booth, M. J. Adaptive optics in microscopy. Philosophical Transactions: Mathematical, Physical and Engineering Sciences 365, 2829–2843 (2007).\n\n\n30. Hell, S. W. Far-field optical nanoscopy. Science 316, 1153–1158 (2007).\n\n\n31. Vicidomini, G., Bianchini, P. & Diaspro, A. STED super-resolved microscopy. Nature Methods 15, 173–182 (2018).\n\n\n32. Wu, Y. & Shroff, H. Faster, sharper, and deeper: Structured illumination microscopy for biological imaging. Nature Methods 15, 1011–1019 (2018).\n\n\n33. Chen, F., Tillberg, P. W. & Boyden, E. S. Expansion microscopy. Science 347, 543–548 (2015).\n\n\n34. Wassie, A. T., Zhao, Y. & Boyden, E. S. Expansion microscopy: Principles and uses in biological research. Nature Methods vol. 16 33–41 (2019).\n\n\n35. Valli, J. et al. Seeing beyond the limit: A guide to choosing the right super-resolution microscopy technique. Journal of Biological Chemistry 297, 100791 (2021).\n\n\n36. Chen, H. et al. Advancements and practical considerations for biophysical research: Navigating the challenges and future of super-resolution microscopy. Chemical & Biomedical Imaging 2, 331–344 (2024).\n\n\n37. Hagen, G. M. et al. Fluorescence microscopy datasets for training deep neural networks. GigaScience 10, giab032 (2021).\n\n\n38. Weigert, M. et al. Content-aware image restoration: Pushing the limits of fluorescence microscopy. Nature Methods 15, 1090–1097 (2018).\n\n\n39. Chen, J. et al. Three-dimensional residual channel attention networks denoise and sharpen fluorescence microscopy image volumes. Nature Methods 18, 678–687 (2021).\n\n\n40. Qiao, C. et al. Evaluation and development of deep neural networks for image super-resolution in optical microscopy. Nature Methods 18, 194–202 (2021).\n\n\n41. Hou, X. et al. HD2Net: A deep learning framework for simultaneous denoising and deaberration in fluorescence microscopy. Opt. Express 33, 27317–27333 (2025).\n\n\n42. Zhang, K., Zuo, W., Chen, Y., Meng, D. & Zhang, L. Beyond a gaussian denoiser: Residual learning of deep CNN for image denoising. IEEE Transactions on Image Processing 26, 3142–3155 (2017).\n\n\n43. Dabov, K., Foi, A., Katkovnik, V. & Egiazarian, K. Image denoising by sparse 3-d transform-domain collaborative filtering. IEEE Transactions on Image Processing 16, 2080–2095 (2007).\n\n\n44. Krull, A., Buchholz, T.-O. & Jug, F. Noise2Void - learning denoising from single noisy images. in 2019 IEEE/CVF conference on computer vision and pattern recognition (CVPR) 2124–2132 (2019). doi:10.1109/CVPR.2019.00223.\n\n\n45. Batson, J. & Royer, L. Noise2Self: Blind denoising by self-supervision. in Proceedings of the 36th international conference on machine learning (eds. Chaudhuri, K. & Salakhutdinov, R.) vol. 97 524–533 (PMLR, 2019).\n\n\n46. Lehtinen, J. et al. Noise2Noise: Learning image restoration without clean data. in Proceedings of the 35th international conference on machine learning (eds. Dy, J. & Krause, A.) vol. 80 2965–2974 (PMLR, 2018).\n\n\n47. Li, Y. et al. Incorporating the image formation process into deep learning improves network performance. Nature Methods 19, 1427–1437 (2022).\n\n\n48. Yanny, K., Monakhova, K., Shuai, R. W. & Waller, L. Deep learning for fast spatially varying deconvolution. Optica 9, 96–99 (2022).\n\n\n49. Saha, D. et al. Practical sensorless aberration estimation for 3D microscopy with deep learning. Opt. Express 28, 29044–29053 (2020).\n\n\n50. Kang, I., Zhang, Q., Yu, S. X. & Ji, N. Coordinate-based neural representations for computational adaptive optics in widefield microscopy. Nature Machine Intelligence 6, 714–725 (2024).\n\n\n51. Kang, I. et al. Adaptive optical correction in in vivo two-photon fluorescence microscopy with neural fields. bioRxiv (2024) doi:10.1101/2024.10.20.619284.\n\n\n52. Fersini, F. et al. Wavefront estimation through structured detection in laser scanning microscopy. Biomed. Opt. Express 16, 2135–2155 (2025).\n\n\n53. Zhou, Y., Jin, Z., Zhao, Q., Xiong, B. & Cao, X. Aberration modeling in deep learning for volumetric reconstruction of light-field microscopy. Laser & Photonics Reviews 17, 2300154 (2023).\n\n\n54. Qiao, C. et al. Deep learning-based optical aberration estimation enables offline digital adaptive optics and super-resolution imaging. Photon. Res. 12, 474–484 (2024).\n\n\n55. Guo, M. et al. Deep learning-based aberration compensation improves contrast and resolution in fluorescence microscopy. Nature Communications 16, 313 (2025).\n\n\n56. Hu, L., Hu, S., Gong, W. & Si, K. Image enhancement for fluorescence microscopy based on deep learning with prior knowledge of aberration. Opt. Lett. 46, 2055–2058 (2021).\n\n\n57. Wang, H. et al. Deep learning enables cross-modality super-resolution in fluorescence microscopy. Nature Methods 16, 103–110 (2019).\n\n\n58. Park, H. et al. Deep learning enables reference-free isotropic super-resolution for volumetric fluorescence microscopy. Nature Communications 13, 3297 (2022).\n\n\n59. Ning, K. et al. Deep self-learning enables fast, high-fidelity isotropic resolution restoration for volumetric fluorescence microscopy. Light: Science & Applications 12, 204 (2023).\n\n\n60. Ronneberger, O., Fischer, P. & Brox, T. U-net: Convolutional networks for biomedical image segmentation. in Medical image computing and computer-assisted intervention – MICCAI 2015 (eds. Navab, N., Hornegger, J., Wells, W. M. & Frangi, A. F.) 234–241 (Springer International Publishing, Cham, 2015). doi:10.1007/978-3-319-24574-4_28.\n\n\n61. Ji, Z., Li, J. D. & Telgarsky, M. Early-stopped neural networks are consistent. in Advances in neural information processing systems 34 - 35th conference on neural information processing systems, NeurIPS 2021 (eds. Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, {Percy. S. }. & Vaughan}, J. {Wortman) 1805–1817 (Neural information processing systems foundation, 2021).\n\n\n62. Santos, C. F. G. D. & Papa, J. P. Avoiding overfitting: A survey on regularization methods for convolutional neural networks. ACM Comput. Surv. 54, (2022).\n\n\n63. Miseta, T., Fodor, A. & Vathy-Fogarassy, Á. Surpassing early stopping: A novel correlation-based stopping criterion for neural networks. Neurocomputing 567, 127028 (2024).\n\n\n64. Shah, Z. H. et al. Image restoration in frequency space using complex-valued CNNs. Frontiers in Artificial Intelligence Volume 7 - 2024, (2024).\n\n\n65. Liu, J., Gao, F., Zhang, L. & Yang, H. A saturation artifacts inpainting method based on two-stage GAN for fluorescence microscope images. Micromachines 15, (2024).\n\n\n66. Bouchard, C. et al. Resolution enhancement with a task-assisted GAN to guide optical nanoscopy image analysis and acquisition. Nature Machine Intelligence 5, 830–844 (2023).\n\n\n67. Qiao, C. et al. Rationalized deep learning super-resolution microscopy for sustained live imaging of rapid subcellular processes. Nature Biotechnology 41, 367–377 (2023).\n\n\n68. Zhong, L., Liu, G. & Yang, G. Blind denoising of fluorescence microscopy images using GAN-based global noise modeling. in ISBI 863–867 (2021).\n\n\n69. Osuna-Vargas, P. et al. Denoising diffusion models for high-resolution microscopy image restoration. in 2025 IEEE/CVF winter conference on applications of computer vision (WACV) 4320–4330 (2025). doi:10.1109/WACV61041.2025.00424.\n\n\n70. Park, E. et al. Unsupervised inter-domain transformation for virtually stained high-resolution mid-infrared photoacoustic microscopy using explainable deep learning. Nature Communications 15, 10892 (2024).",
    "crumbs": [
      "Image Acquisition",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Image Restoration</span>"
    ]
  },
  {
    "objectID": "7-smart-microscopy.html",
    "href": "7-smart-microscopy.html",
    "title": "7  Adding AI to Your Hardware",
    "section": "",
    "text": "Introduction\n\nDefine and motivate the “problem”\n\nWhat is a Biological “Event”?\n\nWhy is it important to study these?\nHow is it distinct from an “Object”?\n\n\n\nSetting the bounds\n\nWhat kinds of signals are we looking to extract events from?\nWhat spatiotemporal scales are relevant?\nWhat types of microscopes/imaging assays are in the scope of this chapter?\n\nLimiting the scope\n\n\nBrief history/evolution of automated event/object detection in general\n\nOffer some background to current methods\n\nIncluding classical up to SVM\n\ne.g micropilot and limitations\nCase Study #1 : CellProfiler\nCase Study #2 : Micropilot\n\n\n\nThe need for new methods\n\nDeep learning based and ML approaches\n\nWhere the algorithm learns what is important\nWhat are the advantages of these methods?\n\nWhat has been done recently in the literature?\n\nWhat is required to label, train and implement these methods?\n\nSpecial considerations for event detection modesl\nSpecifics of event detection in contast the other ML tasks\n\n\n\nReal-time vs a posteriori inference\n\nChallenges, opportunities\nEvent detection as the first step in the microscopy workflow\n\nLimitations and notes of caution related to inference\n\n“Trusting the algorithms”\n\nConclusion",
    "crumbs": [
      "Image Acquisition",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Adding AI to Your Hardware</span>"
    ]
  },
  {
    "objectID": "8-existing-tools.html",
    "href": "8-existing-tools.html",
    "title": "8  Chapter 8: How do you select and find a tool?",
    "section": "",
    "text": "8.1 Introduction\nWe are now transitioning away from a discussion of hardware and into a discussion of software. And, more specifically, in this chapter we focus on image analysis, particularly the human thought processes and decisions needed for selecting what software to use and figuring out how to appropriately/best use it. Amazing technological developments have occured since the invention of microscopy almost half a millenium ago; Our current relationship to biological microscopy is profoundly shaped by the development of both quantitative microscopy and artificial intelligence in the mid-twentieth century. However, underlying the actual use of any technological development is a human (in our case, likely a biologist) selecting a specific technology to solve a specific problem.\nIn this chapter, we answer the questions of how do you find new-to-you AI models and how do you assess whether a new-to-you model or tool will meet your needs? In this chapter, we first help you assess your needs. We then introduce a way of categorizing tools and help you use your needs assessment to select the right tool category. We suggest several places where you can find AI models for bioimage analysis and describe how to assess how well tools in those locations meet your needs. Finally, we show a couple of case studies that fulfill different requirements.",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: How do you select and find a tool?</span>"
    ]
  },
  {
    "objectID": "8-existing-tools.html#assessing-your-requirements-for-a-tool-in-a-perfect-world",
    "href": "8-existing-tools.html#assessing-your-requirements-for-a-tool-in-a-perfect-world",
    "title": "8  Chapter 8: How do you select and find a tool?",
    "section": "8.2 Assessing your requirements for a tool (in a perfect world)",
    "text": "8.2 Assessing your requirements for a tool (in a perfect world)\nBefore finding and selecting an AI tool, you first need to decide on the type of tool you need. Your tool selection will be influenced by a number of factors and you should first honestly answer the questions that follow.\n\n\n\n\n\n\nNoteFigure 1: Workflows contain modular components\n\n\n\n Figure 1. When thinking of your workflow and how you might incorporate an AI tool, consider that a workflow (left of image) typically consists of multiple, modular subcomponents (right of image). You may be able to replace your whole workflow with an appropriate AI tool or you may be able to replace only a subcomponent while still keeping many of the original workflow subcomponents the same. Replacing a single subcomponent with an AI tool may force you to change surrounding subcomponents in your workflow - either with new tools or new configurations of existing tools - as the AI tool may require different inputs and produce different outputs.\nThough you may have built a workflow across multiple softwares, it is worth considering whether a single software (or a reduced number of softwares) can run your workflow and part of this consideration includes assessing how well your desired AI tool can run from within other software. Some examples of softwares that can be used to run other softwares include CellProfiler1, Fiji2, Bilayers3, ZeroCostDL4Mic4 or DL4MicEverywhere5, and Jupyter notebooks6.\nFigure adapted from7.\n\n\n\n8.2.1 Questions about your overall workflow:\n\nWhat is your desired output?\nYou must be able to concisely and specifically state what you would like output by your workflow. Do you need to segment objects? If so, do you need semantic, instance, or panoptic segmentation (Box 1)? (see Chapter 4 and Chapter 9 for more information) Do you need measurements and if so should they be made on individual objects or on images?\n\nHow will you assess your outputs?\nWill your assessment be qualitative or quantitative? Do you have ground truth? Do you have the expertise to assess output quality? (see ?sec-10 for more information)\nWhat is your desired “quality” of your outputs?\nAre you expecting outputs that approach ground truth or will “quick and dirty” be enough for you?\nWhat does your ideal workflow look like?\nDo you need all of the outputs to be made in a single software or are you comfortable using multiple softwares? If your images require preprocessing, does that need to happen in the same step? The same software?\n\n\n\n\n\n\n\nNoteGround Truth\n\n\n\nGround truth can mean many things. The Broad Bioimage Benchmark Collection (BBBC)8 provides an nice example of this. In the BBBC, all datasets include ground truth, but that ground truth can be one or many of counts, foreground/background, outlines of objects, biological labels, location, or bounding boxes.\n\n\n\n\n8.2.2 Questions about your resources: time, expertise, and compute:\n\nHow much time are you able to put into the task?\nAn honest assessment at the beginning of any project about the time you are willing to invest is critical. If you are in a rush, you’ll probably want to select a method that is already familiar or best matches your existing competencies.\n\nWhat is your priority?\nDetermining what your priority is goes hand in hand with assessing the amount of time you can put into your task. Perhaps you’re in a time crunch and speed is the most important consideration for you. Perhaps you have a specific quality control (QC) metric and you need the tool that will give you the outputs that optimize this metric. Perhaps you’ve always wanted to gain experience with a particular class of tool, so you want to figure out if this is the right use case for trying it.\n\nWhat is your level of computational domain expertise and comfort?\nIf you don’t have a high level of computational comfort, do you have the time and motivation to expand your skillset by building new skills outside of your comfort zone? Do you have a computational collaborator and how much time, in either teaching you or in handling the data themselves, are they able to contribute?  Computation domain expertise has two critical, but separable, components. The first is the ability to understand what you are doing thoroughly enough that you can design, analyze, and interpret your experiment. The other component is the ability to comfortably interact with software as a computationalist might (e.g. are you comfortable working from the command line (which is text only) or would you prefer a GUI (graphical user interface), where you can point and click).\nHow do you like to learn things?\nAre you a person who likes to jump in and just turn all the knobs and buttons to see what they do? Do you love to cozy up with a good written manual at the end of the day? Do video walkthroughs bring you joy or frustration? How important is it to you that you be able to ask the tool creator questions directly? The type of documentation and instructional materials as well as their depth can vary greatly between tools.\n\nWhat is your level of biological domain expertise?\nAre you confident that you fully understand the task so you can assess how well your AI model is performing? Do you have a biologist collaborator and how much time are they able to contribute to designing the experiment and/or analyzing data? Do you understand what controls you will need and/or corrections (such as single-color controls and/or measured flat-field corrections) you will need to make to be able to interpret your outputs?\n\nWhat access do you have to compute resources?\nDo you need to be able to run everything on your laptop or do you have ready access to a high performance cluster or scalable cloud compute? Do you have access to GPUs? Different tools have different compute requirements, especially as you scale them, and those requirements don’t always scale linearly with data size.\n\n\n\n8.2.3 Questions about your input data:\n\nHow difficult are your images?\nPerfect, clean data is the ideal input to any analysis. But that’s not always the regime we’re in. There are many sources of “difficult”. A couple examples and corresponding questions are below.\n\nDo your images have debris or other technical artifacts such as out-of-focus regions? Do those artifacts need to be identified and removed? If identified, should the whole image be dropped or do you need to keep non-artifact areas of the image?\n\nDo you have metadata that needs to be associated with your images? Is that metadata organized? How is that metadata organized (e.g. in the file name, in a .csv file, in a picture of a handwritten lab notebook) and does it play nicely with the tool you would like to use?\n\nAre your images in a friendly format? Are they in a proprietary file format? Are they in a file format that allows access to chunks?\n\n\nHow big is your data?\nThe larger data is, the harder it can be to work with. If your data is large, what dimensionality is big? e.g. many images, large individual file sizes, many channels\n\nDo you need outputs in multiple dimensions?\nIf you have z-planes, do you need 3D objects or can each plane be handled separately? If you have multiple timepoints, do you need objects tracked across timepoints or can each timepoint be handled separately?\n\nDo your images require preprocessing?\nThere are many different reasons that images might require preprocessing and some of those reasons may be a way to overcome concerns/technical challenges raised in other questions above. Some examples of preprocessing include stitching or cropping of images, denoising, background subtraction, or flat field correction.\n\n\n\n\n\n\n\nNoteBox 1: Segmentation methods\n\n\n\nIn computer vision, there are several discrete kinds of segmentation, although in the field of bioimaging we often refer to all kinds of segmentation under the single blanket term of “segmentation”. Semantic segmentation divides an image into classes. Ilastik is an example of a popular image analysis software that performs semantic segmentation. Instance segmentation detects individual, specific objects within an image. CellProfiler is an example of a popular image analysis software that is most commonly used for instance segmentation. Panoptic segmentation is a combination of semantic segmentation and instance segmentation that separates an image into regions while also detecting individual object instances within those regions. Deep learning can be used for semantic, instance, or panoptic segmentation. Most classic image analysis methods are built around instance segmentation for cell-based images, though semantic segmentation is not uncommon for tissue/histology images.\n\n\n\nSegmentation comparison of CellProfiler and Ilastik\n\n\nExample of different types of segmentations produced between CellProfiler and Ilastik. A) Input image of Drosophila Kc167 cells, provided in the Example pipeline packaged with CellProfiler. B) CellProfiler uses classic image processing to create instance segmentations. The individual objects identified are shown as individually colored masks in the upper panel. The lower image is the input image overlaid with nuclei objects outlined in green and cell objects outlined in pink.\nC) Ilastik uses pixel-based machine learning to create semantic segmentation. The image is the input image with green shaded areas in the class “Cells” and red shaded areas in the class “Background”.",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: How do you select and find a tool?</span>"
    ]
  },
  {
    "objectID": "8-existing-tools.html#assessing-your-requirements-for-a-tool-in-the-real-world",
    "href": "8-existing-tools.html#assessing-your-requirements-for-a-tool-in-the-real-world",
    "title": "8  Chapter 8: How do you select and find a tool?",
    "section": "8.3 Assessing your requirements for a tool (in the real world)",
    "text": "8.3 Assessing your requirements for a tool (in the real world)\nIn several previous surveys we have run, scientists have reported that they generally don’t actually follow any thoughtful, rigorous process for deciding what tool they will use for a particular process. Instead, they sit down with a thing they know how to use and hope they can make it follow their use case9 10. If they don’t already have familiarity with a tool that is appropriate for their task, they ask their labmates or maybe do a quick internet search. So know that if you found the section above overwhelming, you are not alone.\nIf you already have familiarity with a tool and it fits your newest use case (especially if your newest use case isn’t very different from a use case where you’ve previously successfully used it) go ahead and stick with that tool. “If it ain’t broke, don’t fix it” is a common adage for a reason. But even for existing use cases, it’s worth actively contemplating your priorities every-so-often - maybe your GPU cluster is filling up these days and is harder to access, or after a new operating system upgrade a tool that was working great is now randomly crashing, or maybe there simply is something new (or new-to-you) available that will make your life easier. In our experience, the best way to be sure you’re best serving your scientific needs is simply to step back and consider your big picture goals and whether your tool choices are serving those goals with some regularity.\nOnce you have conducted a thorough needs assessment, it should theoretically be simple to determine if a given tool or model is the right one for you. Unfortunately, no tool is likely to perfectly fit every single need that you have! You will likely need to break your needs into “must haves” and “nice-to-haves”, and rank tools accordingly. Make sure you consider not just how well the tool will perform your specific task, but also how well it will fit in your workflow (Figure 1)",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: How do you select and find a tool?</span>"
    ]
  },
  {
    "objectID": "8-existing-tools.html#how-do-i-decide-when-my-workflow-is-good-enough",
    "href": "8-existing-tools.html#how-do-i-decide-when-my-workflow-is-good-enough",
    "title": "8  Chapter 8: How do you select and find a tool?",
    "section": "8.4 How do I decide when my workflow is “good enough”?",
    "text": "8.4 How do I decide when my workflow is “good enough”?\nDeciding when a workflow is “good enough” can be really hard! We generally factor in two major considerations when deciding what quality level we are aiming to achieve: Is this a pilot or final data? and How big is the effect size we’re looking for?\nWe generally hold pilot data to a lower quality standard than final or production data. We are looking for a proof of concept that our assay/method can work but understand that there are likely to be refinements and improvements at many points in the workflow that will increase our final quality. Additionally, during the piloting phase, there are likely to be changes introduced that change the inputs to our workflow so time spent beyond a certain point of workflow refinement is lost with the retuning/adjustments required for the next pilot batch.\nPhenotypic effect size has a large impact on necessary quality level. Simple, easily distinguished phenotypes (e.g. GFP is either in the nucleus or cytoplasm) may reach statistical significance with many fewer individual measurements and using much simple/cruder regions of interest (e.g. cell bounding boxes instead of careful cell margin segmentations) than more subtle phenotypes.\n\n\n\n\n\n\nNoteResearch progress is rarely linear\n\n\n\n Figure 2. As scientists who care about the quality of our work, it’s tempting to always try to maximize the accuracy of our analysis. But it’s worth considering that “accuracy” vs “time spent” is often an asymptotic curve - you can keep pushing the accuracy higher and higher, but the gains may be marginal past a particular point. Time and effort spent on a project can have very different relationships with the output quality. Though one might hope to be in a regime where the amount of time spent is linearly correlated with the quality of a project’s output (blue line) so that it is clear exactly how much time must be put in to get a desired product, research rarely proceeds in such a fashion. Instead, it progresses in unpredictable fits and starts (green line) and/or it is in a state of diminishing returns (red line) where the closer you get to “perfect”, the longer it takes to improve the quality.\n\n\nIn11 we detail and provide examples for a few key heuristics that we think about when deciding if a workflow is “good enough”. Those heuristics are:\n\nHow close to accurate do I need to be to assess the % or fold change I expect to see in this experiment?\n\nHow close to 100% accurate is it possible to get with my current workflow?\n\nHow important is this aspect of my experiment to my overall hypothesis?\n\nHow wrong is my current pipeline output?\n\nWhat else could I do (e.g. develop a different approach/workflow, learn a new tool, generate new input data) in the time it will take me to make my pipeline maximally accurate?",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: How do you select and find a tool?</span>"
    ]
  },
  {
    "objectID": "8-existing-tools.html#choosing-the-kind-of-tool-to-use",
    "href": "8-existing-tools.html#choosing-the-kind-of-tool-to-use",
    "title": "8  Chapter 8: How do you select and find a tool?",
    "section": "8.5 Choosing the kind of tool to use",
    "text": "8.5 Choosing the kind of tool to use\nLet’s consider the use case of instance segmentation (see Box 1), which is often a critical step in image analysis workflows. We have defined 6 categories of segmentation methods, each of which has its valid use cases. Ordered by increasing levels of computation comfort required they are:\n\nManual annotation\n\nClassical image processing\n\nPixel based machine learning\n\nPretrained deep learning models\n\nFinetuned deep learning models\n\nFrom-scratch deep learning models\n\nWe will not cover from-scratch deep learning models in this chapter. They are covered in Chapter 9.\nIt is worth noting that many of the tool suggestions below are not confined to a single “class”. Many tools listed in the non-deep learning categories allow you to run pre-trained deep learning models inside them (e.g. CellProfiler has a RunCellPose module, ImageJ and QuPath also contain ability to run pixel classifiers). All deep learning models can be re-tuned with some effort (they may just not have a friendly interface for doing so).\n\n\n\nMethod category\nSuggested light microscopy tools\nTime required\nBest if your priorities include\nBiological expertise level required\nAbility to handle reasonable technical noise and variability\nLargest drawback\n\n\n\n\nManual Annotation: Hand-drawing in boundaries of all objects\nImageJ2 12, Napari13, see also this forum thread for 3D annotation\nHighest\nHigh accuracy/ outputs approaching ground truth\nHigh\nHigh\nInter- and even intra-operator manual annotations (e.g. after time or image rotation) variability. Irreproducibility. Can not be automated.\n\n\nClassical Image Processing: Combining mathematically well-defined operations such as thresholding and watershedding to create objects\nCellProfiler1, QuPath14, Icy15\nMedium\nNeeding to have a good sense of what the failure cases will be (e.g. when cells are too big, too small, too crowded, etc); needing to run in minimal computational footprints\nHigh\nInitially low, can be improved with experience\nMost performant on very bright objects on dark backgrounds\n\n\nPixel based machine learning: Using properties of individual pixels to classify each pixel into belonging to a given set of “classes” (e.g. “true signal”, “autofluorescence”, and “background”)\nIlastik16, Labkit17\nLow to Medium\nIdentifying well-separated objects that are not bright-on-dark or are not the only bright-on-dark objects present\nHigh\nHigh if properly represented in the training set\nMust be followed by Classical Image Processing if individual object masks are required\n\n\nPretrained deep learning models: Using a set of learned transformations (created by the developer) to define the positions and boundaries of objects\nStardist18, Instanseg19\nLow\nMaximum likelihood for good quality with minimal human time required\nHigh\nHigh if properly represented in the training set (which you may not know)\nUnlikely to work if data similar to your data is not present in the training set. Can be hard to package into workflows.\n\n\nFine tuned deep learning models: Updating or adding to a pretrained deep learning model with your data\nCellpose20, µSAM21\nMedium\nMaximum likelihood for very good quality with less human time than manual annotation\nHigh\nHigh if properly represented in the training set\nMost computationally intensive. Many available models are not provided in a non-computationalist friendly format. Can be hard to package into workflows.\n\n\n\nYou may have noticed that all methods are listed as requiring a high level of biological expertise. It can be tempting for researchers without deep biological expertise to assume that they could just use a pretrained or fine tuned deep learning model and call it a day. However, we would caution that, while they may require less of a biologist’s time to tune at a first pass, it is still essential to involve a biologist with subject matter expertise to verify that the models are producing results that match known biology.",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: How do you select and find a tool?</span>"
    ]
  },
  {
    "objectID": "8-existing-tools.html#finding-tools-and-models",
    "href": "8-existing-tools.html#finding-tools-and-models",
    "title": "8  Chapter 8: How do you select and find a tool?",
    "section": "8.6 Finding tools and models",
    "text": "8.6 Finding tools and models\n\n8.6.1 Where to look for tools and models\nThere are many places that one can look to find new (or new-to-them) tools and models. A few suggestions are as follows:\n\nBioImage Model Zoo22 is a community-driven AI model repository that offers a variety of pretrained AI models. All models are described by a common Model Resource Description File Specification which simplifies using models in the BioImage Model Zoo with partner software such as ilastik16, ImJoy23, Fiji2, deepImageJ24 and ZeroCostDL4Mic4.\nBioimage Informatics Index (Biii)25 is a community-curated index that includes software and helps researchers find tools whether their search is problem-based (e.g. “find nuclei in cells”), method-based (e.g. “active contour-based segmentation”), or tool-based (e.g. “CellProfiler”).\n\nThe Scientific Community Image Forum (forum.image.sc)26 is a platform where users can pose bioimage software-related questions and receive feedback from a broad user community and maintainers from &gt;75 partner softwares.\n\nAcademic literature can include biology-focused research where users describe applications or computer science research where the tools and models themselves are more likely to be described. Keeping an eye on preprint servers like arXiv (particularly in Computing Research Repository or Quantitative Biology sections) and bioRxiv can help you find the most cutting-edge developments.\n\n\n\n\n\n\n\nNoteBioimage analysts\n\n\n\nThere is a whole discipline of people who specialize in figuring out workflows for analyzing microscopy images - their official titles often vary, but many think of themselves as bioimage analysts. If you want to find a local bioimage analyst, or even become one, you can check out the Global Bioimage Analysts’ Society (GloBIAS), which holds events, coordinates trainings (including free help sessions), and hosts a database of bioimage analysts and bioimage analysis trainers.",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: How do you select and find a tool?</span>"
    ]
  },
  {
    "objectID": "8-existing-tools.html#case-studies",
    "href": "8-existing-tools.html#case-studies",
    "title": "8  Chapter 8: How do you select and find a tool?",
    "section": "8.7 Case Studies",
    "text": "8.7 Case Studies\n\n8.7.1 Case Study 1:\n\n8.7.1.1 Introduction\nElena is an image analyst who uses high content microscopy as a method for exploring drug mechanism of action. She uses an automated microscope to acquire 9 fields of view in each well of a 384-well plate and just acquired a batch of 4 plates worth of images. She often performs the same assay so she has a series of pipelines that she has nicely tuned for her usual experimental parameters. Elena has a new collaborator that would like to perform a similar screen in a different cell line as the collaborator is interested in a different specific area of biology. Because this is the first time that Elena has performed this assay in a new cell line, she needs to carefully tune her existing pipelines so that she has high-quality segmentation of Nuclei and Cell objects.\n\n\n8.7.1.2 Workflow\n\n\n\nCellProfiler for nuclei and cell segmentation\n\n\nElena usually uses CellProfiler as an image analysis workflow tool, using classical image processing methods for object identification in her pipeline. Elena typically performs her screens in A549, U2OS, or HeLa cells which are all considered easy to use for high-content microscopy because they are relatively consistent in size, relatively round, and can grow both sparsely and confluently while remaining in a monolayer. She’s quite used to making parameter changes to the IdentifyPrimaryObjects and IdentifySecondaryObjects modules to fine-tune her nuclei and cell segmentations, respectively. The image above shows shows the modules in her CellProfiler pipeline (left) and the outputs of her IdentifyPrimaryObjects (center) and IdentifySecondaryObjects modules (right).\n\n\n\nFiji for compositing a new png image\n\n\nElena’s collaborator has acquired images in an adipocyte cell line and they are morphologically quite different from her typical screening cell lines. After being unsatisfied with what she was able to tune using classical segmentation in her standard Cellprofiler pipeline, Elena decides to turn to an AI tool. She starts by opening Cellpose in its web console as she has heard from colleagues that it is both very good out of the box and allows a user to fine tune it for their own data. She initially struggles to load one of her own images into the web tool but notices that the web page says “upload one PNG or JPG &lt;10 MB”. She also notices that the example images that are most similar to hers are in color, with a nuclear channel in blue and a cell marker in green. Elena’s raw images are in .tif format with a single grayscale channel per image so she uses Fiji to make a composite .png image. The image above shows her single-channel .tif images opened with FIJI (left, center) and the composite .png image that she created (right).\n\n\n\nCellpose web console\n\n\nElena loads her new composite .png into the Cellpose web console and sees that it has identified a number of reasonable cell objects but is not of the quality that she wants for use in her screen. However, Elena is excited to explore a new tool and has been actively builing her computational skills so she decides to proceed with Cellpose and see if she can improve the outputs. The image above shows her example image loaded into the Cellpose web console (far left) along with the predicted object outlines (center left), masks (center right), and flows (far right).\n\n\n\nRunCellpose in CellProfiler\n\n\nElena takes advantage of the fact that Cellpose allows for fine-tuning of the pretrained deep learning models it provides. She re-trains Cellpose using a Colab notebook and sees that it now performs quite well on test images from her dataset. Because CellProfiler is also an image analysis workflow tool, Elena is able to insert a RunCellpose module in her original pipeline instead of the original secondary object identification module and keep the rest of her image handling and analysis the same while using the newer, better performing object identification that Cellpose provides. The image above shows her new CellProfiler pipeline with the RunCellpose module pointing to her custom fine-tuned Cellpose model.\n\n\n8.7.1.3 Conclusion\nThis case study starts with a scientist trying to use her standard classical image processing methodds for object identification but finding that she gets better results with a new challenging segmentation task by moving to an AI tool. It shows iterative improvements using the same tool in different ways and highlights some of the simple changes she needed to make to her overall workflow to use the new tool.\n\n\n\n8.7.2 Case Study 2:\n\n8.7.2.1 Introduction\nEsteban is an image analyst, working with imaging data acquired by a collaborator. The collaborator studies zebrafish brains and sent Esteban fluorescent images taken in a large z-stack in live zebrafish. The zebrafish express a GFP-tagged protein in their microglia and the collaborators would like to be able to identify the microglia using an automated pipeline so that the researchers can ask biological questions such as where they are within the brain and whether they are close to other cell types (labeled in other fluorescent channels). The task is complicated because there is additional autofluorescence in the skin that is the same intensity as the microglia and they do not want skin autofluorescence identified. The images also have technical challenges because of the nature of their acquisition: because they are 3D live images, as you go deeper in the tissue the signal gets dimmer. This makes using normal computer vision segmentation quite hard.\n\n\n8.7.2.2 Workflow\n\n\n\nPoint Annotations in napari\n\n\nTo help orient Esteban to the image data, he was given a spreadsheet of x,y,z coordinates of microglia that the collaborators manually annotated. Esteban started by opening the images and their point annotations in napari to orient himself to the data. The image above shows an example image opened in napari with red circles around the point annotations.\n\n\n\nGuided Annotations in napari with SAM\n\n\nEsteban then loaded µSAM in napari to use as an annotation tool so that he could use the point annotations to make object masks. He tried using automatic segmentation but it didn’t work on his images. Instead, he used µSAM assisted segmentation, manually providing positive prompts, and got out reasonable segmentations of the microglia in the image. The image above is quite similar to the previous image but also has the µSAM segmentations in color overlays.\n\n\n\nVisualizing 3D annotations in napari\n\n\nThough Esteban has three dimensional data, the segmentations he is creating are 2D so he confirms that he is happy with his final masks by examining the image and masks in a 3D viewer. The above image shows a 3 dimensional view of his data that contains all of the image slices and annotations viewed together.\n\n\n\nCellpose predictions\n\n\nNow that Esteban has both images and masks, he calls Cellpose in Python and retrains the cyto model with his annotations. He runs the retrained model on a new image (left image) and finds that the cell probability prediction seems to identify microglia and autofluorescence (center image), but, unfortunately, the gradient prediction is not finding any cell objects (right image). Esteban’s training images are quite sparse (the vast majority of the image is background) so the Cellpose model learned that it can be mostly correct by deciding that everything is background.\n\n\n\nilastik 3 class predictions\n\n\nSince neither a pre-trained model (µSAM) nor a fine-tuned model (Cellpose) worked on his images, Esteban decided to try pixel classification. He used ilastik, first providing annotations for microglia and background. The classifier worked quite well at distinguishing microglia from background, but classified the skin autofluorescence as microglia. Esteban built on this by using larger filters so that the classifier had more contextual information, training 3 classes (microglia, skin, and background) instead of 2, and adding in autocontext27 which improves classification by running it in multiple stages. Esteban’s final 3-class pixel classifier performs well to identify microglia separately from skin across the images in his datasets. The above image shows a zoom-in of one of his images with the three classes overlaid - red for background pixels, blue for skin pixels, and yellow for microglia pixels.\n\n\n8.7.2.3 Conclusion\nThis case study starts with a scientist trying multiple state-of-the-art deep learning segmentation tools for a specific, challenging segmentation task. He ultimately discovers that a machine learning pixel classifier works better for this task, illustrating that 1) “less-advanced” tools can perform better than deep learning tools and 2) biological expertise is always needed when inspecting any tool’s outputs.",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: How do you select and find a tool?</span>"
    ]
  },
  {
    "objectID": "8-existing-tools.html#conclusion-2",
    "href": "8-existing-tools.html#conclusion-2",
    "title": "8  Chapter 8: How do you select and find a tool?",
    "section": "8.8 Conclusion",
    "text": "8.8 Conclusion\nAs we shift our focus from hardware to software, it is common and understandable to feel overwhelmed - the bioimage analysis tool space is a constantly-changing array of best practices, and not all microscopists are highly computationally comfortable. This chapter therefore mostly talks about how to make decisions, as opposed to use X tool for Y task. Ultimately, the “right tool for your task” is the tool (or chain of tools) that allows you to be confident that your analysis has been performed well enough, and helps you understand the answer to your biological question and how confident you can be in your conclusion. Even if you try a tool and decide it isn’t the right solution for this question, this time still isn’t wasted - it simply enlarged the toolbox you can use for answering future questions. In the subsequent chapters, we will teach you about how to train models which could be important parts of your toolbox, but we suggest you keep our guidelines in mind as you finalize the decisions around when your model is “trained enough” and how you decide on your overall workflow.\n\n\n\n\n1. Stirling, D. R. et al. CellProfiler 4: Improvements in speed, utility and usability. BMC Bioinformatics 22, 433 (2021).\n\n\n2. Schindelin, J. et al. Fiji: An open-source platform for biological-image analysis. Nature Methods 9, 676–682 (2012).\n\n\n3. Shah, R., Gogoberidze, N. & Cimini, B. Bilayers.\n\n\n4. Chamier, L. von et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nature Communications 12, 2276 (2021).\n\n\n5. Hidalgo-Cenalmor, I. et al. DL4MicEverywhere: Deep learning for microscopy made flexible, shareable and reproducible. Nat. Methods 21, 925–927 (2024).\n\n\n6. Kluyver, T. et al. Jupyter notebooks - a publishing format for reproducible computational workflows. in Positioning and power in academic publishing: Players, agents and agendas (eds. Loizides, F. & Scmidt, B.) 87–90 (IOS Press, Netherlands, 2016).\n\n\n7. Haase, R., Tischer, C., Bankhead, P., Miura, K. & Cimini, B. A call for FAIR and open-access training materials to advance BioImage analysis. (2024).\n\n\n8. Ljosa, V., Sokolnicki, K. L. & Carpenter, A. E. Annotated high-throughput microscopy image sets for validation. Nat. Methods 9, 637 (2012).\n\n\n9. Jamali, N., Dobson, E. T. A., Eliceiri, K. W., Carpenter, A. E. & Cimini, B. A. 2020 BioImage analysis survey: Community experiences and needs for the future. Biological Imaging 1, e4 (2021).\n\n\n10. Sivagurunathan, S. et al. Bridging imaging users to imaging analysis - a community survey. J. Microsc. (2023).\n\n\n11. Cimini, B. A. When to say ’good enough’. (2019).\n\n\n12. Schindelin, J., Rueden, C. T., Hiner, M. C. & Eliceiri, K. W. The ImageJ ecosystem: An open platform for biomedical image analysis. Mol. Reprod. Dev. 82, 518–529 (2015).\n\n\n13. Napari: A multi-dimensional image viewer for python. doi:10.5281/zenodo.3555620.\n\n\n14. Bankhead, P. et al. QuPath: Open source software for digital pathology image analysis. Sci. Rep. 7, 16878 (2017).\n\n\n15. Chaumont, F. de et al. Icy: An open bioimage informatics platform for extended reproducible research. Nat. Methods 9, 690–696 (2012).\n\n\n16. Berg, S. et al. Ilastik: Interactive machine learning for (bio)image analysis. Nat. Methods 16, 1226–1232 (2019).\n\n\n17. Arzt, M. et al. LABKIT: Labeling and segmentation toolkit for big image data. Front. Comput. Sci. 4, (2022).\n\n\n18. Schmidt, U., Weigert, M., Broaddus, C. & Myers, G. Cell detection with star-convex polygons. in Medical image computing and computer assisted intervention – MICCAI 2018 (eds. Frangi, A. F., Schnabel, J. A., Davatzikos, C., Alberola-López, C. & Fichtinger, G.) 265–273 (Springer International Publishing, Cham, 2018). doi:10.1007/978-3-030-00934-2_30.\n\n\n19. Goldsborough, T. et al. InstanSeg: An embedding-based instance segmentation algorithm optimized for accurate, efficient and portable cell segmentation. arXiv [cs.CV] (2024).\n\n\n20. Pachitariu, M. & Stringer, C. Cellpose 2.0: How to train your own model. Nat. Methods 19, 1634–1641 (2022).\n\n\n21. Archit, A. et al. Segment anything for microscopy. Nature Methods 22, 579–591 (2025).\n\n\n22. Ouyang, W. et al. BioImage model zoo: A community-driven resource for accessible deep learning in BioImage analysis. bioRxiv 2022.06.07.495102 (2022) doi:10.1101/2022.06.07.495102.\n\n\n23. Ouyang, W., Mueller, F., Hjelmare, M., Lundberg, E. & Zimmer, C. ImJoy: An open-source computational platform for the deep learning era. Nat. Methods 16, 1199–1200 (2019).\n\n\n24. Gómez-de-Mariscal, E. et al. DeepImageJ: A user-friendly environment to run deep learning models in ImageJ. Nat. Methods 18, 1192–1195 (2021).\n\n\n25. Zhang, C. et al. Bio-image informatics index BIII: A unique database of image analysis tools and workflows for and by the bioimaging community. arXiv [q-bio.QM] (2023).\n\n\n26. Rueden, C. T. et al. Scientific community image forum: A discussion forum for scientific image software. PLoS Biol. 17, e3000340 (2019).\n\n\n27. Kreshuk, A. & Zhang, C. Machine learning: Advanced image segmentation using ilastik. Methods Mol. Biol. 2040, 449–463 (2019).",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: How do you select and find a tool?</span>"
    ]
  },
  {
    "objectID": "9-train-models.html",
    "href": "9-train-models.html",
    "title": "9  How to Train and Use Deep Learning Models in Microscopy",
    "section": "",
    "text": "9.1 Introduction",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>How to Train and Use Deep Learning Models in Microscopy</span>"
    ]
  },
  {
    "objectID": "9-train-models.html#sec-9.1",
    "href": "9-train-models.html#sec-9.1",
    "title": "9  How to Train and Use Deep Learning Models in Microscopy",
    "section": "",
    "text": "9.1.1 The challenge\nThe growing volume and complexity of image data necessitate increasingly advanced analytical tools. One example of challenging tasks is image segmentation, the process of identifying and delineating structures of interest within images. Segmentation can be particularly difficult and time-consuming when dealing with large, multidimensional datasets, such as 3D volumes or time-lapse sequences, where manual annotation becomes impractical. Machine learning (ML), especially deep learning (DL), can provide effective solutions to these challenges1.\nML algorithms learn patterns from data to perform tasks such as image classification and segmentation. Traditional ML methods, like random forest classifiers, depend on manually defined image features to classify pixels2–4. In contrast, DL algorithms can automatically discover and extract relevant features directly from image data using multilayer neural networks, which eliminates the need for manual feature selection. DL techniques are widely applied in complex image analysis tasks, including segmentation, object detection, feature extraction, denoising, and restoration5,6. Due to their ability to automatically learn hierarchical features, DL methods usually achieve greater accuracy and efficiency than traditional ML techniques7,8.\nSegmentation greatly benefits from ML and DL, as manual segmentation is extremely time-consuming and impractical for large datasets. This chapter offers practical guidance on preparing a segmentation project and emphasises effective DL applications to tackle these challenges. While we focus on segmentation as a case study, the principles, workflows, and considerations discussed here are broadly applicable to other image analysis tasks, such as classification or denoising. Readers interested in these areas can adapt the described approaches to their specific needs.",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>How to Train and Use Deep Learning Models in Microscopy</span>"
    ]
  },
  {
    "objectID": "9-train-models.html#sec-9.2",
    "href": "9-train-models.html#sec-9.2",
    "title": "9  How to Train and Use Deep Learning Models in Microscopy",
    "section": "9.2 Preparing for your project",
    "text": "9.2 Preparing for your project\n\n9.2.1 Defining your task and success criteria\nEvery image analysis project should begin by clearly defining the scientific question you wish to answer, along with the criteria by which success will be measured. These foundational decisions will fundamentally shape your entire workflow. Careful planning of your objectives ensures that the chosen approach closely aligns with your scientific goals and will guide critical decisions about data annotation, model selection and performance evaluation.\nSince segmentation serves as the central example in this chapter, it is important to first understand the different types of segmentation tasks encountered in microscopy before discussing how deep learning methods can be applied. These tasks can typically be categorised into three main types (Figure 9.1). Each segmentation type presents distinct challenges and is suited to different biological questions:\nBinary segmentation: This is the simplest form of segmentation that separates the foreground from the background. For example, in a microscopy image, this involves distinguishing cell nuclei (foreground) from the rest of the image (background). This method is useful for detecting whether a structure is present or absent without distinguishing individual objects.\nInstance segmentation: This type of segmentation identifies and labels each object independently. For instance, each cell in an image obtains a unique label. This method is crucial for tracking individual cells over time or measuring specific characteristics of each cell separately.\nSemantic segmentation: This segmentation strategy involves labelling every pixel in an image according to its class, such as “nucleus,” “cytoplasm,” or “background.” Unlike instance segmentation, semantic segmentation does not differentiate between individual objects within the same class. This method is beneficial for analysing the spatial relationships and distribution of various cellular components.\n\n\n\n\n\n\nFigure 9.1: The three main types of segmentation in microscopy images. original: A raw grayscale fluorescence microscopy image showing cell nuclei stained with a nuclear marker. binary segmentation: Simplifies the image into two classes—foreground (white, nuclei) and background (black). instance segmentation: Assigns a unique label (shown in different colours) to each nucleus, facilitating individual object identification. semantic segmentation: Categorises each pixel into predefined classes—nucleus (purple), nucleus edge (yellow), and background (teal)—without distinguishing between individual objects.\n\n\n\nConsider whether your segmentation solution is meant for a specific experiment or needs to generalise across various imaging techniques, sample types, or experimental conditions. Additionally, evaluate the volume of data to analyse, the feasibility of manual analysis, and the resources available to create a tailored image analysis pipeline. Avoid overengineering a solution when a simple analysis could provide the answer you seek.\nAlongside task-specific considerations, it is equally important to clearly define the success criteria based on your objectives. For example, be prepared to answer the question, “What do I need to accomplish for my analysis to be sufficient?” – see ?sec-10 for more information. This is important because no analysis is ever 100% accurate. Establishing these criteria early streamlines both the development and evaluation processes, ensuring that your outcomes are scientifically meaningful and practically useful (see ?sec-10).\nWhile the following steps focus on segmentation, the underlying principles can be readily adapted to a wide range of DL tasks in microscopy.\n\n\n9.2.2 Evaluating alternatives: Is DL the right choice?\n\n\n\n\n\n\nFigure 9.2: Is DL the right choice for your segmentation project? This decision tree guides the selection of appropriate segmentation approaches based on data complexity and project needs. Begin by testing classical image processing methods, such as intensity-based thresholding, which are efficient and easy to apply for well-defined features. If these methods prove insufficient, consider using a pixel classifier, which provides a user-friendly and effective solution for smaller datasets. Only consider DL if you possess a large annotated dataset and previous methods have failed. In the absence of suitable data or methods, manual annotation may be necessary.\n\n\n\nChoosing the right computational method is essential for consistent and reproducible image analysis. For example in segmentation tasks, while DL can deliver exceptional segmentation performance, traditional methods and pixel classifiers still offer straightforward and efficient solutions for most tasks (Figure 9.2).\nTraditional image processing techniques—such as intensity-based thresholding, morphological operations, edge detection, and filtering—are ideal for objects with clear, distinguishable features. These methods are well-documented, easy to understand, and usually require minimal computing resources. Pixel classifiers, in particular, are user-friendly and can efficiently tackle many segmentation challenges with minimal manual annotation, making them highly effective for simpler analyses or smaller datasets.\nDL methods excel in complex scenarios where traditional approaches fail, especially when dealing with noisy or context-dependent data. When trained on large, annotated datasets, DL models can effectively generalise across diverse imaging conditions and sample types, rapidly processing significant volumes of images. However, in the absence of pre-trained models, DL methods rarely offer shortcuts for data analysis. DL methods generally take effort and time to implement.\nIf you are unsure which approach to use, we usually recommend first trying classical image processing methods and pixel classifiers (Figure 9.1). We typically initiate a DL project only if these methods fail to produce satisfactory results (see Section 9.3.3.2).",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>How to Train and Use Deep Learning Models in Microscopy</span>"
    ]
  },
  {
    "objectID": "9-train-models.html#sec-9.3",
    "href": "9-train-models.html#sec-9.3",
    "title": "9  How to Train and Use Deep Learning Models in Microscopy",
    "section": "9.3 Implementing a DL segmentation workflow",
    "text": "9.3 Implementing a DL segmentation workflow\nAlthough we use segmentation as our primary example, the workflow outlined in this section can be adapted to other deep learning tasks in microscopy and bioimage analysis.\n\n9.3.1 Overview of a typical DL segmentation workflow\nOnce you decide to implement a DL approach for segmentation, the workflow can be divided into a series of steps (Figure 9.3).\nThe process starts by clearly defining your task (in this example, segmentation) and selecting the right DL approach (Step 1). Next, evaluate whether any existing pre-trained models can be used directly on your data or adapted for use (Step 2). If additional training is required—either from scratch or through transfer learning—prepare an appropriate training dataset that reflects your segmentation problem (Step 3). Then, train your model using the prepared dataset (Step 4) and thoroughly evaluate its performance using validation or test data (Step 5). Based on the results, you may need to refine the model by adjusting hyperparameters, improving annotations, or expanding the dataset. Once the model performs satisfactorily, it can be used to segment new, unseen data (Step 6). We will now discuss each step in more detail.\n\n\n\n\n\n\nFigure 9.3: Conceptual workflow for training a DL segmentation model. The workflow begins with defining the segmentation task (Step 1), followed by searching for suitable pre-trained models (Step 2). If no such model exists, a training dataset must be prepared (Step 3), and the model is trained (Step 4). The trained model is then evaluated on validation or test data (Step 5). If it performs well, it can be applied to new data (Step 6); otherwise, the model is iteratively refined by returning to earlier steps. Feedback loops from evaluation to earlier stages help refine and improve the model accuracy.\n\n\n\n\n\n9.3.2 Selecting a suitable DL approach\nThe first step in choosing a DL approach for image segmentation is to clearly define your segmentation task, whether it’s binary, semantic, or instance segmentation (Figure 9.1), and to determine if you require 2D or 3D segmentation. Next, you should consider whether the model or tool you plan to use makes assumptions about the shapes or structures of the objects you want to segment. Understanding these assumptions will aid in selecting a model that fits your specific biological problem (see Section 9.2.1). Additionally, consider the amount of data that needs annotation for a particular DL approach (see Section 9.3.4.5). Finally, take into account your available computational resources (see Section 9.4.2). More complex models typically demand more GPU memory, longer training times, and additional storage, especially for 3D data or large datasets.\nFor example, StarDist9, a widely used tool for nuclei segmentation, assumes that objects are star-convex polygons: i.e., a shape for which any two points on its boundary can be connected by a single line that does not intersect the boundary. This assumption works well for round or oval shapes but makes StarDist less suitable for segmenting irregularly shaped or elongated structures. In contrast, Cellpose10 uses spatial vector flows to direct pixels toward object centres. This approach enables Cellpose to segment objects of various shapes and sizes, including irregular, elongated, or non-convex forms.\nChoosing the right DL strategy requires aligning your goal, object shape, data dimensionality, and computing capacity with the strengths and assumptions of the available DL architectures.\n\n\n9.3.3 Deciding whether to train a new model\n\n\n\n\n\n\nFigure 9.4: Decision workflow for selecting a DL model training approach. This flowchart outlines how to determine an appropriate training approach based on the availability and performance of pre-trained models.\n\n\n\n\n9.3.3.1 Leveraging pre-trained models\nThe increasing availability of already trained (pre-trained) DL models has greatly simplified image analysis. Many of these models can be directly applied to your data, removing the need to train your own model11,12. This reduces the technical barrier and saves time, making advanced analysis more accessible. However, it is essential to evaluate the quality of any pre-trained model before relying on its results (Figure 9.4). A model that performs well in one context may not be as effective on your specific data. Always conduct quality control by visually inspecting the outputs and assessing performance with quantitative metrics such as Intersection over Union (IoU) or F1-score, using a small, representative test set. This step is vital when model predictions are used in downstream analyses (see Section 9.3.7).\nAnother significant benefit of pre-trained models is their adaptability. Instead of starting from scratch, you can often fine-tune an existing model (see Section 9.3.6). This method entails retraining the model with a smaller, task-specific dataset, enabling it to adjust to your images while requiring far fewer annotations.\nSeveral excellent resources host pre-trained models suitable for microscopy. Researchers also increasingly share trained models alongside their datasets and publications, promoting open science. Platforms like Zenodo are commonly used for this purpose13,14, although deployment may require handling specific file formats or environments (see Chapter 8 for more information).\n\n\n9.3.3.2 When to train your model\nPre-trained models serve as an excellent starting point for various microscopy tasks. However, there are many scenarios where training a custom model becomes essential. Custom training enables the model to learn the specific characteristics of your dataset, experiment, or imaging modality, resulting in enhanced performance15–17. This is particularly crucial when your data differs significantly from the data used to train existing models. Thus, their performance should always be validated. If quality assessment metrics are poor or key features are not accurately segmented, consider training your own model.\nUltimately, always evaluate the model’s performance against your defined success criteria (see Section 9.3.7). Custom training may be the best path forward if the current model does not meet your needs.\n\n\n\n9.3.4 Preparing your dataset for training\nA well-designed training dataset is essential for developing successful DL models on tasks such as segmentation. The number of images and the quality of annotations needed vary based on factors such as task complexity and the architecture of the intended model.\n\n9.3.4.1 Types of model training\nFor segmentation, most DL models are trained using supervised learning, where each input image is paired with a manually annotated ground truth mask. In this context, all objects that need segmentation must be annotated in the training dataset. This approach enables the model to learn a direct mapping from raw images to segmentation outputs (Figure 9.7).\nHowever, alternative approaches can help reduce the need for extensive manual annotations:\n\nUnsupervised learning trains models without paired input and output data. Instead, the network identifies patterns or similarities in unlabelled images18.\n\nSelf-supervised learning involves designing tasks in which the model learns useful features directly from the input data without needing explicit labels19.\n\nWeakly supervised learning uses partial, noisy, or imprecise labels to guide training, which can significantly reduce annotation effort20,21.\n\n\n\n9.3.4.2 Creating Manual Annotations\nCreating accurate annotations manually is time-consuming, particularly for 3D datasets. Tools like Fiji22, Napari23, and QuPath24 are frequently employed for manual labelling. Typically, manual annotation involves drawing each object on the image and converting it into a mask or label.\n\n\n\n\n\n\nTipHere it is an example pipeline for manually annotating data using Fiji\n\n\n\n\n\nSee also13.\n\nOpen Fiji – activate the LOCI update site and restart Fiji.\nLOCI tools are required for exporting ROI maps. To enable them, go to Help &gt; Update &gt; Manage Update Sites, look for ‘LOCI’ and check the Active checkbox. Then, click on Apply and Close and Apply Changes, this update site ensures the necessary plugins are installed. Finally, restart Fiji.\nOpen your image you wish to annotate.\nUse File › Open to browse and load the microscopy image that you want to label manually. You can also drag and drop your image to Fiji.\nSelect the Oval or Freehand selection tool.\nThese tools, found in the Fiji toolbar, allow you to manually outline the structures of interest in your image.\nStart drawing around each object (yes, each one!).\nCarefully trace each cell or feature you want to annotate—precision is key to ensure useful training data for DL.\nAfter drawing each object, press “t” on your keyboard → the selection will be stored in the ROI manager.\nThis adds the drawn region to the ROI (Region of Interest) list, keeping track of all annotated objects in the image.\nRepeat until all objects are in the ROI manager.\nContinue drawing and pressing “t” until you have annotated every relevant object in the image.\nWhen finished, go to Plugins › LOCI › ROI Map.\nThis plugin converts all saved ROIs into a single labeled ROI map image, assigning unique values to each region.\nSave the generated ROI map with the same title as the original image in a separate folder.\nConsistent naming ensures each annotated map can be correctly matched with its corresponding raw image during training or analysis.\nAt the end, you will have one folder with the original images and another for the ROI maps.\nThis separation makes it easier to organise and use your data with image analysis or DL pipelines.\n\n\n\n\n\n\n9.3.4.3 Accelerating annotation with automatic initial segmentations\nCreating high-quality annotations often represents the most time-consuming aspect of training a DL model for segmentation. To alleviate this burden, you can start from automatically produced initial segmentations. For example, using simple thresholding methods such as Otsu’s thresholding to generate rough segmentations can decrease the total annotation time. Even more powerfully, pre-trained DL models such as those provided with StarDist9 and Cellpose10 can generate more accurate initial segmentations that users can manually refine. These annotations can then be used to retrain the model, establishing an iterative cycle that accelerates both labelling and model refinement.\nNew tools are also pushing the boundaries of interactive annotation. For example, Segment Anything for Microscopy (μSAM)15 facilitates automatic and user-guided segmentation and allows the model to be retrained on user-provided data. Similarly, Cellpose 2.016 features a human-in-the-loop workflow, allowing users to edit DL-generated segmentations. This hybrid approach enhances accuracy while significantly reducing the time and effort required for manual annotation.\n\n\n9.3.4.4 Expanding your dataset with augmentation and synthetic data\nWhen the number of training samples is limited, augmentation techniques can enhance dataset diversity to improve the model’s generalisation ability and performance on validation and testing25,26. Common augmentation strategies include image rotation, flipping, scaling, and contrast adjustment. However, it’s important to apply augmentation carefully, as excessive or unrealistic augmentation can confuse the model or cause it to learn patterns that do not exist in real data.\nIn the absence of sufficient real data, synthetic data generated through simulations or domain randomization can help pre-training a model27,28. These synthetic samples can expose the model to a broader range of scenarios early in training, before transitioning to fine-tuning with real, annotated data.\nIn summary, a successful segmentation pipeline relies on a careful balance between data quantity and annotation quality. Augmentation strategies can efficiently help to scale and balance training datasets.\n\n\n9.3.4.5 Choosing the dataset size: specific vs. general models\nIn supervised training, it is crucial that each image in the training set is accompanied by a corresponding label image (see Section 9.3.4.1). The number of image-label pairs required depends on the number of labels per image, the complexity of the model and the desired level of generalisability. Still, the key is having enough representative examples and corresponding annotations for the model to learn meaningful patterns.\nSmall and well-curated datasets consisting of tens of images may suffice for highly specific applications, such as segmenting cells or nuclei using a defined imaging modality17. In these scenarios, transfer learning can also be especially beneficial (see Section 9.3.6). Models designed to generalise across a wide range of conditions, tissue types, or imaging modalities typically require much larger and more diverse datasets (hundreds to thousands of annotated images)10. These datasets are essential for capturing the inherent variability in broader use cases.\n\n\n\n9.3.5 Training a segmentation model from scratch\nOnce you have annotated your training dataset, the next steps are to organise your data for training, initialise your model by selecting appropriate hyperparameters, and start the training process (Figure 9.7).\n\n9.3.5.1 Splitting your training data: training, validation, and test sets\nA crucial part of preparing your dataset is dividing it into three subsets: training, validation, and test sets. Each subset should contain the original microscopy images paired with their corresponding ground truth segmentations. A common strategy is to allocate 70–80% of the data for training, 10–15% for validation, and the remainder for testing. To ensure unbiased evaluation, ensure these subsets do not overlap in terms of fields of view, represent the variability of your entire dataset, and are randomly assigned to each set respectively.\nThe training set is used to train the model to recognise relevant features. To enhance generalisation, it must encompass a broad spectrum of scenarios and image conditions. Otherwise, the model risks overfitting—excelling with the training data but faltering with new images (Figure 9.6).\nThe validation set is used during training to provide feedback on the model’s performance with unseen data. This feedback, conveyed as validation loss, assists in detecting overfitting (Figure 9.6), guiding hyperparameter tuning (see Section 9.3.5.3), and informing training decisions. Although a separate validation set is ideal, many workflows create one in practice by reserving a portion (typically 10% to 30%) of the training data.\nThe test set, which serves a separate role, evaluates the model’s performance on entirely unseen data. Unlike the validation set, the test set is not utilised during training, ensuring an unbiased performance assessment. Test images should also include ground truth annotations to facilitate quantitative quality control. Reporting test set performance, using metrics such as accuracy, IoU, or F1-score, is crucial, especially when publishing or benchmarking your model29.\n\n\n9.3.5.2 Understanding the training process\nA DL model is composed of multiple layers (Figure 9.5). Each layer contains tens to hundreds of image processing operations (typically multiplications or convolutions), each controlled by multiple adjustable parameters (called weights). Altogether, a DL model may contain millions of adjustable weights. When an input image is processed by a DL model, it is sequentially processed by each layer until an output is generated. Segmentation tasks typically involve converting input images into labelled outputs. During training, the model weights are modified as the model learns how to perform a specific task.\n\n\n\n\n\n\nFigure 9.5: 2D U-Net architecture for image segmentation. It applies layers of convolutions, pooling, and upsampling to extract features and generate labelled segmentation masks. During training, model weights are iteratively adjusted based on the difference between predictions and ground truth labels, using a loss function and backpropagation.\n\n\n\nTraining begins with initialising these weights. When training from scratch, the initialisation is often random. However, when using a pre-trained model, the weights are already optimized based on previous training, providing the model with a significant head start (see Section 9.3.6).\nThe training process is iterative (Figure 9.7). Each cycle of training is called an epoch. During each epoch, the model typically learns from every image in the training set. Since datasets are often too large to fit into memory all at once, each epoch is divided into steps or iterations, with each step processing a smaller subset of the data known as a batch. The batch size determines how many samples are processed simultaneously.\nDuring each step, the model generates predictions for the current data batch. These predictions are compared to the ground truth labels using a loss function that calculates the similarity between the predictions and the ground truths. This score is called the training loss. The model utilises this feedback to adjust its weights through a process known as backpropagation, guided by an optimisation algorithm, to improve its accuracy in future iterations.\nAt the end of each epoch, the model assesses its performance on the validation set, which comprises data it has not encountered during training. This produces the validation loss, indicating how well the model generalises to new data.\nMonitoring both training and validation losses during training helps determine whether the model is learning effectively. A consistently decreasing validation loss indicates that the model is improving and generalising well (see Section 9.3.5.4).\n\n\n9.3.5.3 Choosing your model hyperparameters\nNow that you understand the training process, the next step is to configure the model’s hyperparameters, which are the settings that dictate how the model learns. While the model’s parameters (its weights) are updated during training, hyperparameters are established beforehand, defining the structure and behaviour of the training process. Below are some of the most common hyperparameters and their effects on training:\n\nBatch size: This refers to the number of images processed simultaneously in each training step. Smaller batch sizes are less demanding on memory and may enhance generalisation, although they can result in slower training. In contrast, larger batch sizes accelerate training but necessitate more GPU memory.\nEpochs: An epoch refers to a training cycle in which the model processes the entire training dataset. Increasing the number of epochs allows the model to learn more, but also raises the risk of overfitting. More is not always better; it is essential to monitor performance on the validation set.\nLearning rate: It determines the extent to which the model’s weights are adjusted during training. A high learning rate can result in quicker training but may overshoot the optimal solution. Conversely, a low learning rate provides more stable progress, although it may slow down convergence.\nOptimizer: An algorithm that updates weights to minimise the loss function. Common optimisers include SGD (stochastic gradient descent) and Adam (adaptive moment estimation), the latter being widely used for its adaptive learning rate and robust performance.\nLearning rate scheduler: Dynamically adjusts the learning rate during training, typically decreasing it after a specific number of epochs or when the validation loss plateaus. This approach helps balance rapid early learning with more refined convergence later on.\nPatch size: Instead of using full-resolution images, smaller patches are often utilised for training to reduce memory usage and enhance training speed. The patch size is determined by both available resources and the scale of the structures to be segmented.\nPatience (early stopping): This parameter defines the number of epochs to wait before halting training if the validation loss does not improve. It helps prevent wasting resources on overfitting and overtraining.\n\nGiven the many possible configurations, tuning hyperparameters is often essential—especially when applying a model to new data. Start with the recommended values from the model’s original publication, but you might need to conduct a hyperparameter search to optimize performance. This can range from a simple grid search to more advanced methods, such as Gaussian process-based Bayesian optimisation30 or genetic algorithms31.\n\n\n9.3.5.4 Monitoring training and validation Losses\nOnce your model begins training, it is helpful to evaluate its learning progress. The two key metrics for assessment are the training loss and the validation loss (Figure 9.6). Monitoring both throughout the training process offers insight into whether your model is improving and learning to generalise beyond the training data. The three main behaviours that you may encounter during training are:\n\n\n\n\n\n\nFigure 9.6: Monitoring training and validation losses during model training. The plot illustrates three typical learning behaviours: underfitting (both losses remain high and similar), good fitting (both losses decrease, with validation loss slightly higher), and overfitting (training loss continues to decrease while validation loss plateaus or increases), highlighting the importance of tracking these metrics to assess model performance and generalisation.\n\n\n\n\nUnderfitting: The model has been trained with insufficient data or for too few epochs, resulting in similar training and validation losses, which is far from optimal.\n\nGood fitting: Both training and validation losses decrease, with the validation loss slightly higher (worse) than the training loss, which is expected. This represents the ideal scenario.\n\nOverfitting: The model achieves an excellent training loss, but the validation loss does not improve and may in fact diverge. This may indicate overly similar training data or excessive training epochs, preventing the model from generalising to new data.\n\n\n\n\n9.3.6 Fine-Tuning Pre-existing Models\nInstead of training a model from scratch, fine-tuning an existing DL model is usually more efficient, especially when your data resembles the dataset used to train the original model. This approach utilises pre-trained weights and previously learned features, significantly decreasing the amount of required annotated data, training time, and computational resources.\n\n9.3.6.1 Applying Transfer Learning\nTransfer learning refers to the process of taking a pre-trained model and adapting it to a new but related task by providing task-specific training data, typically in the form of manually annotated image pairs (Figure 9.7). Transfer learning typically involves freezing part of the model (for instance, the initial layers or all layers except the last ones), so their weights are not updated during training. Only the unfrozen layers are updated when the model is trained on new data. Then, the model is trained on the new data, but only the layers that you have unfrozen will be updated. Since the base model already encodes many useful low-level features (e.g., edges, shapes, textures), this approach allows researchers to focus on refining the model for their specific biological structures or imaging modalities32,33.\nThis method is especially effective when:\n\nYou have limited training data available.\n\nYour imaging conditions closely match those of the pre-trained model.\n\nYou wish to quickly adapt a general model to a specific dataset.\n\n\n\n9.3.6.2 Conducting fine-tuning\nIn classic fine-tuning, all layers of the pre-trained model are retrained, with their weights initialised from the original training (Figure 9.7)15,16. Thus, you continue training the full model using the new data. This approach allows the model to adjust more comprehensively to new data while still preserving the advantages of pre-learned features.\nClassic fine-tuning is ideal when:\n\nYour dataset is moderately different from the original training data (e.g., the same biological structure but different staining or modality).\n\nYou expect that earlier layers may need to adapt, not just the final classifier or output layers. Early layers in deep networks typically learn to detect general features such as edges, textures, or simple shapes, while later layers capture more complex, task-specific patterns. If your new data differs in basic appearance or imaging modality, updating the early layers helps the model better extract relevant low-level features from your images.\n\nYou have enough annotated data to avoid overfitting during full model training. Although this method is more computationally demanding than transfer learning, where only a subset of layers are retrained, it often leads to better results on diverse datasets..\n\n\n\n9.3.6.3 Iterative training: keeping humans in the loop\nIterative fine-tuning is an interactive approach that combines model prediction with human annotation (see Section 9.3.4.3). The workflow typically starts with a pre-trained model predicting new images. A user then manually corrects or annotates these predictions, and the improved annotations update the model (Figure 9.7). This cycle continues, progressively enhancing the model’s accuracy with each iteration until it performs as expected16,34,35.\nThis method is particularly powerful when:\n\nAnnotated data is scarce or expensive to generate.\n\nYou work with rare structures, unusual imaging conditions, or new experimental systems.\n\nYou want to efficiently build a custom model using feedback from domain experts.\n\n\n\n\n\n\n\nFigure 9.7: Strategies for training DL models for image segmentation. A model can be trained from scratch using a large annotated dataset, fine-tuned from a pre-trained model with task-specific data, or refined through human-in-the-loop workflows where model predictions are manually corrected and fed back for retraining. These approaches balance performance, data availability, and annotation effort.\n\n\n\n\n\n\n9.3.7 Evaluating the performance of your model\n\n\n\n\n\n\nFigure 9.8: Workflow for evaluating DL model performance during training and on test data. Evaluating model performance is essential before deploying any DL model for image segmentation. This diagram outlines a two-stage process: assessment during training and on a separate test set. During training, validation and training losses (see Section 9.3.5.4) guide whether to continue training, stop, or expand the dataset. After training, performance is evaluated using a test set. High test metrics (e.g., IoU, F1-score) indicate readiness for deployment. Borderline or poor results suggest reviewing errors, refining training data, or trying a different model. This approach ensures model reliability and task-specific performance.\n\n\n\nWith the rapid increase in DL tools and pre-trained models, it has become easier to use DL for image segmentation, but harder to determine which model will work best for your data. Regardless of how promising a model appears, you must always evaluate its performance before trusting its results. Evaluation is not optional; it is a critical step to ensure that the model meets the requirements of your specific task29 (Figure 9.8).\nThere are two main ways to evaluate a model:\n\nQualitative evaluation entails visually inspecting the model’s predictions. This approach can help you quickly identify clear errors or failures. It is effective for a small number of images, but it becomes impractical for large datasets or for comparing similar-looking outputs across multiple models.\n\nQuantitative evaluation provides objective metrics for comparing models and tracking improvements. To achieve this, you need a small, labelled test set (typically 5 to 10 images with accurate ground truth segmentations). This test set must remain independent of your training and validation data to ensure an unbiased assessment.\n\nCommon metrics used in quantitative evaluation include:\n\nIntersection over Union (IoU), also known as the Jaccard Index, measures the overlap between the predicted segmentation and the ground truth.\n\nF1-score (Dice coefficient): This is especially valuable when the object of interest covers a small area in the image, as it balances precision and recall.\n\nTrue Positives (TP), False Positives (FP), and False Negatives (FN) are particularly important in semantic segmentation and can be used to calculate the IoU or F1 score.\n\nFor more information on these metrics, we recommend29 (also see ?sec-10 for more information).\nIf a model fails to produce reasonable results, even on simple examples, you can often reject it based solely on qualitative inspection. However, in these cases, quantitative metrics can still help you understand how and where the model fails.\nIf your evaluation metrics indicate weak performance, especially for certain structures or image types, you may need to fine-tune the model (see Section 9.3.6). Consistently strong scores across various test images suggest that a model could be dependable and ready for deployment. If no pre-trained model meets your expectations, the best course may be to train your model using your images (see Section 9.3.5).\nIn summary, never skip evaluation. Every model must be tested—both visually and quantitatively—to ensure it truly works for your data and provides results you can trust.\n\n\n9.3.8 Deploying your model on new data\nOnce a segmentation model has been trained and validated, it can be used on new, unseen images. This step typically involves feeding new images into the model to generate segmentation predictions. The deployment approach relies on the computational resources (see Section 9.4.2) as well as the size and complexity of your dataset (Figure 9.9).\n\n\n\n\n\n\nFigure 9.9: Decision workflow for model deployment strategy based on computational resources. The choice of deployment strategy depends on the availability of computational resources (see Section 9.4.2) and the sensitivity of the data . If high-performance computing resources are available locally, these should be used for deployment. In their absence, consider whether the data can be transferred to the cloud. If so, cloud-based resources offer an efficient solution. However, if data transfer is restricted—due to size or sensitivity—local deployment remains the only option, though it may require significantly more time.\n\n\n\n\n\n9.3.9 Troubleshooting Common Problems\nI found a tool or DL model online, but it does not work. What should I do?\n\n\n\n\n\n\nFigure 9.10: Common DL segmentation problems and troubleshooting tips.\n\n\n\nWhen should I train a model or segment manually?\nRefer to (Section 9.3.3.2) for more details, but generally, this decision depends on your dataset and the performance of existing pre-trained models (Figure 9.10). If you only need to segment a small number of images, manually segmenting them is often the quickest and simplest solution. However, if you are dealing with a large dataset, it may be more efficient to annotate a small subset and use it to train a deep-learning model that can automate the segmentation of the rest.\nI decided to train my DL model, but it is not performing correctly. What should I do?\nFirst, ensure that you have trained the model for a sufficient number of epochs—this depends on the size of your dataset and the architecture of the model. Check the training loss: if it has plateaued, your model may be fully trained. If it is still decreasing, continue training.\nIf training is completed but results are poor, examine your data. Is the model missing specific features? Are there types of cells or structures that it consistently fails to segment? If so, ensure those examples are well represented and correctly annotated in your training data. You may need to enhance or expand your annotations.\nIf performance is poor, you may need additional annotated data to help the model generalise more effectively (Figure 9.8). Consider the following questions:\n\nIs my dataset balanced? Does it include sufficient examples of each structure or class I want to segment?\n\nAm I training on one experimental batch while validating or testing on another?\n\nHow many images should I have to train my model?\nRefer to Section 9.2.1 for more details. There’s no one-size-fits-all answer—it depends on the complexity of your task, your model architecture, and the variability in your data. More complex tasks typically require more data. Larger images can also be broken into more patches, effectively increasing your dataset size. While few-shot models are being developed for small datasets, most established DL models require a substantial amount of data.\nPossible technical issues that you may encounter when training your DL model.\n\nThe model predicts the same class for all pixels or segments in every cell. Your dataset might be unbalanced, containing too many similar examples. Adding more diverse or underrepresented examples can help the model learn to differentiate between classes.\n\nOut-of-memory errors during training: Consider reducing the batch size or the image patch size. If that doesn’t resolve the issue, consider switching to a workstation or cloud service with greater computational capacity.\n\nThe model performs well on training data but poorly on new images, suggesting overfitting (Figure 9.6). Implement data augmentation and increase dataset diversity to help the model generalise better.\n\nInconsistent results across different computers: Differences in GPUs or environments can cause slight variations in outcomes. If the differences are significant, verify that all systems use consistent software versions and configurations. For further information on this topic, refer to Section 9.4.3.",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>How to Train and Use Deep Learning Models in Microscopy</span>"
    ]
  },
  {
    "objectID": "9-train-models.html#sec-9.4",
    "href": "9-train-models.html#sec-9.4",
    "title": "9  How to Train and Use Deep Learning Models in Microscopy",
    "section": "9.4 Further considerations for DL segmentation",
    "text": "9.4 Further considerations for DL segmentation\n\n9.4.1 Choosing the Right Tools for DL\nSelecting the right tools to train and use DL models depends mainly on your level of programming experience and comfort with technical interfaces.\nIf you prefer not to write code or use command-line tools, opt for platforms that offer graphical user interfaces (GUIs) or interactive notebooks with pre-configured workflows. These tools let you perform powerful segmentation tasks using intuitive interfaces and simple widgets.\nGUI-based tools include, for instance (see Chapter 8 for more tools):\n\nCellpose GUI\n\nFiji with DeepImageJ and StarDist plugins\n\nNapari\n\nIlastik\n\nQuPath\n\nInteractive Jupyter notebooks provide a flexible balance between code and GUI. They enable you to execute code in manageable steps (cells) and immediately see the results. Tools like BiaPy, and DL4MicEverywhere36 leverage Jupyter notebooks, concealing complex code behind user-friendly interfaces. These platforms cater to users with little or no coding experience while still allowing advanced users to access and modify code as needed. DL4MicEverywhere, in particular, established a widely adopted framework for training DL models via notebooks, contributing to the standardisation and simplification of the workflow.\nIf you are comfortable with programming, you will have even more flexibility. Languages such as Python, MATLAB, Julia, Java, and Rust provide options for building and customizing DL workflows. Python stands out as the most beginner-friendly and widely supported choice, boasting a large ecosystem of libraries and community support. Popular Python libraries for DL include PyTorch, TensorFlow, Keras, and JAX.\nWhile coding can involve a steeper learning curve, it allows you to create customized pipelines, integrate various tools, and troubleshoot intricate workflows, unlocking the full potential of DL for microscopy segmentation.\n\n\n9.4.2 Managing Computational Resources\nWhen using DL for microscopy, an important consideration is the availability and capacity of your computational resources (Figure 9.9). High-performance DL models, particularly those used for 3D image data, can be very demanding regarding memory and processing power.\nWhen selecting or designing a DL model, evaluate your available infrastructure:\n\nGPU memory: Determines how large your model and batch size can be.\n\nTraining time: Influences your ability to iterate quickly; simpler models train faster.\n\nDataset size: Larger datasets benefit from more powerful hardware and longer training times.\n\nA practical strategy involves starting with lightweight models that demand fewer resources and scaling up to more complex architectures only if performance improvements become necessary. Tools like StarDist and Cellpose, for example, provide efficient options that function effectively with relatively modest hardware.\nAdditionally, consider whether to train and deploy your model locally or in the cloud (Figure 9.11). Local training is often more feasible if you already have access to a compatible workstation and want full control over data and execution. However, cloud-based services like Google Colab or AWS offer access to more powerful hardware, removing the need for local infrastructure—this is especially beneficial when working with large models or 3D datasets.\nThere are four typical combinations of training and prediction workflows:\n\nTraining and prediction locally is well suited for small to medium-sized datasets, especially when computational demands are moderate and data privacy is a priority. This approach also supports some user-friendly desktop applications, such as the Cellpose 2.016, which can be run locally without requiring cloud access or advanced technical setup.\nTraining locally, prediction in the cloud may be useful when models are trained in-house but need to be deployed at scale or integrated into cloud-based pipelines.\nTraining in the cloud, prediction locally enables researchers to take advantage of powerful cloud GPUs for model development, while keeping inference close to the data source (e.g., a microscope workstation or in case of sensitive data).\nTraining and prediction in the cloud is well suited for collaborative projects or large-scale deployments, where access to centralized, scalable infrastructure is critical.\n\nChoosing between these strategies depends on your data size, hardware access, choice of software, collaboration needs, and whether your workflow prioritizes flexibility, scalability, or control.\n\n\n\n\n\n\nFigure 9.11: Training and deployment strategies for DL models in microscopy. Depending on the available tools and infrastructure, models can be trained and deployed locally or in the cloud. Modified from17.\n\n\n\n\n\n9.4.3 Ensuring Reproducibility in DL\nWhen sharing how you trained a DL model, two key elements often come to mind: the dataset used and the code that runs the model. However, in practice, reproducibility extends beyond just data and code. In programming environments like Python, which rely heavily on external libraries, ensuring reproducibility also requires capturing the exact configuration of the environment in which the model was trained.\nDL models are sensitive to changes in library versions and dependencies. Even minor differences in the software stack can result in inconsistent outcomes or training failures. While sharing a list of dependencies (e.g., a requirements.txt or a Conda environment file) is a constructive step, differences in operating systems or local setups can still lead to issues.\nA robust and increasingly popular solution is containerisation. Containers package software, dependencies, and environment settings into a portable and self-contained unit. One of the most widely used containerization tools is Docker. A Docker container can be considered a lightweight, standalone virtual machine that includes everything needed to run code, such as the operating system, libraries, and runtime, ensuring applications run consistently across different machines.\nUsing containers ensures that your model training and inference processes remain consistent, no matter who executes them or where they are conducted. This greatly simplifies the ability of collaborators or reviewers to reproduce your results.\nFor researchers unfamiliar with software development, tools like DL4MicEverywhere36 and bia-binder37 simplify the use of containers by integrating them into user-friendly Jupyter notebook environments. These platforms enable researchers to benefit from the reproducibility of containers without needing to manage complex setups or command-line tools.\nReproducibility is crucial for establishing trust in computational results and facilitating long-term scientific collaboration. To ensure your DL workflows are reproducible, follow these best practices:\n\nPin every software version used in your workflow.\n\nDocument your environment setup thoroughly.\n\nProvide a containerised version of your training and inference pipeline when possible.\n\nTaking these steps will make it easier for others to reproduce your results, build on your work, and apply your models in different research settings.\n\nFor more information on best practices, consult29.",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>How to Train and Use Deep Learning Models in Microscopy</span>"
    ]
  },
  {
    "objectID": "9-train-models.html#sec-9.5",
    "href": "9-train-models.html#sec-9.5",
    "title": "9  How to Train and Use Deep Learning Models in Microscopy",
    "section": "9.5 Summary & Outlook",
    "text": "9.5 Summary & Outlook\nSegmenting microscopy images remains a critical yet challenging task in bioimage analysis. In this chapter, we have used segmentation as a representative example to illustrate deep learning workflows and considerations. However, the strategies and best practices described here—such as data preparation, model selection, training, evaluation, and deployment—are relevant to a wide range of image analysis tasks, including classification, detection, and tracking. DL has undeniably transformed this field, offering robust solutions for segmenting complex and variable structures. However, as this chapter emphasizes, DL is not always the fastest or the best approach. Classical image processing techniques or pixel classifiers often provide faster, simpler, and highly effective alternatives in many scenarios.\nThe decision to use DL should be driven by the complexity of the task, the availability of annotated data, and the specific goals of the segmentation project. Successful DL implementations often require significant investments in data curation, annotation, and computational resources. Furthermore, training from scratch is frequently avoidable thanks to the growing ecosystem of pre-trained models and resources shared by the community.\nNotably, the landscape of DL segmentation is rapidly evolving. The emergence of foundation models, which are large, versatile networks pre-trained on vast and diverse datasets, promises to further lower the barriers to entry15. These models enable transfer learning, fine-tuning, and even zero-shot segmentation, where accurate predictions can be made on previously unseen data with minimal or no task-specific training. This shift opens exciting new avenues for researchers who previously lacked the resources or expertise to apply DL in their work.\nThe ongoing development and democratization of DL tools, along with enhancements in model generalisability, human-in-the-loop workflows, and reproducibility, are changing how microscopy data is analyzed. Still, the key to successful segmentation will always involve careful planning, quality control, and selecting the right tool for the task, whether it involves DL or not.\n\n\n\n\n\n1. Heinrich, L. et al. Whole-cell organelle segmentation in volume electron microscopy. Nature 599, 141–146 (2021).\n\n\n2. Arganda-Carreras, I. et al. Trainable Weka Segmentation: A machine learning tool for microscopy pixel classification. Bioinformatics 33, 2424–2426 (2017).\n\n\n3. Arzt, M. et al. LABKIT: Labeling and segmentation toolkit for big image data. Front. Comput. Sci. 4, (2022).\n\n\n4. Berg, S. et al. Ilastik: Interactive machine learning for (bio)image analysis. Nat. Methods 16, 1226–1232 (2019).\n\n\n5. Moen, E. et al. Deep learning for cellular image analysis. Nature Methods 16, 1233–1246 (2019).\n\n\n6. Pylvänäinen, J. W., Gómez-de-Mariscal, E., Henriques, R. & Jacquemet, G. Live-cell imaging in the deep learning era. Current Opinion in Cell Biology 85, 102271 (2023).\n\n\n7. Krizhevsky, A., Sutskever, I. & Hinton, G. E. ImageNet Classification with Deep Convolutional Neural Networks. in Advances in Neural Information Processing Systems vol. 25 (Curran Associates, Inc., 2012).\n\n\n8. Ronneberger, O., Fischer, P. & Brox, T. U-net: Convolutional networks for biomedical image segmentation. in Medical image computing and computer-assisted intervention – MICCAI 2015 (eds. Navab, N., Hornegger, J., Wells, W. M. & Frangi, A. F.) 234–241 (Springer International Publishing, Cham, 2015). doi:10.1007/978-3-319-24574-4_28.\n\n\n9. Schmidt, U., Weigert, M., Broaddus, C. & Myers, G. Cell detection with star-convex polygons. in Medical image computing and computer assisted intervention – MICCAI 2018 (eds. Frangi, A. F., Schnabel, J. A., Davatzikos, C., Alberola-López, C. & Fichtinger, G.) 265–273 (Springer International Publishing, Cham, 2018). doi:10.1007/978-3-030-00934-2_30.\n\n\n10. Stringer, C., Wang, T., Michaelos, M. & Pachitariu, M. Cellpose: A generalist algorithm for cellular segmentation. Nature Methods 18, 100–106 (2021).\n\n\n11. Bejarano, L. et al. Interrogation of endothelial and mural cells in brain metastasis reveals key immune-regulatory mechanisms. Cancer Cell 42, 378–395.e10 (2024).\n\n\n12. Fisch, D. et al. Molecular definition of the endogenous Toll-like receptor signalling pathways. Nature 631, 635–644 (2024).\n\n\n13. Fazeli, E. et al. Automated cell tracking using StarDist and TrackMate. F1000Research 9, 1279 (2020).\n\n\n14. Follain, G. et al. Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers. (2024) doi:10.1101/2024.09.30.615654.\n\n\n15. Archit, A. et al. Segment anything for microscopy. Nature Methods 22, 579–591 (2025).\n\n\n16. Pachitariu, M. & Stringer, C. Cellpose 2.0: How to train your own model. Nat. Methods 19, 1634–1641 (2022).\n\n\n17. Chamier, L. von et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nature Communications 12, 2276 (2021).\n\n\n18. Kochetov, B. et al. UNSEG: Unsupervised segmentation of cells and their nuclei in complex tissue samples. Communications Biology 7, 1–14 (2024).\n\n\n19. Liu, B. et al. Self-supervised learning reveals clinically relevant histomorphological patterns for therapeutic strategies in colon cancer. Nature Communications 16, 2328 (2025).\n\n\n20. Caicedo, J. C., McQuin, C., Goodman, A., Singh, S. & Carpenter, A. E. Weakly Supervised Learning of Single-Cell Feature Embeddings. in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition 9309–9318 (2018). doi:10.1109/CVPR.2018.00970.\n\n\n21. Moshkov, N. et al. Learning representations for image-based profiling of perturbations. Nature Communications 15, 1594 (2024).\n\n\n22. Schindelin, J. et al. Fiji: An open-source platform for biological-image analysis. Nature Methods 9, 676–682 (2012).\n\n\n23. Ahlers, J. et al. Napari: A multi-dimensional image viewer for Python. (2023) doi:10.5281/zenodo.8115575.\n\n\n24. Bankhead, P. et al. QuPath: Open source software for digital pathology image analysis. Sci. Rep. 7, 16878 (2017).\n\n\n25. Lecun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-based learning applied to document recognition. Proceedings of the IEEE 86, 2278–2324 (1998).\n\n\n26. Shorten, C. & Khoshgoftaar, T. M. A survey on Image Data Augmentation for Deep Learning. Journal of Big Data 6, 60 (2019).\n\n\n27. Lin, B. et al. A deep learned nanowire segmentation model using synthetic data augmentation. npj Computational Materials 8, 1–12 (2022).\n\n\n28. Rangel DaCosta, L., Sytwu, K., Groschner, C. K. & Scott, M. C. A robust synthetic data generation framework for machine learning in high-resolution transmission electron microscopy (HRTEM). npj Computational Materials 10, 1–11 (2024).\n\n\n29. Laine, R. F., Arganda-Carreras, I., Henriques, R. & Jacquemet, G. Avoiding a replication crisis in deep-learning-based bioimage analysis. Nature methods 18, 1136–1144 (2021).\n\n\n30. Ilievski, I., Akhtar, T., Feng, J. & Shoemaker, C. Efficient hyperparameter optimization for deep learning algorithms using deterministic RBF surrogates. Proceedings of the AAAI Conference on Artificial Intelligence 31, (2017).\n\n\n31. Alibrahim, H. & Ludwig, S. A. Hyperparameter optimization: Comparing genetic algorithm against grid search and bayesian optimization. in 2021 IEEE congress on evolutionary computation (CEC) 1551–1559 (2021). doi:10.1109/CEC45853.2021.9504761.\n\n\n32. Li, Y. & Shen, L. cC-GAN: A Robust Transfer-Learning Framework for HEp-2 Specimen Image Segmentation. IEEE Access 6, 14048–14058 (2018).\n\n\n33. Morid, M. A., Borjali, A. & Del Fiol, G. A scoping review of transfer learning research on medical image analysis using ImageNet. Computers in Biology and Medicine 128, 104115 (2021).\n\n\n34. Chen, J. et al. The Allen Cell and Structure Segmenter: A new open source toolkit for segmenting 3D intracellular structures in fluorescence microscopy images. (2020) doi:10.1101/491035.\n\n\n35. Conrad, R. & Narayan, K. Instance segmentation of mitochondria in electron microscopy images with a generalist deep learning model trained on a diverse dataset. Cell Systems 14, 58–71.e5 (2023).\n\n\n36. Hidalgo-Cenalmor, I. et al. DL4MicEverywhere: Deep learning for microscopy made flexible, shareable and reproducible. Nat. Methods 21, 925–927 (2024).\n\n\n37. Russell, C. T. et al. Bia-binder: A web-native cloud compute service for the bioimage analysis community. (2024) doi:10.48550/arXiv.2411.12662.",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>How to Train and Use Deep Learning Models in Microscopy</span>"
    ]
  },
  {
    "objectID": "10-output-quality.html",
    "href": "10-output-quality.html",
    "title": "10  Output Quality",
    "section": "",
    "text": "10.1 Metrics and Losses\nStarting prompt for this chapter: Chapter 10 addresses how to assess the quality of a model’s output, mentioning Metrics Reloaded. This chapter should address the question: how do I know my model is good enough? It should frame this discussion using the example of a segmentation model and discuss how tools can identify uncertain decisions from a model.",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Output Quality</span>"
    ]
  },
  {
    "objectID": "10-output-quality.html#metrics-and-losses",
    "href": "10-output-quality.html#metrics-and-losses",
    "title": "10  Output Quality",
    "section": "",
    "text": "difference between “metric” and “loss”\nloss:\n\nhas to be “differentiable”\nused to train the network\nshould already be close to the metric you want to use\ncan be used to assess model quality on validation data (but a custom metric might be more insightful)\n\nmetric:\n\nan application specific measure of how close you are to the ground truth\nused to select a model and to measure progress\ndoes not need to be “differentiable”\nbut if it is, that’s great, you can use it as a loss\notherwise, find a loss that is a good proxy or chose your model based on the metric on the validation dataset",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Output Quality</span>"
    ]
  },
  {
    "objectID": "10-output-quality.html#what-metric-to-pick",
    "href": "10-output-quality.html#what-metric-to-pick",
    "title": "10  Output Quality",
    "section": "10.2 What Metric to Pick?",
    "text": "10.2 What Metric to Pick?\n\nideally: metric reflects time/cost needed to clean up for a particular application\nthis can mean different things, and sometimes there is no single number (see Cell Tracking below)\nDiscuss tradeoffs in metrics choice\nInterpretation of different metrics with guidance on how to balance/trade-off priorities\n\nE.g. segmentation for counting (segmentation as object detection) vs. segmentation for size estimation",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Output Quality</span>"
    ]
  },
  {
    "objectID": "10-output-quality.html#examples",
    "href": "10-output-quality.html#examples",
    "title": "10  Output Quality",
    "section": "10.3 Examples",
    "text": "10.3 Examples\nNote: We are considering picking one example to place in call out boxes throughout the chapter to facilitate an ongoing discussion with a concrete example. For examples, we will use real data/GT, but synthetically generate the predictions in order to better highlight specific issues.\n\n10.3.1 Segmentation Metrics\n\nshow on examples:\n\ndice\nHausdorff\nAP_x (and required matching)\n\nmention MetricsReloaded\n\n\n\n10.3.2 Cell Tracking Metrics\n\nusually not a single number: topological correctness vs. positional correctness\nshow traccuracy and discuss some of their metrics",
    "crumbs": [
      "Image Analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Output Quality</span>"
    ]
  },
  {
    "objectID": "11-outlook.html",
    "href": "11-outlook.html",
    "title": "11  Outlook",
    "section": "",
    "text": "11.1 Include section headers as appropriate\nUnder your first header, include a brief introduction to your chapter.\nStarting prompt for this chapter: Chapter 11 concludes the book with a forward-looking assessment of AI in microscopy, focusing on how it can/will enable biological discovery. It should highlight a few motivational examples of discoveries that AI has already enabled and discuss opportunities to which the reader is primed to contribute after reading this book.\nUse markdown heading level two for section headers. You can use standard markdown formatting, for example emphasize the end of this sentence.\nThis is a new paragraph with more text. Your paragraphs can cross reference other items, such as Figure 11.1. Use fig to reference figures, and eq to reference equations, such as Equation 11.1.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Outlook</span>"
    ]
  },
  {
    "objectID": "11-outlook.html#include-section-headers-as-appropriate",
    "href": "11-outlook.html#include-section-headers-as-appropriate",
    "title": "11  Outlook",
    "section": "",
    "text": "11.1.1 Sub-subsection headers are also available\nTo make your sections cross reference-able throughout the book, include a section reference, as shown in the header for Section 11.4.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Outlook</span>"
    ]
  },
  {
    "objectID": "11-outlook.html#bibliography-and-citations",
    "href": "11-outlook.html#bibliography-and-citations",
    "title": "11  Outlook",
    "section": "11.2 Bibliography and Citations",
    "text": "11.2 Bibliography and Citations\nTo cite a research article, add it to references.bib and then refer to the citation key. For example, reference1 refers to CellPose and reference2 refers to ZeroCostDL4Mic.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Outlook</span>"
    ]
  },
  {
    "objectID": "11-outlook.html#adding-to-the-glossary",
    "href": "11-outlook.html#adding-to-the-glossary",
    "title": "11  Outlook",
    "section": "11.3 Adding to the Glossary",
    "text": "11.3 Adding to the Glossary\nWe are using R code to create a glossary for this book. To add a definition, edit the glossary.yml file. To reference the glossary, enclose the word as in these examples: LLMs suffer from hallucinations. It is important to understand the underlying training data, validation data and false positives to interpret your results. Clicking on the word will reveal its definition by taking you to the entry on the Glossary page. Pressing back in your browser will return you to your previous place in the textbook.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Outlook</span>"
    ]
  },
  {
    "objectID": "11-outlook.html#sec-equation",
    "href": "11-outlook.html#sec-equation",
    "title": "11  Outlook",
    "section": "11.4 Code and Equations",
    "text": "11.4 Code and Equations\nThis is an example of including a python snippet that generates a figure\n\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\n\n\n\n\n\n\nFigure 11.1: Simple Plot\n\n\n\n\n\nIn some cases, you may want to include a code-block that is not executed when the book is compiled. Use the eval: false option for this.\n\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\nFigures can also be generated that do not show the code by using the option for code-fold: true.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 11.2: A spiral on a polar axis\n\n\n\n\n\nHere is an example equation.\n\\[\ns = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\overline{x})^2}\n\\tag{11.1}\\]\n\n11.4.1 Embedding Figures\nYou can also embed figures from other notebooks in the repo as shown in the following embed example.\n\n\n\n\n\n\n\n\n\nFigure 11.3: Polar plot of circles of random areas at random coords\n\n\n\n\n\n\nWhen embedding notebooks, please store the .ipynb file in the notebook directory. Include the chapter in the name of your file. For example, chapter4_example_u-net.ipynb. This is how we will handle chapter- or example-specific environments. We will host notebooks on Google Colab so that any required packages for the code–but not for rendering the book at large–will be installed there. That way, we will not need to handle a global environment across the book.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Outlook</span>"
    ]
  },
  {
    "objectID": "11-outlook.html#quarto-has-additional-features.",
    "href": "11-outlook.html#quarto-has-additional-features.",
    "title": "11  Outlook",
    "section": "11.5 Quarto has additional features.",
    "text": "11.5 Quarto has additional features.\nYou can learn more about markdown options and additional Quarto features in the Quarto documentation. One example that you might find interesting is the option to include callouts in your text. These callouts can be used to highlight potential pitfalls or provide additional optional exercises that the reader might find helpful. Below are examples of the types of callouts available in Quarto.\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important. They can default to open (like this example) or collapsed (example below).\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThese could be good for extra material or exercises.\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThere are caveats when applying these tools. Expand the code below to learn more.\n\n\nCode\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe careful to avoid hallucinations.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis is key information.\n\n\n\n\n\n\n1. Stringer, C., Wang, T., Michaelos, M. & Pachitariu, M. Cellpose: A generalist algorithm for cellular segmentation. Nature Methods 18, 100–106 (2021).\n\n\n2. Chamier, L. von et al. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nature Communications 12, 2276 (2021).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Outlook</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Aberration\nThe spreading of light (also called ‘wavefront distortion’) due to imperfections in the optical path or variations in refractive index at the sample, which results in images that are blurrier than the ideal diffraction-limited image we would expect were aberrations absent.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#adaptive-optics",
    "href": "glossary.html#adaptive-optics",
    "title": "Glossary",
    "section": "Adaptive Optics",
    "text": "Adaptive Optics\nTechnology that senses distortions in the wavefront of light and cancels them, thereby suppressing optical aberrations to enhance image clarity.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#autoencoder",
    "href": "glossary.html#autoencoder",
    "title": "Glossary",
    "section": "Autoencoder",
    "text": "Autoencoder\nA deep learning architecture used to learn efficient coding of unlabeled data. An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#backpropagation",
    "href": "glossary.html#backpropagation",
    "title": "Glossary",
    "section": "Backpropagation",
    "text": "Backpropagation\nThe method used by neural networks to learn from its predictions. Once the prediction is done, it is compared with the ground truth through a training loss and the value of the comparison is used backwards to sequentially update the weights in the neural network, reward it when making a good prediction and punish it when making a bad prediction.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#batch",
    "href": "glossary.html#batch",
    "title": "Glossary",
    "section": "Batch",
    "text": "Batch\nA small group of data that is processed together at the same time. For example, when training a machine learning model, a batch is a group of data that is given to the model for learning. Batches are commonly used to make the processes more efficient.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#bayesian-optimization",
    "href": "glossary.html#bayesian-optimization",
    "title": "Glossary",
    "section": "Bayesian Optimization",
    "text": "Bayesian Optimization\nA strategy that allows the optimization of black-box functions such as deep neural networks. It creates a surrogate model, which is a probabilistic representation of the objective function, using only a few example points.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#binary-segmentation",
    "href": "glossary.html#binary-segmentation",
    "title": "Glossary",
    "section": "Binary Segmentation",
    "text": "Binary Segmentation\nA type of image segmentation where each pixel is classified into one of two categories—typically “foreground” (e.g., cell) or “background.” The output is a binary mask distinguishing objects (set to a value of 1) from their background (0).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#care-content-aware-image-restoration",
    "href": "glossary.html#care-content-aware-image-restoration",
    "title": "Glossary",
    "section": "CARE (Content-aware image restoration)",
    "text": "CARE (Content-aware image restoration)\nA deep learning-based method for image restoration that leverages content-specific features to enhance degraded images. See https://github.com/CSBDeep/CSBDeep for more information.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#computer-vision",
    "href": "glossary.html#computer-vision",
    "title": "Glossary",
    "section": "Computer Vision",
    "text": "Computer Vision\nA field of computer science wherein computers extract information from images. It often involves object detection within images and can involve classification of the images and/or objects.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#convolution",
    "href": "glossary.html#convolution",
    "title": "Glossary",
    "section": "Convolution",
    "text": "Convolution\nA mathematical process where a kernel (small matrix) slides over input data (e.g., images) to compute feature maps, highlighting patterns like edges or textures.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#convolutional-neural-networks-cnns",
    "href": "glossary.html#convolutional-neural-networks-cnns",
    "title": "Glossary",
    "section": "Convolutional Neural Networks (CNNs)",
    "text": "Convolutional Neural Networks (CNNs)\nA deep learning architecture that applies convolutions to automatically learn features from images for computer vision tasks like classification and detection.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#data-augmentation",
    "href": "glossary.html#data-augmentation",
    "title": "Glossary",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nA strategy to artificially increase the diversity of a dataset prior to training by applying transformations such as rotation, flipping, or brightness adjustment. It helps improve model robustness and generalisation.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#deconvolution",
    "href": "glossary.html#deconvolution",
    "title": "Glossary",
    "section": "Deconvolution",
    "text": "Deconvolution\nA mathematical process to partially reverse the blurring effect caused by the microscope’s PSF, increasing contrast and resolution over the raw image data if performed carefully.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#domain-randomization",
    "href": "glossary.html#domain-randomization",
    "title": "Glossary",
    "section": "Domain Randomization",
    "text": "Domain Randomization\nUsing simulations or synthetic training data, domain randomization applies random and exaggerated variations to background, lighting, shapes, or textures in the synthetic dataset. This strategy helps the model learn domain-invariant features and is usually used for pretraining a neural network or to enable simulation-to-real transfer.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#effect-size",
    "href": "glossary.html#effect-size",
    "title": "Glossary",
    "section": "Effect Size",
    "text": "Effect Size\nHow “strong” a phenotype is, or how mathematically possible it is to distinguish a given population from the control population.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#epoch",
    "href": "glossary.html#epoch",
    "title": "Glossary",
    "section": "Epoch",
    "text": "Epoch\nOne complete pass through the entire training dataset during the training process.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#f1-score",
    "href": "glossary.html#f1-score",
    "title": "Glossary",
    "section": "F1 Score",
    "text": "F1 Score\nA classification metric that gives the harmonic mean of precision (proportion of correct true positive predictions across all predicted positive cases) and recall (proportion of true positive predictions against the total positive cases). The harmonic mean is a method to balance both metrics equally. This metric was originally designed for binary classification but can be adapted to multiclass classification by calculating the F1 score per class.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#false-negatives",
    "href": "glossary.html#false-negatives",
    "title": "Glossary",
    "section": "False Negatives",
    "text": "False Negatives\nIn a scenario where you have two classes “positive” and “negative”, you try to predict cases as one of those classes. False negatives are the cases that you incorrectly predicted as negative and were really positive.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#false-positives",
    "href": "glossary.html#false-positives",
    "title": "Glossary",
    "section": "False Positives",
    "text": "False Positives\nIn a scenario where you have two classes “positive” and “negative”, you try to predict cases as one of those classes. False positives are the cases that you incorrectly predicted as positive and were really negative.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#fiji",
    "href": "glossary.html#fiji",
    "title": "Glossary",
    "section": "FIJI",
    "text": "FIJI\nAn image processing platform that comes bundled with many plugins for scientific image analysis. See https://imagej.net/software/fiji/ for more information.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#frequency-domain",
    "href": "glossary.html#frequency-domain",
    "title": "Glossary",
    "section": "Frequency Domain",
    "text": "Frequency Domain\nThe representation of an image as a function of spatial frequency, obtained by transforming an image into the spatial domain using the Fourier transform.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gaussian-process",
    "href": "glossary.html#gaussian-process",
    "title": "Glossary",
    "section": "Gaussian Process",
    "text": "Gaussian Process\nA common surrogate model for optimization strategies such as Bayesian Optimization. Gaussian Processes are non-parametric a case that models a conditional probability function. In the hyperparameter search scenario, the Gaussian Process models the probability of getting an objective function value based on some hyperparameters.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#generative-adversarial-networks-gans",
    "href": "glossary.html#generative-adversarial-networks-gans",
    "title": "Glossary",
    "section": "Generative Adversarial Networks (GANs)",
    "text": "Generative Adversarial Networks (GANs)\nA deep learning architecture where two neural networks, a generator and a discriminator, are trained in an adversarial process, enabling the generator to create synthetic data, such as realistic images, by learning to deceive the discriminator.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#genetic-algorithms",
    "href": "glossary.html#genetic-algorithms",
    "title": "Glossary",
    "section": "Genetic Algorithms",
    "text": "Genetic Algorithms\nAn optimisation method inspired by the principles of natural selection and genetics. It starts with a population of solutions. These solutions are combined through a process called crossover to produce new solutions (offspring). During this process, random changes or mutations may occur to introduce diversity. After crossover and mutation, a selection step chooses the best solutions from both the parent and offspring populations to form the next generation. This cycle repeats for a set number of generations or until a predefined goal or stopping criterion is met.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#ground-truth",
    "href": "glossary.html#ground-truth",
    "title": "Glossary",
    "section": "Ground Truth",
    "text": "Ground Truth\nAccurate data against which a model can be evaluated. Ground truth data is often manually annotated. The data type itself will vary depending on the task and evaluation. e.g. instance segmentation may be compared to ground truth object counts or masks.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#hallucinations",
    "href": "glossary.html#hallucinations",
    "title": "Glossary",
    "section": "Hallucinations",
    "text": "Hallucinations\nOutputs from a model that do not have a basis in the input data and may contain false or misleading information.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#hyperparameters",
    "href": "glossary.html#hyperparameters",
    "title": "Glossary",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nThe options you choose when training a machine learning model that affect the training process or the architecture of the model (e.g., learning rate, batch size, number of layers, training loss, etc.) are called hyperparameters. This term is used to differentiate them from the parameters (also known as weights) of the machine learning model.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#image-classification",
    "href": "glossary.html#image-classification",
    "title": "Glossary",
    "section": "Image Classification",
    "text": "Image Classification\nA computer vision task where each image is associated with one class and the goal of this task is to correctly predict that class.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#image-restoration",
    "href": "glossary.html#image-restoration",
    "title": "Glossary",
    "section": "Image Restoration",
    "text": "Image Restoration\nThe process of recovering clear, high-quality images from degraded raw data contaminated by blur, noise, or other distortions.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#instance-segmentation",
    "href": "glossary.html#instance-segmentation",
    "title": "Glossary",
    "section": "Instance Segmentation",
    "text": "Instance Segmentation\nA segmentation task that not only separates objects from the background but also distinguishes between individual objects of the same type (e.g., separating touching cells one by one).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#iou",
    "href": "glossary.html#iou",
    "title": "Glossary",
    "section": "IoU",
    "text": "IoU\n“Intersection over Union”. A segmentation metric that calculates the difference between the area of overlap between two segmentation masks divided by the area of union.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#manual-annotation",
    "href": "glossary.html#manual-annotation",
    "title": "Glossary",
    "section": "Manual Annotation",
    "text": "Manual Annotation\nThe process of manually labeling specific structures or objects in an image using drawing tools. Typically done in software like Fiji or Napari, this step is essential for creating ground truth data to train or evaluate machine learning models.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#metadata",
    "href": "glossary.html#metadata",
    "title": "Glossary",
    "section": "Metadata",
    "text": "Metadata\nAny data that provides additional information about other data. In bioimaging, examples include information about sample preparation, the imaging instrument, and image acquisition parameters.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#n2n-noise2noise",
    "href": "glossary.html#n2n-noise2noise",
    "title": "Glossary",
    "section": "N2N (Noise2Noise)",
    "text": "N2N (Noise2Noise)\nA supervised denoising method that trains a neural network on pairs of independently noisy images of the same scene, requiring no clean reference data but needing paired noisy inputs. See https://github.com/NVlabs/noise2noise for more information.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#n2s-noise2self",
    "href": "glossary.html#n2s-noise2self",
    "title": "Glossary",
    "section": "N2S (Noise2Self)",
    "text": "N2S (Noise2Self)\nA self-supervised denoising method that trains a neural network assuming statistically independent noise across the image, requiring only single noisy images without paired clean data. See https://github.com/czbiohub-sf/noise2self for more information.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#n2v-noise2void",
    "href": "glossary.html#n2v-noise2void",
    "title": "Glossary",
    "section": "N2V (Noise2Void)",
    "text": "N2V (Noise2Void)\nA self-supervised denoising method that trains a neural network to predict pixel values from noisy images by masking input pixels, requiring only single noisy images without paired clean data. See https://github.com/juglab/n2v for more information.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#nonlinear-problem",
    "href": "glossary.html#nonlinear-problem",
    "title": "Glossary",
    "section": "Nonlinear Problem",
    "text": "Nonlinear Problem\nA mathematical problem where the governing equations or operations are nonlinear, meaning outputs are not linearly proportional to inputs.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#object-detection",
    "href": "glossary.html#object-detection",
    "title": "Glossary",
    "section": "Object Detection",
    "text": "Object Detection\nA computer vision task that identifies and locates individual objects within an image, typically by drawing bounding boxes around them. It provides both the category (what) and position (where) of each object.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#panoptic-segmentation",
    "href": "glossary.html#panoptic-segmentation",
    "title": "Glossary",
    "section": "Panoptic Segmentation",
    "text": "Panoptic Segmentation\nA computer vision technique that is a combination of semantic segmentation and instance segmentation. It separates an image into regions while also detecting individual object instances within those regions.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#pixel-classifiers",
    "href": "glossary.html#pixel-classifiers",
    "title": "Glossary",
    "section": "Pixel Classifiers",
    "text": "Pixel Classifiers\nMachine learning models that classify each pixel in an image based on features such as intensity, texture, or local neighborhood. Commonly used in traditional workflows for segmentation or classification tasks.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#point-spread-function-psf",
    "href": "glossary.html#point-spread-function-psf",
    "title": "Glossary",
    "section": "Point Spread Function (PSF)",
    "text": "Point Spread Function (PSF)\nA mathematical function that describes how an imaging system blurs a point source.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#quality-control-metric",
    "href": "glossary.html#quality-control-metric",
    "title": "Glossary",
    "section": "Quality Control Metric",
    "text": "Quality Control Metric\nAny metric that can be used to evaluate quality. It will vary depending on the task and data type. It can be binary (e.g. an image doesn’t have debris) or continuous (e.g. annotated object centroids are within 5 pixels of the ground truth centroids).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#rcan-residual-channel-attention-network",
    "href": "glossary.html#rcan-residual-channel-attention-network",
    "title": "Glossary",
    "section": "RCAN (residual channel attention network)",
    "text": "RCAN (residual channel attention network)\nA deep learning-based method using residual learning and channel attention to improve image restoration tasks. See https://github.com/AiviaCommunity/3D-RCAN for more information.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#relu",
    "href": "glossary.html#relu",
    "title": "Glossary",
    "section": "ReLU",
    "text": "ReLU\nAn activation function common in deep learning that outputs the input directly if it is positive, and outputs zero otherwise; this characteristic helps introduce non-linearity into the model and mitigate the vanishing gradient problem.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#self-supervised-learning",
    "href": "glossary.html#self-supervised-learning",
    "title": "Glossary",
    "section": "Self-supervised learning",
    "text": "Self-supervised learning\nA deep learning method where models generate their own supervisory signals from unlabeled data, often by using pretext tasks, to learn useful representations that can be applied to various downstream tasks.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#semantic-segmentation",
    "href": "glossary.html#semantic-segmentation",
    "title": "Glossary",
    "section": "Semantic Segmentation",
    "text": "Semantic Segmentation\nA form of segmentation where each pixel in an image is assigned to a class (e.g., nucleus, cytoplasm, background), but it does not distinguish between separate instances of the same class.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#sigmoid-function",
    "href": "glossary.html#sigmoid-function",
    "title": "Glossary",
    "section": "Sigmoid Function",
    "text": "Sigmoid Function\nAn activation function common in deep learning that non-linearly maps real inputs to outputs between 0 and 1, being most sensitive to changes in inputs around zero and increasingly compressing extreme positive or negative inputs as they approach 1 or 0 respectively; this characteristic enables it to model probabilities for binary classification and introduce smooth non-linearity.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#spatial-domain",
    "href": "glossary.html#spatial-domain",
    "title": "Glossary",
    "section": "Spatial Domain",
    "text": "Spatial Domain\nThe representation of an image as a function of spatial coordinates.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#star-convex-polygon",
    "href": "glossary.html#star-convex-polygon",
    "title": "Glossary",
    "section": "Star-convex Polygon",
    "text": "Star-convex Polygon\nA geometric shape used in segmentation algorithms like StarDist. Imagine drawing straight lines (rays) from the centre of an object out toward its edges—if you can see the edge from the centre in all directions, the object is considered star-convex. This method works well for blob-like structures such as nuclei, because their general shape can be captured by measuring how far each ray travels from the centre to the boundary.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#supervised-learning",
    "href": "glossary.html#supervised-learning",
    "title": "Glossary",
    "section": "Supervised learning",
    "text": "Supervised learning\nA deep learning method where models learn from labeled data (input-output pairs), enabling them to learn a mapping function for making predictions or decisions on unseen inputs.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#training-data",
    "href": "glossary.html#training-data",
    "title": "Glossary",
    "section": "Training Data",
    "text": "Training Data\nData used to train an algorithm to make predictions.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#transfer-learning",
    "href": "glossary.html#transfer-learning",
    "title": "Glossary",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nA deep learning technique that reuses a model pre-trained on one task as the starting point for a new, related task, leveraging its learned knowledge to improve performance or reduce training requirements. In practice, part of a pretrained neural network (usually the initial layers, responsible for feature extraction) is frozen and reused in a new model. These frozen layers, with the knowledge from a previous dataset, are combined with untrained layers tailored for a specific bioimaging task. During training, only the new layers will be updated, allowing the model to adapt to the new task with limited data.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#transformer-models",
    "href": "glossary.html#transformer-models",
    "title": "Glossary",
    "section": "Transformer Models",
    "text": "Transformer Models\nA deep learning architecture based on the multi-head attention mechanism; specifically referring to the ‘vision transformer’ architecture. A vision transformer (ViT) is a transformer designed for computer vision. A ViT decomposes an input image into a series of patches (rather than text into tokens), serializes each patch into a vector, and maps it to a smaller dimension with a single matrix multiplication. These vector embeddings are then processed by a transformer encoder as if they were token embeddings.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#true-negatives",
    "href": "glossary.html#true-negatives",
    "title": "Glossary",
    "section": "True Negatives",
    "text": "True Negatives\nIn a scenario where you have two classes “positive” and “negative”, you try to predict cases as one of those classes. True positives are the cases that you predicted as negative and were really negative.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#true-positives",
    "href": "glossary.html#true-positives",
    "title": "Glossary",
    "section": "True Positives",
    "text": "True Positives\nIn a scenario where you have two classes “positive” and “negative”, you try to predict cases as one of those classes. True positives are the cases that you predicted as positive and were really positive.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#virtual-machine",
    "href": "glossary.html#virtual-machine",
    "title": "Glossary",
    "section": "Virtual Machine",
    "text": "Virtual Machine\nOn a physical computer, you install an operating system (e.g., Windows or Ubuntu) that you interact with. A virtual machine is a program that simulates a complete computer with its own operating system. This lets you run a “computer inside your computer” (e.g., using Linux inside Windows or the other way around). As this simulated computer is separate from your physical one, it adds an extra layer of security, because unless the user specifically allows it, the virtual machine cannot access or connect to your real computer.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#zernike-modes",
    "href": "glossary.html#zernike-modes",
    "title": "Glossary",
    "section": "Zernike Modes",
    "text": "Zernike Modes\nA set of orthogonal polynomials used to describe and correct wavefront aberrations in optical systems.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "1. Balasubramanian, H., Hobson, C. M., Chew, T.-L.\n& Aaron, J. S. Imagining the future\nof optical microscopy: Everything, everywhere, all at once.\nCommunications Biology 6, 1096 (2023).\n\n\n2. Wait, E. C., Reiche, M. A. & Chew, T.-L. Hypothesis-driven quantitative\nfluorescence microscopy – the importance of reverse-thinking in\nexperimental design. Journal of Cell Science\n133, jcs250027 (2020).\n\n\n3. Lee,\nR. M., Eisenman, L. R., Khuon, S., Aaron, J. S. & Chew, T.-L. Believing is seeing – the\ndeceptive influence of bias in quantitative microscopy. Journal\nof Cell Science 137, jcs261567 (2024).\n\n\n4. Stringer, C., Wang, T., Michaelos, M. &\nPachitariu, M. Cellpose: A generalist\nalgorithm for cellular segmentation. Nature Methods\n18, 100–106 (2021).\n\n\n5. Chamier, L. von et al. Democratising deep\nlearning for microscopy with ZeroCostDL4Mic. Nature\nCommunications 12, 2276 (2021).\n\n\n6. Guo,\nM. et al. Deep learning-based\naberration compensation improves contrast and resolution in fluorescence\nmicroscopy. Nature Communications 16, 313\n(2025).\n\n\n7. Wu,\nY. & Shroff, H. Multiscale\nfluorescence imaging of living samples. Histochemistry and Cell\nBiology 158, 301–323 (2022).\n\n\n8. Schermelleh, L., Heintzmann, R. &\nLeonhardt, H. A guide to\nsuper-resolution fluorescence microscopy. Journal of Cell\nBiology 190, 165–175 (2010).\n\n\n9. Archit, A. et al. Segment anything for\nmicroscopy. Nature Methods 22, 579–591\n(2025).\n\n\n10. Sahl, S. J., Hell, S. W. & Jakobs, S. Fluorescence nanoscopy in\ncell biology. Nature Reviews Molecular Cell Biology\n18, 685–701 (2017).\n\n\n11. Schermelleh, L. et al. Super-resolution\nmicroscopy demystified. Nature Cell Biology\n21, 72–84 (2019).\n\n\n12. Ji,\nN. Adaptive optical\nfluorescence microscopy. Nature Methods\n14, 374–380 (2017).\n\n\n13. Hampson, K. M. et al. Adaptive optics for\nhigh-resolution imaging. Nature Reviews Methods Primers\n1, 68 (2021).\n\n\n14. Shroff, H., Testa, I., Jug, F. & Manley, S.\nLive-cell imaging\npowered by computation. Nature Reviews Molecular Cell\nBiology 25, 443–463 (2024).\n\n\n15. Venkatesh, M., Mohan, K. & Seelamantula, C.\nS. Directional bilateral\nfilters for smoothing fluorescence microscopy images. AIP\nAdvances 5, 084805 (2015).\n\n\n16. Danielyan, A., Wu, Y.-W., Shih, P.-Y.,\nDembitskaya, Y. & Semyanov, A. Denoising of\ntwo-photon fluorescence images with block-matching 3D filtering.\nMethods 68, 308–316 (2014).\n\n\n17. Zhang, Y. et al. A poisson-gaussian\ndenoising dataset with real fluorescence microscopy images. in 2019\nIEEE/CVF conference on computer vision and pattern recognition\n(CVPR) 11702–11710 (Optica Publishing Group, 2019). doi:10.1109/CVPR.2019.01198.\n\n\n18. Li,\nJ., Luisier, F. & Blu, T. Pure-let deconvolution of 3D fluorescence\nmicroscopy images. in 2017 IEEE 14th international symposium on\nbiomedical imaging (ISBI 2017) 723–727 (2017). doi:10.1109/ISBI.2017.7950621.\n\n\n19. Makitalo, M. & Foi, A. Optimal inversion of the\ngeneralized anscombe transformation for poisson-gaussian noise.\nIEEE Transactions on Image Processing 22,\n91–103 (2013).\n\n\n20. Luisier, F., Vonesch, C., Blu, T. & Unser,\nM. Fast\ninterscale wavelet denoising of poisson-corrupted images. Signal\nProcessing 90, 415–427 (2010).\n\n\n21. Walt, S. van der et al. Scikit-image: Image processing in\npython. PeerJ 2, e453 (2014).\n\n\n22. Wiener, N. Extrapolation, Interpolation,\nand Smoothing of Stationary Time Series: With Engineering\nApplications. (The MIT Press, 1949). doi:10.7551/mitpress/2946.001.0001.\n\n\n23. Tikhonov, A. N. Solution of incorrectly\nformulated problems and the regularization method. Soviet Math.\nDokl. 4, 1035–1038 (1963).\n\n\n24. Miller, K. Least squares methods for\nill-posed problems with a prescribed bound. SIAM Journal on\nMathematical Analysis 1, 52–74 (1970).\n\n\n25. Beck, A. & Teboulle, M. A fast iterative\nshrinkage-thresholding algorithm for linear inverse problems.\nSIAM Journal on Imaging Sciences 2, 183–202\n(2009).\n\n\n26. Lucy, L. B. An iterative\ntechnique for the rectification of observed distributions.\nAstronomical Journal 79, 745 (1974).\n\n\n27. Richardson, W. H. Bayesian-based iterative\nmethod of image restoration*.\nJ. Opt. Soc. Am. 62, 55–59 (1972).\n\n\n28. Sarder, P. & Nehorai, A. Deconvolution methods\nfor 3-d fluorescence microscopy images. IEEE Signal Processing\nMagazine 23, 32–45 (2006).\n\n\n29. Goodwin, P. C. Chapter 10 -\nquantitative deconvolution microscopy. in Quantitative imaging\nin cell biology (eds. Waters, J. C. & Wittman, T.) vol. 123\n177–192 (Academic Press, 2014).\n\n\n30. Guo, M. et al. Rapid image\ndeconvolution and multiview fusion for optical microscopy.\nNature Biotechnology 38, 1337–1346\n(2020).\n\n\n31. Schindelin, J. et al. Fiji: An open-source platform\nfor biological-image analysis. Nature Methods\n9, 676–682 (2012).\n\n\n32. Sage, D. et al. DeconvolutionLab2: An\nopen-source software for deconvolution microscopy. Methods\n115, 28–41 (2017).\n\n\n33. Bazin, P.-L. et al. Volumetric\nneuroimage analysis extensions for the MIPAV software package.\nJournal of Neuroscience Methods 165, 111–121\n(2007).\n\n\n34. Booth, M. J. Adaptive optics in\nmicroscopy. Philosophical Transactions: Mathematical, Physical\nand Engineering Sciences 365, 2829–2843\n(2007).\n\n\n35. Hell, S. W. Far-field optical\nnanoscopy. Science 316, 1153–1158\n(2007).\n\n\n36. Vicidomini, G., Bianchini, P. & Diaspro, A.\nSTED super-resolved\nmicroscopy. Nature Methods 15, 173–182\n(2018).\n\n\n37. Wu,\nY. & Shroff, H. Faster, sharper, and\ndeeper: Structured illumination microscopy for biological imaging.\nNature Methods 15, 1011–1019 (2018).\n\n\n38. Chen, F., Tillberg, P. W. & Boyden, E. S.\nExpansion\nmicroscopy. Science 347, 543–548\n(2015).\n\n\n39. Wassie, A. T., Zhao, Y. & Boyden, E. S. Expansion microscopy:\nPrinciples and uses in biological research. Nature Methods\nvol. 16 33–41 (2019).\n\n\n40. Valli, J. et al. Seeing beyond the\nlimit: A guide to choosing the right super-resolution microscopy\ntechnique. Journal of Biological Chemistry\n297, 100791 (2021).\n\n\n41. Chen, H. et al. Advancements and practical\nconsiderations for biophysical research: Navigating the challenges and\nfuture of super-resolution microscopy. Chemical\n& Biomedical Imaging 2, 331–344\n(2024).\n\n\n42. Hagen, G. M. et al. Fluorescence\nmicroscopy datasets for training deep neural networks.\nGigaScience 10, giab032 (2021).\n\n\n43. Weigert, M. et al. Content-aware image\nrestoration: Pushing the limits of fluorescence microscopy.\nNature Methods 15, 1090–1097 (2018).\n\n\n44. Chen, J. et al. Three-dimensional\nresidual channel attention networks denoise and sharpen fluorescence\nmicroscopy image volumes. Nature Methods\n18, 678–687 (2021).\n\n\n45. Qiao, C. et al. Evaluation and\ndevelopment of deep neural networks for image super-resolution in\noptical microscopy. Nature Methods 18,\n194–202 (2021).\n\n\n46. Hou, X. et al. HD2Net: A deep learning\nframework for simultaneous denoising and deaberration in fluorescence\nmicroscopy. Opt. Express 33, 27317–27333\n(2025).\n\n\n47. Zhang, K., Zuo, W., Chen, Y., Meng, D. &\nZhang, L. Beyond a\ngaussian denoiser: Residual learning of deep CNN for image\ndenoising. IEEE Transactions on Image Processing\n26, 3142–3155 (2017).\n\n\n48. Dabov, K., Foi, A., Katkovnik, V. &\nEgiazarian, K. Image\ndenoising by sparse 3-d transform-domain collaborative filtering.\nIEEE Transactions on Image Processing 16,\n2080–2095 (2007).\n\n\n49. Krull, A., Buchholz, T.-O. & Jug, F.\nNoise2Void - learning denoising from single noisy images. in 2019\nIEEE/CVF conference on computer vision and pattern recognition\n(CVPR) 2124–2132 (2019). doi:10.1109/CVPR.2019.00223.\n\n\n50. Batson, J. & Royer, L. Noise2Self:\nBlind denoising by self-supervision. in Proceedings of the 36th\ninternational conference on machine learning (eds. Chaudhuri, K.\n& Salakhutdinov, R.) vol. 97 524–533 (PMLR, 2019).\n\n\n51. Lehtinen, J. et al. Noise2Noise:\nLearning image restoration without clean data. in Proceedings of\nthe 35th international conference on machine learning (eds. Dy, J.\n& Krause, A.) vol. 80 2965–2974 (PMLR, 2018).\n\n\n52. Li,\nY. et al. Incorporating the\nimage formation process into deep learning improves network\nperformance. Nature Methods 19, 1427–1437\n(2022).\n\n\n53. Yanny, K., Monakhova, K., Shuai, R. W. &\nWaller, L. Deep learning\nfor fast spatially varying deconvolution. Optica\n9, 96–99 (2022).\n\n\n54. Saha, D. et al. Practical sensorless aberration\nestimation for 3D microscopy with deep learning. Opt.\nExpress 28, 29044–29053 (2020).\n\n\n55. Kang, I., Zhang, Q., Yu, S. X. & Ji, N. Coordinate-based\nneural representations for computational adaptive optics in widefield\nmicroscopy. Nature Machine Intelligence 6,\n714–725 (2024).\n\n\n56. Kang, I. et al. Adaptive optical\ncorrection in in vivo two-photon fluorescence microscopy with neural\nfields. bioRxiv (2024) doi:10.1101/2024.10.20.619284.\n\n\n57. Fersini, F. et al. Wavefront estimation through\nstructured detection in laser scanning microscopy. Biomed. Opt.\nExpress 16, 2135–2155 (2025).\n\n\n58. Zhou, Y., Jin, Z., Zhao, Q., Xiong, B. &\nCao, X. Aberration\nmodeling in deep learning for volumetric reconstruction of light-field\nmicroscopy. Laser & Photonics Reviews\n17, 2300154 (2023).\n\n\n59. Qiao, C. et al. Deep learning-based optical\naberration estimation enables offline digital adaptive optics and\nsuper-resolution imaging. Photon. Res. 12,\n474–484 (2024).\n\n\n60. Hu,\nL., Hu, S., Gong, W. & Si, K. Image enhancement for\nfluorescence microscopy based on deep learning with prior knowledge of\naberration. Opt. Lett. 46, 2055–2058\n(2021).\n\n\n61. Wang, H. et al. Deep learning enables\ncross-modality super-resolution in fluorescence microscopy.\nNature Methods 16, 103–110 (2019).\n\n\n62. Park, H. et al. Deep learning enables\nreference-free isotropic super-resolution for volumetric fluorescence\nmicroscopy. Nature Communications 13, 3297\n(2022).\n\n\n63. Ning, K. et al. Deep self-learning\nenables fast, high-fidelity isotropic resolution restoration for\nvolumetric fluorescence microscopy. Light: Science\n& Applications 12, 204\n(2023).\n\n\n64. Ronneberger, O., Fischer, P. & Brox, T.\nU-net: Convolutional networks for biomedical image segmentation. in\nMedical image computing and computer-assisted intervention – MICCAI\n2015 (eds. Navab, N., Hornegger, J., Wells, W. M. & Frangi, A.\nF.) 234–241 (Springer International Publishing, Cham, 2015). doi:10.1007/978-3-319-24574-4_28.\n\n\n65. Ji,\nZ., Li, J. D. & Telgarsky, M. Early-stopped\nneural networks are consistent. in Advances in neural\ninformation processing systems 34 - 35th conference on neural\ninformation processing systems, NeurIPS 2021 (eds. Ranzato, M.,\nBeygelzimer, A., Dauphin, Y., Liang, {Percy. S. }. & Vaughan}, J.\n{Wortman) 1805–1817 (Neural information processing systems foundation,\n2021).\n\n\n66. Santos, C. F. G. D. & Papa, J. P. Avoiding overfitting: A survey on\nregularization methods for convolutional neural networks. ACM\nComput. Surv. 54, (2022).\n\n\n67. Miseta, T., Fodor, A. & Vathy-Fogarassy, Á.\nSurpassing early\nstopping: A novel correlation-based stopping criterion for neural\nnetworks. Neurocomputing 567, 127028\n(2024).\n\n\n68. Shah, Z. H. et al. Image restoration in\nfrequency space using complex-valued CNNs. Frontiers in\nArtificial Intelligence Volume 7 - 2024,\n(2024).\n\n\n69. Liu, J., Gao, F., Zhang, L. & Yang, H. A saturation artifacts\ninpainting method based on two-stage GAN for fluorescence microscope\nimages. Micromachines 15, (2024).\n\n\n70. Bouchard, C. et al. Resolution enhancement\nwith a task-assisted GAN to guide optical nanoscopy image analysis and\nacquisition. Nature Machine Intelligence\n5, 830–844 (2023).\n\n\n71. Qiao, C. et al. Rationalized deep\nlearning super-resolution microscopy for sustained live imaging of rapid\nsubcellular processes. Nature Biotechnology\n41, 367–377 (2023).\n\n\n72. Zhong, L., Liu, G. & Yang, G. Blind denoising of\nfluorescence microscopy images using GAN-based global noise\nmodeling. in ISBI 863–867 (2021).\n\n\n73. Park, E. et al. Unsupervised\ninter-domain transformation for virtually stained high-resolution\nmid-infrared photoacoustic microscopy using explainable deep\nlearning. Nature Communications 15, 10892\n(2024).\n\n\n74. Osuna-Vargas, P. et al. Denoising\ndiffusion models for high-resolution microscopy image restoration. in\n2025 IEEE/CVF winter conference on applications of computer vision\n(WACV) 4320–4330 (2025). doi:10.1109/WACV61041.2025.00424.\n\n\n75. Zhang, Y. et al. Image super-resolution using\nvery deep residual channel attention networks. (2018).\n\n\n76. Wang, Z., Bovik, A. C., Sheikh, H. R. &\nSimoncelli, E. P. Image quality assessment:\nFrom error visibility to structural similarity. IEEE\nTransactions on Image Processing 13, 600–612\n(2004).\n\n\n77. Gonzalez, R. C. & Woods, R. E. Digital Image\nProcessing. (Prentice Hall, 2008).\n\n\n78. Haase, R., Tischer, C., Bankhead, P., Miura, K.\n& Cimini, B. A call\nfor FAIR and open-access training materials to advance\nBioImage analysis. (2024).\n\n\n79. Ljosa, V., Sokolnicki, K. L. & Carpenter,\nA. E. Annotated\nhigh-throughput microscopy image sets for validation. Nat.\nMethods 9, 637 (2012).\n\n\n80. Cimini, B. A. When\nto say ’good enough’. (2019).\n\n\n81. Jamali, N., Dobson, E. T. A., Eliceiri, K. W.,\nCarpenter, A. E. & Cimini, B. A. 2020\nBioImage analysis survey: Community experiences and needs\nfor the future. Biological Imaging 1, e4\n(2021).\n\n\n82. Sivagurunathan, S. et al. Bridging imaging users to\nimaging analysis - a community survey. J. Microsc.\n(2023).\n\n\n83. Schindelin, J., Rueden, C. T., Hiner, M. C.\n& Eliceiri, K. W. The\nImageJ ecosystem: An open platform for biomedical image\nanalysis. Mol. Reprod. Dev. 82, 518–529\n(2015).\n\n\n84. Napari: A multi-dimensional image viewer for\npython. doi:10.5281/zenodo.3555620.\n\n\n85. Stirling, D. R. et al. CellProfiler\n4: Improvements in speed, utility and usability. BMC\nBioinformatics 22, 433 (2021).\n\n\n86. Bankhead, P. et al. QuPath:\nOpen source software for digital pathology image analysis. Sci.\nRep. 7, 16878 (2017).\n\n\n87. Chaumont, F. de et al. Icy: An open bioimage\ninformatics platform for extended reproducible research. Nat.\nMethods 9, 690–696 (2012).\n\n\n88. Berg, S. et al. Ilastik: Interactive\nmachine learning for (bio)image analysis. Nat. Methods\n16, 1226–1232 (2019).\n\n\n89. Arzt, M. et al. LABKIT:\nLabeling and segmentation toolkit for big image data. Front.\nComput. Sci. 4, (2022).\n\n\n90. Schmidt, U., Weigert, M., Broaddus, C. &\nMyers, G. Cell detection with star-convex polygons. in Medical image\ncomputing and computer assisted intervention – MICCAI 2018 (eds.\nFrangi, A. F., Schnabel, J. A., Davatzikos, C., Alberola-López, C. &\nFichtinger, G.) 265–273 (Springer International Publishing, Cham, 2018).\ndoi:10.1007/978-3-030-00934-2_30.\n\n\n91. Goldsborough, T. et al. InstanSeg:\nAn embedding-based instance segmentation algorithm optimized for\naccurate, efficient and portable cell segmentation. arXiv\n[cs.CV] (2024).\n\n\n92. Pachitariu, M. & Stringer, C. Cellpose 2.0: How to\ntrain your own model. Nat. Methods 19,\n1634–1641 (2022).\n\n\n93. Zhang, C. et al. Bio-image informatics\nindex BIII: A unique database of image analysis tools and\nworkflows for and by the bioimaging community. arXiv\n[q-bio.QM] (2023).\n\n\n94. Ouyang, W. et al.\nBioImage model zoo: A community-driven resource for\naccessible deep learning in BioImage analysis.\nbioRxiv 2022.06.07.495102 (2022) doi:10.1101/2022.06.07.495102.\n\n\n95. Rueden, C. T. et al. Scientific community\nimage forum: A discussion forum for scientific image software.\nPLoS Biol. 17, e3000340 (2019).\n\n\n96. Gómez-de-Mariscal, E. et al. DeepImageJ:\nA user-friendly environment to run deep learning models in\nImageJ. Nat. Methods 18,\n1192–1195 (2021).\n\n\n97. Ouyang, W., Mueller, F., Hjelmare, M.,\nLundberg, E. & Zimmer, C. ImJoy: An\nopen-source computational platform for the deep learning era.\nNat. Methods 16, 1199–1200 (2019).\n\n\n98. Shah, R., Gogoberidze, N. & Cimini, B. Bilayers.\n\n\n99. Hidalgo-Cenalmor, I. et al. DL4MicEverywhere:\nDeep learning for microscopy made flexible, shareable and\nreproducible. Nat. Methods 21, 925–927\n(2024).\n\n\n100. Kluyver, T. et al. Jupyter notebooks - a\npublishing format for reproducible computational workflows. in\nPositioning and power in academic publishing: Players, agents and\nagendas (eds. Loizides, F. & Scmidt, B.) 87–90 (IOS Press,\nNetherlands, 2016).\n\n\n101. Kreshuk, A. & Zhang, C. Machine\nlearning: Advanced image segmentation using ilastik. Methods\nMol. Biol. 2040, 449–463 (2019).\n\n\n102. Arganda-Carreras, I. et al. Trainable\nWeka Segmentation: A machine learning tool for\nmicroscopy pixel classification. Bioinformatics\n33, 2424–2426 (2017).\n\n\n103. Fazeli, E. et al. Automated cell\ntracking using StarDist and TrackMate.\nF1000Research 9, 1279 (2020).\n\n\n104. Krizhevsky, A., Sutskever, I. & Hinton, G.\nE. ImageNet\nClassification with Deep\nConvolutional Neural\nNetworks. in Advances in Neural\nInformation Processing\nSystems vol. 25 (Curran Associates, Inc., 2012).\n\n\n105. Ahlers, J. et al. Napari: A\nmulti-dimensional image viewer for Python. (2023) doi:10.5281/zenodo.8115575.\n\n\n106. Russell, C. T. et al. Bia-binder:\nA web-native cloud compute service for the bioimage\nanalysis community. (2024) doi:10.48550/arXiv.2411.12662.\n\n\n107. Follain, G. et al. Fast label-free\nlive imaging reveals key roles of flow dynamics and\nCD44-HA interaction in cancer cell arrest on\nendothelial monolayers. (2024) doi:10.1101/2024.09.30.615654.\n\n\n108. Moen, E. et al. Deep learning for\ncellular image analysis. Nature Methods\n16, 1233–1246 (2019).\n\n\n109. Pylvänäinen, J. W., Gómez-de-Mariscal, E.,\nHenriques, R. & Jacquemet, G. Live-cell imaging in\nthe deep learning era. Current Opinion in Cell Biology\n85, 102271 (2023).\n\n\n110. Laine, R. F., Arganda-Carreras, I., Henriques,\nR. & Jacquemet, G. Avoiding a replication\ncrisis in deep-learning-based bioimage analysis. Nature\nmethods 18, 1136–1144 (2021).\n\n\n111. Heinrich, L. et al. Whole-cell organelle\nsegmentation in volume electron microscopy. Nature\n599, 141–146 (2021).\n\n\n112. Liu, B. et al. Self-supervised\nlearning reveals clinically relevant histomorphological patterns for\ntherapeutic strategies in colon cancer. Nature\nCommunications 16, 2328 (2025).\n\n\n113. Moshkov, N. et al. Learning\nrepresentations for image-based profiling of perturbations.\nNature Communications 15, 1594 (2024).\n\n\n114. Caicedo, J. C., McQuin, C., Goodman, A., Singh,\nS. & Carpenter, A. E. Weakly Supervised\nLearning of Single-Cell\nFeature Embeddings. in 2018\nIEEE/CVF Conference on\nComputer Vision and Pattern\nRecognition 9309–9318 (2018). doi:10.1109/CVPR.2018.00970.\n\n\n115. Li, Y. & Shen, L. cC-GAN: A\nRobust Transfer-Learning\nFramework for HEp-2 Specimen\nImage Segmentation. IEEE Access\n6, 14048–14058 (2018).\n\n\n116. Morid, M. A., Borjali, A. & Del Fiol, G. A scoping review\nof transfer learning research on medical image analysis using\nImageNet. Computers in Biology and Medicine\n128, 104115 (2021).\n\n\n117. Kochetov, B. et al. UNSEG:\nUnsupervised segmentation of cells and their nuclei in complex tissue\nsamples. Communications Biology 7, 1–14\n(2024).\n\n\n118. Chen, J. et al. The Allen\nCell and Structure Segmenter: A\nnew open source toolkit for segmenting 3D intracellular\nstructures in fluorescence microscopy images. (2020) doi:10.1101/491035.\n\n\n119. Conrad, R. & Narayan, K. Instance segmentation\nof mitochondria in electron microscopy images with a generalist deep\nlearning model trained on a diverse dataset. Cell Systems\n14, 58–71.e5 (2023).\n\n\n120. Fisch, D. et al. Molecular definition\nof the endogenous Toll-like receptor signalling\npathways. Nature 631, 635–644\n(2024).\n\n\n121. Bejarano, L. et al. Interrogation of\nendothelial and mural cells in brain metastasis reveals key\nimmune-regulatory mechanisms. Cancer Cell\n42, 378–395.e10 (2024).\n\n\n122. Rangel DaCosta, L., Sytwu, K., Groschner, C. K.\n& Scott, M. C. A robust synthetic\ndata generation framework for machine learning in high-resolution\ntransmission electron microscopy (HRTEM). npj\nComputational Materials 10, 1–11 (2024).\n\n\n123. Lin, B. et al. A deep learned\nnanowire segmentation model using synthetic data augmentation.\nnpj Computational Materials 8, 1–12\n(2022).\n\n\n124. Shorten, C. & Khoshgoftaar, T. M. A survey on\nImage Data Augmentation for\nDeep Learning. Journal of Big\nData 6, 60 (2019).\n\n\n125. Lecun, Y., Bottou, L., Bengio, Y. &\nHaffner, P. Gradient-based\nlearning applied to document recognition. Proceedings of the\nIEEE 86, 2278–2324 (1998).\n\n\n126. Alibrahim, H. & Ludwig, S. A.\nHyperparameter optimization: Comparing genetic algorithm against grid\nsearch and bayesian optimization. in 2021 IEEE congress on\nevolutionary computation (CEC) 1551–1559 (2021). doi:10.1109/CEC45853.2021.9504761.\n\n\n127. Ilievski, I., Akhtar, T., Feng, J. &\nShoemaker, C. Efficient hyperparameter\noptimization for deep learning algorithms using deterministic RBF\nsurrogates. Proceedings of the AAAI Conference on Artificial\nIntelligence 31, (2017).",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "6-image-restoration-appendix.html",
    "href": "6-image-restoration-appendix.html",
    "title": "Appendix A — 3D-RCAN for Image Restoration",
    "section": "",
    "text": "A.1 Brief Introduction to the Model\nResidual Channel Attention Networks (RCAN1), are deep learning architectures initially designed for 2D super-resolution images, leveraging residual learning and channel attention mechanisms to prioritize high-frequency details. Their hierarchical “residual-in-residual” structure and adaptive channel-wise feature scaling enable superior recovery of fine textures in natural images. Researchers adapted RCAN for 3D fluorescence microscopy data to address challenges in volumetric imaging. This 3D-RCAN2 implementation modifies the original architecture by replacing 2D convolutions with 3D operations, optimizing parameters (e.g., residual groups, channel dimensions) for GPU efficiency, and processing spatiotemporal (4D) data. This adaptation enables denoising and resolution enhancement of volumetric time-lapse microscopy, achieving prolonged live-cell imaging with minimal photo bleaching. By training on paired low/high-quality datasets (e.g., confocal \\(\\leftrightarrow\\) STED or iSIM \\(\\leftrightarrow\\) expansion microscopy), 3D-RCAN restores sub cellular structures, improves lateral and axial resolution, and outperforms existing networks (e.g., CARE)3) in fidelity metrics (e.g., SSIM4 or peak signal to noise ratio). Its applications span fixed and live-cell studies, offering a versatile tool to overcome trade offs between spatiotemporal resolution and phototoxicity in fluorescence microscopy.\nSeveral features of this neural network enable it to handle complex image restoration tasks effectively, as described in the paragraphs below. In later sections, we will employ this model as an example to show you how to undertake AI restoration in fluorescence microscopy.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>3D-RCAN for Image Restoration</span>"
    ]
  },
  {
    "objectID": "6-image-restoration-appendix.html#brief-introduction-to-the-model",
    "href": "6-image-restoration-appendix.html#brief-introduction-to-the-model",
    "title": "Appendix A — 3D-RCAN for Image Restoration",
    "section": "",
    "text": "A.1.1 Residual Learning\n3D-RCAN uses a residual learning approach, which means that the network learns the difference (or residual) between the input image and the desired output. Instead of learning the entire image transformation directly, the network focuses on predicting the fine details that need improvement. This makes it easier for the network to capture high-frequency details (like edges and textures) while ignoring unnecessary background information. The network architecture is built using a residual-in-residual structure. This means that within each “residual block” (a unit of processing), there are even smaller “residual blocks.” This hierarchical design improves the network’s ability to learn intricate patterns in the data, allowing for better restoration of fine image details. The network essentially learns to focus on increasingly refined features at different levels of abstraction.\n3D-RCAN incorporates skip connections in its architecture. Skip connections are shortcuts that allow the input image to bypass certain network layers and pass directly to later layers. There are long skip connections (which bypass multiple layers) and short skip connections (which skip only a few layers). These connections enable the network to focus on high-frequency information and avoid losing important details that may be “washed out” in deeper layers.\n\n\nA.1.2 Channel Attention Mechanism\nOne of the distinctive features of 3D-RCAN is its channel attention mechanism. In any image, different features (such as color or depth in the case of microscopy images) have different levels of influence in the image. The channel attention mechanism allows 3D-RCAN to focus on the most important channels by adaptively adjusting the weight given to each feature channel. This helps the network to enhance high-resolution details in relevant channels while minimizing the influence of less important features.\n\n\nA.1.3 3D Adaptation for Image Volumes\n3D-RCAN is designed to handle not only 2D images but also 3D image volumes. In many microscopy techniques, images are captured as a series of 2D planes stacked together to form a 3D volume. RCAN adapts its structure to process these multi-dimensional data efficiently. By working in 3D, the network can improve both the lateral (XY) and axial (Z) resolution of the image volumes, which is especially important in applications like fluorescence microscopy.\n\n\nA.1.4 Channel Reduction for Efficient 3D Processing\nBy reducing channel dimensions before processing and restoring them afterward, the network minimizes redundant computations while retaining essential spatial and structural details. This mechanism enables deeper architectures without overwhelming GPU memory, particularly crucial for large 3D microscopy volumes. Coupled with channel attention, the compressed features are dynamically recalibrated to prioritize high-frequency details, ensuring robust restoration of fine structures like microtubules or mitochondrial membranes. The approach optimizes both training speed and inference performance, validated across synthetic and experimental datasets.\n\n\n\n\n\n\nFigure A.1: The architecture of 3D-RCAN. This diagram illustrates the 3D Residual Channel Attention Network (RCAN) architecture for volumetric resolution enhancement. The low-resolution (LR) 3D input progresses through cascaded Residual Groups (RG), each containing Residual Channel Attention Blocks (RCAB) with channel attention to prioritize critical features. Skip connections preserve structural details across groups. The upscale module progressively enhances spatial resolution, culminating in the high-resolution (HR) 3D output.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>3D-RCAN for Image Restoration</span>"
    ]
  },
  {
    "objectID": "6-image-restoration-appendix.html#overview-of-the-code-and-data",
    "href": "6-image-restoration-appendix.html#overview-of-the-code-and-data",
    "title": "Appendix A — 3D-RCAN for Image Restoration",
    "section": "A.2 Overview of the Code and Data",
    "text": "A.2 Overview of the Code and Data\n\nA.2.1 System Requirements\nThe basic system requirements are:\n\nOperating System: Windows 10, Linux, or Mac OS\nPython 3.7\nGPU: NVIDIA GPU with CUDA 10.0 and cuDNN 7.6.5\n\nFor more information about system requirements, please refer to the 3D-RCAN README.\n\n\nA.2.2 Installation of Dependencies\n3D-RCAN itself does not require installation and installation of the required the dependencies takes few seconds on a typical PC. The installation process involves three main steps:\n1. Check system requirements: Ensure your system meets the basic requirements listed above, particularly Python 3.7 and CUDA compatibility.\n2. Create a new virtual environment: Set up an isolated Python environment to avoid conflicts with other projects and ensure reproducibility.\n3. Install dependencies: Download the repository and install required Python packages using the requirements.txt file provided in the 3D-RCAN respository.\n\n\nA.2.3 Quick Setup Steps\n\nClone the repository:\ngit clone https://github.com/AiviaCommunity/3D-RCAN.git\nCreate and activate virtual environment:\n# Create virtual environment\npython -m venv RCAN3D\n\n# Activate (Windows)\n.\\\\RCAN3D\\\\Scripts\\\\activate\n\n# Activate (macOS/Linux)\nsource RCAN3D/bin/activate\nInstall dependencies:\npip install -r requirements.txt\n\nFor comprehensive installation instructions, please refer to the Dependencies Installation section in the 3D-RCAN README.\n\n\nA.2.4 Dataset\nCustom Data: 3D-RCAN supports 3D data with strict low-quality/high-quality paired training (e.g., noisy/blurred inputs vs. high-SNR/clean GT) . Performance depends on domain alignment between training data (intensity distribution, resolution, noise characteristics) and target applications. The data should follow the format of 3D TIFF stacks (Z-Y-X order).\nExample datasets available: You can leverage open-source datasets to obtain paired low-quality and high-quality data, such as the dataset from the 3D-RCAN paper. The 3D RCAN dataset is publicly available on Zenodo. It includes paired low-quality and ground truth (GT) data for diverse applications: denoising mulitple biological structures (actin, ER, Golgi, lysosomes, microtubules, and mitochondria), synthetic blurred phantom spheres with sharp GT, Confocal-to-STED microscopy modality transfer (microtubules, nuclear pores, DNA, and live-cell nuclei), and expansion microscopy (microtubules and mitochondria, pairing synthetic raw images with deconvolved iSIM images). Live-cell test data cover U2OS cells (mitochondria and lysosomes) and Jurkat T cells (EMTB-GFP). Raw data represent inputs (e.g., confocal images, noisy/blurred volumes), while GT corresponds to high-quality outputs (e.g., STED, deconvolved iSIM).\n\n\nA.2.5 Training\nTraining a 3D-RCAN model requires a config.json file to specify parameters and data locations.\nTo initiate training, use the command:\npython train.py -c config.json -o /path/to/training/output/dir\nTraining data can be specified in the config.json file by either:\n\nProviding paths to folders containing raw and ground truth images (training_data_dir).\nListing specific pairs of raw and ground truth image files (training_image_pairs).\n\nNumerous other options can be configured in the JSON file, including validation data, model architecture parameters (like num_residual_blocks, num_residual_groups), data augmentation settings, learning rate, loss functions, and metrics. The defaults for num_residual_blocks (3) and num_residual_groups (5) are set to balance performance and hardware constraints, aiming for optimal accuracy on standard GPUs (16-24GB VRAM) without causing memory overflow.\nThe expected runtime is approximately 5-10 minutes per epoch on a system similar to the tested environment (NVIDIA GeForce GTX 1080 Ti - 11GB) using the example config.json. Training progress and loss values can be monitored using TensorBoard.\nFor comprehensive details on configuring the config.json file, all available training options, and further instructions, please refer to the Training section in the 3D-RCAN README.\n\n\nA.2.6 Applying the Model\nTrained 3D-RCAN models can be applied using the apply.py script. The script will automatically select the model with the lowest validation loss from the specified model directory.\nThere are two primary ways to apply a model:\n\nTo a single image\npython apply.py -m /path/to/model_dir -i input_raw_image.tif -o output.tif\nTo a folder of images (batch mode)\npython apply.py -m /path/to/model_dir \n-i /path/to/input_image_dir \n-o /path/to/output_image_dir\nWhen processing a folder, ground truth images can also be specified using the -g argument for comparison.\n\nKey command-line arguments include:\n\n-m or --model_dir: Path to the folder containing the trained model (required).\n-i or --input: Path to the input raw image or folder (required).\n-o or --output: Path for the output processed image or folder (required).\n-g or --ground_truth: Path to a reference ground truth image or folder (optional).\n-b or --bpp: Bit depth of the output image (e.g., 8, 16, 32).\n-B or --block_shape: Dimensions (Z,Y,X) for processing large images in blocks.\n-O or --block_overlap_shape: Overlap size (Z,Y,X) between blocks.\n--normalize_output_range_between_zero_and_one: Normalizes output intensity to the [0, 1] range, or to the full bit depth range (e.g., [0, 65535] for 16-bit) when combined with -b.\n--rescale: Performs affine rescaling to minimize MSE between restored and ground truth images, useful for comparisons similar to CARE methodology.\n-f or --output_tiff_format: Sets the output TIFF format (e.g., “imagej” or “ome”).\n\nFor a complete list of all options and detailed explanations, please refer to the Model Apply section in the 3D-RCAN README.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>3D-RCAN for Image Restoration</span>"
    ]
  },
  {
    "objectID": "6-image-restoration-appendix.html#denoising-tutorial",
    "href": "6-image-restoration-appendix.html#denoising-tutorial",
    "title": "Appendix A — 3D-RCAN for Image Restoration",
    "section": "A.3 Denoising Tutorial",
    "text": "A.3 Denoising Tutorial\n\nA.3.1 Data Preparation\nInput Format: 3D TIFF stacks (Z-Y-X order)\nData Pairs: The example image pairs we employ are shown in Figure A.2 and are available in the denoising section of the data for the 3D-RCAN paper.\n\nLow-SNR Data: Noisy images. Here we use low signal-to-noise ratio data obtained by imaging mitochondria labeled with mEmerald-Tomm20-C-10 in living U2OS cells using instant structured illumination microscopy under low illumination intensities.\nHigh-SNR Ground Truth: Corresponding clean images. Here we use high signal-to-noise ratio and high-resolution data obtained by imaging mitochondria labeled with mEmerald-Tomm20-C-10 in living U2OS cells using instant structured illumination microscopy under high illumination intensities.\n\n\n\n\n\n\n\nFigure A.2: Dataset for denoising tasks. (left) Low signal-to-noise ratio (SNR) image and (right) high-SNR image of mitochondria in U2OS cells. Images were acquired using instant structured illumination microscopy under low and high illumination intensities, respectively. Scale bar: 5 μm.\n\n\n\nDirectory Structure: Organize the paired image data as follows.\ndataset/\n├── train/\n│   ├── raw/   # Training noisy data\n│   └── gt/    # Training clean data\n└── val/\n    ├── raw/   # Validation noisy data\n    └── gt/    # Validation clean data\nThe validation data is not necessary for model training, but is an important part of assessing the model quality after training.\n\n\n\nA.3.2 Training a Denoising Model\n\nA.3.2.1 Configuration File\nConfigure the settings file (config_denoise.json) to define the data locations for the training/validation sets and the initial network hyperparameters. An example configuration file is shown below.\n{\n  \"training_data_dir\": {\n      \"raw\": \"E:\\\\Denoising\\\\Tomm20_Mitochondria\\\\Training\\\\Raw\",\n      \"gt\": \"E:\\\\Denoising\\\\Tomm20_Mitochondria\\\\Training\\\\GT\"\n  },\n  \"validation_data_dir\": {\n      \"raw\": \"E:\\\\Denoising\\\\Tomm20_Mitochondria\\\\Val\\\\raw\",\n      \"gt\": \"E:\\\\Denoising\\\\Tomm20_Mitochondria\\\\Val\\\\gt\"\n  },\n  \"num_channels\": 32,\n  \"num_residual_blocks\": 3,\n  \"num_residual_groups\": 5,\n  \"epochs\": 100,\n  \"steps_per_epoch\": 256,\n  \"initial_learning_rate\": 1e-4\n}\n\n\nA.3.2.2 Training Command\nRun the training using the following command, updating the path to where you have stored the example images as appropriate.\npython train.py -c config_denoise.json \n-o \" E:\\\\Denoising\\\\Tomm20_Mitochondria\\\\output\"\n\n\n\n\n\n\nTip\n\n\n\nIf you are using a Mac or Linux system, replace the \\ in the example paths with /.\n\n\n\n\nA.3.2.3 Training Outputs\nThe output directory will save the model parameters during the training process. For example, the file weights_092_0.06313289.hdf5 represents the model parameters saved at the 92nd training epoch, with a loss value of 0.06313289.\n\n\n\nA.3.3 Applying the Denoising Model\n\nA.3.3.1 Apply Command\nTo apply the model trainined in the previous section, provide the model (-m), path for the input images (-i), and path for the output denoised images (-o).\npython apply.py \n  -m \"E:\\\\Denoising\\\\Tomm20_Mitochondria\\\\output\"\n  -i \"E:\\Denoising\\\\Tomm20_Mitochondria\\\\Test\\\\Raw\"\n  -o \"E:\\\\Denoising\\\\Tomm20_Mitochondria\\\\Test\\\\Denoised\"\n\n\nA.3.3.2 Output Analysis\nSince the output TIFF file is a two-channel ImageJ Hyperstack containing raw and restored images, we can easily compare the denoising effects.\n\n\n\n\n\n\nFigure A.3: Denoising Performance. The images show raw data input (left) and denoised image after using 3D-RCAN (right). Scale bars: 5 μm for main image and 1 μm for inset.\n\n\n\n3D-RCAN reconstructs continuous organelle membranes (e.g., mitochondrial cristae) in noisy volumetric data by suppressing stochastic noise, achieving sub-diffraction structural integrity without amplifying high-frequency artifacts or distorting fine textures.\nWhen evaluating the quality of denoised results generated by 3D-RCAN, PSNR (Peak Signal-to-Noise Ratio) and MSE (Mean Squared Error) serve as critical quantitative metrics. PSNR measures the logarithmic relationship between the maximum possible pixel intensity and the distortion introduced during restoration, with higher values indicating better alignment between the restored image and the ground truth (GT). Conversely, MSE calculates the average squared difference between corresponding pixels, where lower values reflect smaller pixel-level errors. For meaningful comparisons, the restored image should exhibit a higher PSNR and lower MSE relative to the raw input when both are evaluated against the GT. This directly quantifies the model’s ability to enhance resolution while minimizing deviations from the true signal.\nYou can calculate PSNR and MSE with the formulas below, where \\(m\\), \\(n\\), and \\(p\\) are the dimensions of the 3D image. \\(I_{\\text{raw}}(i,j,k)\\) is the intensity value of the raw image at position \\((i,j,k)\\) and \\(I_{\\text{GT}}(i,j,k)\\) is the intensity value of the ground truth image at position \\((i,j,k)\\).\n\\[\n\\text{MSE} = \\frac{1}{m n p} \\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1} \\sum_{k=0}^{p-1} [I_{\\text{raw}}(i,j,k)-I_{\\text{GT}}(i,j,k)]^2\n\\tag{A.1}\\]\n\\[\n\\text{PSNR} = 10 \\cdot \\log_{10}\\left(\\frac{\\text{MAX}_I^2}{\\text{MSE}}\\right)\n\\tag{A.2}\\]\nWe recommend intensity normalization across raw, restored, and GT images (e.g., scaling to \\([0,1]\\)) to avoid metric distortions. For heterogeneous samples, compute localized metrics within regions of interest (e.g., cell membranes vs. cytoplasm) to identify performance variations. Note that while high PSNR/MSE improvements generally correlate with perceptual quality, they should complement visual inspection and task-specific validations (e.g., downstream analysis accuracy). Always verify that the model’s output preserves biological relevance—metrics alone cannot capture context-specific artifacts or over-smoothing.\n\n\n\nA.3.4 Further Denoising Guidance\nTo improve the performance of the 3D-RCAN model when unsatisfied with denoising results, we recommend starting by tuning hyperparameters in the config.json file. Start by adjusting the model architecture parameters to balance computational constraints and feature extraction capabilities. For instance, increasing num_channels (default:32) enhances the network’s capacity to learn complex features but requires more GPU memory. If limited by hardware, reducing it to 16-24 while moderately increasing num_residual_groups (default:5) or num_residual_blocks per group (default:3) could maintain depth without overwhelming resources. The channel_reduction ratio (default:8) for attention mechanisms can be lowered to 4-6 to amplify the model’s sensitivity to channel-wise dependencies, though this should be validated using metrics to avoid overfitting.\nTraining dynamics can be optimized by revisiting the initial_learning_rate (default:1e-4). If training loss plateaus or fluctuates, gradually lowering it to 1e-5 stabilizes convergence, while aggressive learning rates (e.g., 5e-4) might accelerate early training but risk divergence. Switching the loss function from MAE (robust to noise) to MSE could prioritize pixel-level accuracy for high-frequency details, especially when ground truth data has sharp structures. Extending training epochs beyond the default 300—coupled with early stopping based on validation loss—helps the model capture subtle patterns in volumetric data.\nData-related parameters require careful calibration. Enabling data_augmentation (default:True) remains vital for small datasets to simulate variations in microscopy imaging, but it can be disabled for large, diverse datasets to speed up training. The intensity_threshold (default:0.25) and area_ratio_threshold (default:0.5) act as filters for low-quality patches. Raising the intensity threshold to 0.3-0.4 suppresses noisy backgrounds, while lowering the area ratio to 0.3-0.4 accommodates sparse biological structures like microtubules. For validation, always specify a dedicated validation_data_dir to monitor generalization and prevent overfitting.\nFinally, hardware limitations can be mitigated by reducing block_shape dimensions during inference or simplifying the model architecture. Experiment iteratively: establish a baseline with default settings, then progressively scale model complexity while tracking validation metrics like PSNR/SSIM. Consider hybrid strategies, such as combining MAE loss with SSIM regularization (a loss penalty using SSIM to preserve structural fidelity and prevent over-smoothing) or dynamic learning rate schedules, to adapt to specific data characteristics. Always validate changes against biologically relevant structures in your test set, as quantitative metrics alone may not reflect practical image quality for microscopy analysis.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>3D-RCAN for Image Restoration</span>"
    ]
  },
  {
    "objectID": "6-image-restoration-appendix.html#deconvolution-tutorial",
    "href": "6-image-restoration-appendix.html#deconvolution-tutorial",
    "title": "Appendix A — 3D-RCAN for Image Restoration",
    "section": "A.4 Deconvolution Tutorial",
    "text": "A.4 Deconvolution Tutorial\n\nA.4.1 Data Preparation\nInput Format: 3D TIFF stacks (Z-Y-X order)\nData Pairs: The example image pairs we employ here are shown in Figure A.4 and are available in the ‘DL Decon’ section of DeAbePlusData.\n\nInput (Raw): Low-quality, blurry 3D images with PSF such as single-view volumetric images acquired via diSPIM (0.8 NA \\(\\times\\) 0.8 NA optics), with voxel size 0.1625 \\(\\times\\) 0.1625 \\(\\times\\) 1.0 µm\nGround Truth (GT): High-quality, non-blurry 3D images such as dual-view joint deconvolution-reconstructed volumes (fusion of both diSPIM views), with isotropic resolution.\n\n\n\n\n\n\n\nFigure A.4: Dataset for Deconvolution. The images show the single-view image (network input, left) and dual-view image (right) of C. elegans embryos. Scale bar: 5 μm.\n\n\n\nDirectory Structure: Organize data as follows.\ndataset/\n├── train/\n│   ├── raw/   # Training raw data\n│   └── gt/    # Training label data\n└── val/\n    ├── raw/   # Validation raw data\n    └── gt/    # Validation label data\nThe validation data is not necessary for model training, but is an important part of assessing the model quality after training.\n\n\nA.4.2 Training a Deconvolution Model\n\nA.4.2.1 Configuration File\nConfigure the settings file (config_decon.json) to define the data locations for the training/validation sets and the initial network hyper-parameters.\n{\n  \"training_data_dir\": {\n      \"raw\": \"E:\\\\data_decon\\\\Decon_Training_Data\\\\Raw\",\n      \"gt\":\"E:\\\\data_decon\\\\Decon_Training_Data\\\\gt\"\n  },\n  \"validation_data_dir\": {\n      \"raw\":\"E:\\\\data_decon\\\\Decon_val_Data\\\\Raw\",\n      \"gt\": \"E:\\\\data_decon\\\\Decon_val_Data\\\\gt\"\n  },\n  \"num_channels\": 32,\n  \"num_residual_blocks\": 3,\n  \"num_residual_groups\": 5,\n  \"epochs\": 200,\n  \"steps_per_epoch\": 256,\n  \"initial_learning_rate\": 1e-4\n}\n\n\nA.4.2.2 Training Command\nRun the training using the following command, updating the path to where you have stored the example images as appropriate.\npython train.py -c config_decon.json -o  \"E:\\\\data_decon\\\\model_output\"\n\n\n\n\n\n\nTip\n\n\n\nIf you are using a Mac or Linux system, replace the \\ in the example paths with /.\n\n\n\n\nA.4.2.3 Training Outputs:\nThe output directory will save the model parameters during the training process. For example, the file weights_092_0.06313289.hdf5 represents the model parameters saved at the 92nd training epoch, with a loss value of 0.06313289.\n\n\n\nA.4.3 Applying the Deconvolution Model\n\nA.4.3.1 Apply Command\nTo apply the model trainined in the previous section, provide the model (-m), path for the input images (-i), and path for the output denoised images (-o).\npython apply.py \n  -m \"E:\\\\data_decon\\\\model_output\"\n  -i \"E:\\\\data_decon\\\\Decon_val_Data\\\\Raw\"\n  -o \"E:\\\\data_decon\\\\Decon_val_Data\\\\deconvolved\"\n\n\nA.4.3.2 Output Analysis\nSince the output TIFF file is a two-channel ImageJ Hyperstack containing raw and restored images, we can easily compare the deconvolved effects.\n\n\n\n\n\n\nFigure A.5: Network Prediction for Deconvolution. The images show the raw data from C. elegans embryos (left) and 3D-RCAN prediction (right), demonstrating deconvolution. Scale bar: 5 μm.\n\n\n\n3D-RCAN corrects out-of-focus blur in GFP-labeled C. elegans embryos, resolving densely packed microtubule arrays within mitotic spindles and restoring cortical membrane contours obscured by optical scattering in thick embryonic samples, without introducing grid-like artifacts typical of Fourier-based deconvolution.\nRefer to Section A.3.3.2 for baseline validation metrics. Compare with classical methods (e.g., Richardson5 -Lucy6 Deconvolution).\n\n\n\nA.4.4 Further Deconvolution Guidance\nRefer to Section A.3.4 for general guidance. For deconvolution, use a lightweight attention mechanism (\\(\\leq\\) 3 channel reduction steps) to avoid over-smoothing high-frequency structures.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>3D-RCAN for Image Restoration</span>"
    ]
  },
  {
    "objectID": "6-image-restoration-appendix.html#aberration-correction-tutorial",
    "href": "6-image-restoration-appendix.html#aberration-correction-tutorial",
    "title": "Appendix A — 3D-RCAN for Image Restoration",
    "section": "A.5 Aberration Correction Tutorial",
    "text": "A.5 Aberration Correction Tutorial\n\nA.5.1 Data Preparation\nInput Format: 3D TIFF stacks (Z-Y-X order)\nData Pairs: The sample image pairs employed here were C. elegans embryos expressing a GFP marker , as shown in Figure A.6. You can find these examples in the ‘DL DeAbe’ section of the DeAbePlusData.\n\nAberrated Input Data: Artificially degraded images, corrupted by synthetic aberrations modeled via Zernike polynomials (modes 1–7, defocus coefficient \\(\\leq\\) 1.5 rad, other modes \\(\\leq\\) 0.5 rad) and Poisson noise. Degradation is applied to shallow-layer ground truth using the physical forward model.\nHigh-Quality Label Data: Directly acquired shallow-layer images serve as ground truth, validated by their proximity to the objective lens (minimal inherent aberrations).\n\n\n\n\n\n\n\nFigure A.6: Dataset for Aberration Correction. The images show the aberrated data (network input, left) and shallow-layer data (right) of C. elegans embryos expressing a GFP marker. Scale bar: 5 μm.\n\n\n\nDirectory Structure: Organize data as follows.\ndataset/\n├── train/\n│   ├── raw/   # Training aberrated data\n│   └── gt/    # Training label data\n└── val/\n    ├── raw/   # Validation aberrated data\n    └── gt/    # Validation label data\nThe validation data is not necessary for model training, but is an important part of assessing the model quality after training.\n\n\nA.5.2 Training an Aberration Correction Model\n\nA.5.2.1 Configuration File\nConfigure the settings file (config_deabe.json) to define the data locations for the training/validation sets and the initial network hyperparameters.\n{\n  \"training_data_dir\": {\n      \"raw\": \"E:\\\\data_aberration\\\\CropForTraining_ab\\\\Aberrated\",\n      \"gt\":\"E:\\\\data_aberration\\\\CropForTraining_ab\\\\GT\"\n  },\n  \"validation_data_dir\": {\n      \"raw\":\"E:\\\\data_aberration\\\\CropForTraining_ab\\\\Validation\\\\Aberrated\",\n      \"gt\": \"E:\\\\data_aberration\\\\CropForTraining_ab\\\\Validation\\\\GT\"\n  },\n  \"num_channels\": 32,\n  \"num_residual_blocks\": 3,\n  \"num_residual_groups\": 5,\n  \"epochs\": 100,\n  \"steps_per_epoch\": 100,\n  \"initial_learning_rate\": 1e-4\n}\n\n\nA.5.2.2 Training Command\nRun the training using the following command, updating the path to where you have stored the example images as appropriate.\npython train.py -c config_deabe.json \n-o \" E:\\\\DeAbe\\\\Train\\\\output\"\n\n\n\n\n\n\nTip\n\n\n\nIf you are using a Mac or Linux system, replace the \\ in the example paths with /.\n\n\n\n\nA.5.2.3 Training Outputs:\nThe output directory will save the model parameters during the training process. For example, the file weights_092_0.06313289.hdf5 represents the model parameters saved at the 92nd training epoch, with a loss value of 0.06313289.\n\n\n\nA.5.3 Applying the Aberration Correction Model\n\nA.5.3.1 Apply Command\nTo apply the model trainined in the previous section, provide the model (-m), path for the input images (-i), and path for the output denoised images (-o).\npython apply.py \n  -m \"E:\\\\DeAbe\\\\Train\\\\output\"\n  -i \"E:\\DeAbe\\\\Test\\\\Raw\"\n  -o \"E:\\\\DeAbe\\\\Test\\\\Deabrrated\"\n\n\nA.5.3.2 Output Analysis\nSince the output TIFF file is a two-channel ImageJ Hyperstack containing raw and restored images, we can easily compare the de-aberration effects.\n\n\n\n\n\n\nFigure A.7: 3D-RCAN Deaberration prediction. The images show the raw data input to the network (left) and 3D-RCAN prediction (right), demonstrating aberration correction. Scale bar: 5 μm.\n\n\n\n3D-RCAN effectively corrects system-induced optical aberrations in GFP-labeled C. elegans embryos. The de-aberrated image restores spatially ordered structures obscured by asymmetric blur in the raw data, while suppressing background granularity and enhancing signal-to-background contrast.\nRefer to Section A.3.3.2 for baseline validation metrics. For aberration correction validation, analyze Fourier spectra to assess restored spatial symmetry and high-frequency energy, quantify wavefront errors via phase retrieval or bead-based PSF measurements, and evaluate intensity uniformity/edge sharpness across volumes.\n\n\n\nA.5.4 Further Aberration Correction Guidance\nRefer to Section A.3.4 for basic guidance. Prioritize RCAB layers with wider receptive fields (kernel size \\(\\leq 5^3\\)) to capture large-scale distortion patterns.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>3D-RCAN for Image Restoration</span>"
    ]
  },
  {
    "objectID": "6-image-restoration-appendix.html#resolution-enhancement-tutorial",
    "href": "6-image-restoration-appendix.html#resolution-enhancement-tutorial",
    "title": "Appendix A — 3D-RCAN for Image Restoration",
    "section": "A.6 Resolution Enhancement Tutorial",
    "text": "A.6 Resolution Enhancement Tutorial\n\nA.6.1 Data Preparation\nInput Format: 3D TIFF stacks (Z-Y-X order)\nData Pairs: The example image pairs we employ are shown in Figure A.8. You can find these examples in the ‘confocal_2_STED’ section of the data for the 3D-RCAN paper.\n\nRaw data: The “raw” data here refers to the original confocal microscopy images of the fixed mouse embryonic fibroblast (MEF) cells, specifically focusing on the microtubules. These images are acquired directly from the confocal microscope without any further processing or enhancement. They represent the initial, unaltered state of the microtubule structures as captured by the confocal microscope.\nGround Truth (GT) data: The “ground truth” (GT) data consists of the high-resolution STED (Stimulated Emission Depletion) microscopy images of the same fixed MEF cells, again focusing on the microtubules.\n\n\n\n\n\n\n\nFigure A.8: Dataset for resolution enhancement. Image captured by confocal microscope (network input, left) and image captured by STED microscopy (ground truth, right). Both images show microtubules in mouse embryonic fibroblast cells. Scale bar: 5 μm.\n\n\n\nDirectory Structure: Organize data as follows.\ndataset/\n├── train/\n│   ├── raw/   #confocal data\n│   └── gt/    #STED data\n└── val/\n    ├── raw/   # confocal data\n    └── gt/    # STED data\nThe validation data is not necessary for model training, but is an important part of assessing the model quality after training.\n\n\nA.6.2 Training a Resolution Enhancement Model\n\nA.6.2.1 Configuration File\nConfigure the settings file (config_sr.json) to define the data locations for the training/validation sets and the initial network hyper-parameters.\n{\n  \"training_data_dir\": {\n      \"raw\": \"E:\\\\Confocal_2_STED\\\\Microtubule\\\\Training\\\\raw\",\n      \"gt\": \"E:\\\\Confocal_2_STED\\\\Microtubule\\\\Training\\\\gt1\"\n      \n  },\n  \n  \"num_channels\": 32,\n  \"num_residual_blocks\": 3,\n  \"num_residual_groups\": 5,\n  \"epochs\": 100,\n  \"steps_per_epoch\": 256,\n  \"initial_learning_rate\": 1e-4\n}\n\n\nA.6.2.2 Training Command\nRun the training using the following command, updating the path to where you have stored the example images as appropriate.\npython train.py \n-c config_ex.json \n-o \"E:\\\\Confocal_2_STED\\\\Microtubule\\\\Training\\\\output\"\n\n\n\n\n\n\nTip\n\n\n\nIf you are using a Mac or Linux system, replace the \\ in the example paths with /.\n\n\n\n\n\n\nA.6.3 Applying the Resolution Enhancement Model\n\nA.6.3.1 Apply Command\nTo apply the model trainined in the previous section, provide the model (-m), path for the input images (-i), and path for the output denoised images (-o).\n python apply.py\n -m \"E:\\\\Confocal_2_STED\\\\Microtubule\\\\Training\\\\output\" \n -i \"E:\\DATA_for_3dRCAN\\Confocal_2_STED\\Microtubule\\test\\\" \n -o \"E:\\\\Confocal_2_STED\\\\Microtubule\\\\test\\output\\\\\"\n\n\nA.6.3.2 Output Analysis\nSince the output TIFF file is a two-channel ImageJ Hyperstack containing raw and restored images, we can easily investigate the extent of resolution enhancement.\n\n\n\n\n\n\nFigure A.9: Network Prediction for Enhanced Resolution. The images show the raw data of immunolabeled microtubules from mouse embryonic fibroblast cells (left) and 3D-RCAN predictions (right), demonstrating enhanced resolution. Scale bars: 5 μm for main figure and 1 μm for inset.\n\n\n\nThe model resolves densely packed microtubule intersections and eliminates axial blur in confocal-derived volumes, restoring filament topology to near-STED resolution.\nRefer to Section A.3.3.2 for baseline validation metrics. Check for artifacts in Z-stack transitions and consistency in organelle morphology.\n\n\n\nA.6.4 Further Resolution Enhancement Guidance\nRefer to Section A.3.4 for basic guidance. Optimize residual groups (e.g., increase from 5 to 10) and channel dimensions (\\(\\leq 64\\)) to handle spatially variant upsampling.\n\n\n\n\n1. Zhang, Y. et al. Image super-resolution using very deep residual channel attention networks. (2018).\n\n\n2. Chen, J. et al. Three-dimensional residual channel attention networks denoise and sharpen fluorescence microscopy image volumes. Nature Methods 18, 678–687 (2021).\n\n\n3. Weigert, M. et al. Content-aware image restoration: Pushing the limits of fluorescence microscopy. Nature Methods 15, 1090–1097 (2018).\n\n\n4. Wang, Z., Bovik, A. C., Sheikh, H. R. & Simoncelli, E. P. Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing 13, 600–612 (2004).\n\n\n5. Richardson, W. H. Bayesian-based iterative method of image restoration\\(\\ast\\). J. Opt. Soc. Am. 62, 55–59 (1972).\n\n\n6. Lucy, L. B. An iterative technique for the rectification of observed distributions. Astronomical Journal 79, 745 (1974).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>3D-RCAN for Image Restoration</span>"
    ]
  }
]