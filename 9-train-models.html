<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Joanna W. Pylvänäinen">
<meta name="author" content="Iván Hidalgo-Cenalmor">
<meta name="author" content="Guillaume Jacquemet">

<title>9&nbsp; How to Train and Use Deep Learning Models in Microscopy – AI in Microscopy: A BioImaging Guide</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./10-output-quality.html" rel="next">
<link href="./8-existing-tools.html" rel="prev">
<link href="./settings/favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b651517ce65839d647a86e2780455cfb.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-2641b481724464e61c86985d8c912b6f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-4c6d65c679321d81af7ec8b61b1d5a24.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-2641b481724464e61c86985d8c912b6f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-Y29EKZ8LWD"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y29EKZ8LWD', { 'anonymize_ip': true});
</script>


</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./8-existing-tools.html">Image Analysis</a></li><li class="breadcrumb-item"><a href="./9-train-models.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">How to Train and Use Deep Learning Models in Microscopy</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI in Microscopy: A BioImaging Guide</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/aicjanelia/BioImagingAI" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./AI-in-Microscopy--A-BioImaging-Guide.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Getting Started with AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">AI Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-llms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Foundations of Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Architectures and Loss Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Image Acquisition</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-training-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Collecting Training Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-image-restoration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Extending Your Hardware With AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-smart-microscopy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Adding AI to Your Hardware</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Image Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-existing-tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Finding and Using Existing Tools</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-train-models.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">How to Train and Use Deep Learning Models in Microscopy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-output-quality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Output Quality</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-outlook.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Outlook</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Glossary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-9.1" id="toc-sec-9.1" class="nav-link active" data-scroll-target="#sec-9.1"><span class="header-section-number">9.1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#sec-9.1.1" id="toc-sec-9.1.1" class="nav-link" data-scroll-target="#sec-9.1.1"><span class="header-section-number">9.1.1</span> The challenge</a></li>
  </ul></li>
  <li><a href="#sec-9.2" id="toc-sec-9.2" class="nav-link" data-scroll-target="#sec-9.2"><span class="header-section-number">9.2</span> Preparing for your project</a>
  <ul class="collapse">
  <li><a href="#sec-9.2.1" id="toc-sec-9.2.1" class="nav-link" data-scroll-target="#sec-9.2.1"><span class="header-section-number">9.2.1</span> Defining your task and success criteria</a></li>
  <li><a href="#sec-9.2.2" id="toc-sec-9.2.2" class="nav-link" data-scroll-target="#sec-9.2.2"><span class="header-section-number">9.2.2</span> Evaluating alternatives: Is DL the right choice?</a></li>
  </ul></li>
  <li><a href="#sec-9.3" id="toc-sec-9.3" class="nav-link" data-scroll-target="#sec-9.3"><span class="header-section-number">9.3</span> Implementing a DL segmentation workflow</a>
  <ul class="collapse">
  <li><a href="#sec-9.3.1" id="toc-sec-9.3.1" class="nav-link" data-scroll-target="#sec-9.3.1"><span class="header-section-number">9.3.1</span> Overview of a typical DL segmentation workflow</a></li>
  <li><a href="#sec-9.3.2" id="toc-sec-9.3.2" class="nav-link" data-scroll-target="#sec-9.3.2"><span class="header-section-number">9.3.2</span> Selecting a suitable DL approach</a></li>
  <li><a href="#sec-9.3.3" id="toc-sec-9.3.3" class="nav-link" data-scroll-target="#sec-9.3.3"><span class="header-section-number">9.3.3</span> Deciding whether to train a new model</a></li>
  <li><a href="#sec-9.3.4" id="toc-sec-9.3.4" class="nav-link" data-scroll-target="#sec-9.3.4"><span class="header-section-number">9.3.4</span> Preparing your dataset for training</a></li>
  <li><a href="#sec-9.3.5" id="toc-sec-9.3.5" class="nav-link" data-scroll-target="#sec-9.3.5"><span class="header-section-number">9.3.5</span> Training a segmentation model from scratch</a></li>
  <li><a href="#sec-9.3.6" id="toc-sec-9.3.6" class="nav-link" data-scroll-target="#sec-9.3.6"><span class="header-section-number">9.3.6</span> Fine-Tuning Pre-existing Models</a></li>
  <li><a href="#sec-9.3.7" id="toc-sec-9.3.7" class="nav-link" data-scroll-target="#sec-9.3.7"><span class="header-section-number">9.3.7</span> Evaluating the performance of your model</a></li>
  <li><a href="#sec-9.3.8" id="toc-sec-9.3.8" class="nav-link" data-scroll-target="#sec-9.3.8"><span class="header-section-number">9.3.8</span> Deploying your model on new data</a></li>
  <li><a href="#sec-9.3.9" id="toc-sec-9.3.9" class="nav-link" data-scroll-target="#sec-9.3.9"><span class="header-section-number">9.3.9</span> Troubleshooting Common Problems</a></li>
  </ul></li>
  <li><a href="#sec-9.4" id="toc-sec-9.4" class="nav-link" data-scroll-target="#sec-9.4"><span class="header-section-number">9.4</span> Further considerations for DL segmentation</a>
  <ul class="collapse">
  <li><a href="#sec-9.4.1" id="toc-sec-9.4.1" class="nav-link" data-scroll-target="#sec-9.4.1"><span class="header-section-number">9.4.1</span> Choosing the Right Tools for DL</a></li>
  <li><a href="#sec-9.4.2" id="toc-sec-9.4.2" class="nav-link" data-scroll-target="#sec-9.4.2"><span class="header-section-number">9.4.2</span> Managing Computational Resources</a></li>
  <li><a href="#sec-9.4.3" id="toc-sec-9.4.3" class="nav-link" data-scroll-target="#sec-9.4.3"><span class="header-section-number">9.4.3</span> Ensuring Reproducibility in DL</a></li>
  </ul></li>
  <li><a href="#sec-9.5" id="toc-sec-9.5" class="nav-link" data-scroll-target="#sec-9.5"><span class="header-section-number">9.5</span> Summary &amp; Outlook</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./8-existing-tools.html">Image Analysis</a></li><li class="breadcrumb-item"><a href="./9-train-models.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">How to Train and Use Deep Learning Models in Microscopy</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-9" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">How to Train and Use Deep Learning Models in Microscopy</span></span></h1>
<p class="subtitle lead">Practical considerations through the lens of segmentation</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Joanna W. Pylvänäinen <a href="https://orcid.org/0000-0002-3540-5150" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Turku Bioscience Centre, University of Turku and Åbo Akademi University
          </p>
        <p class="affiliation">
            Faculty of Science and Engineering, Cell Biology, Åbo Akademi University
          </p>
        <p class="affiliation">
            InFLAMES Research Flagship Center, University of Turku
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Iván Hidalgo-Cenalmor <a href="https://orcid.org/0009-0000-8923-568X" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Turku Bioscience Centre, University of Turku and Åbo Akademi University
          </p>
        <p class="affiliation">
            Faculty of Science and Engineering, Cell Biology, Åbo Akademi University
          </p>
        <p class="affiliation">
            InFLAMES Research Flagship Center, University of Turku
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Guillaume Jacquemet <a href="https://orcid.org/0000-0002-9286-920X" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Turku Bioscience Centre, University of Turku and Åbo Akademi University
          </p>
        <p class="affiliation">
            Faculty of Science and Engineering, Cell Biology, Åbo Akademi University
          </p>
        <p class="affiliation">
            InFLAMES Research Flagship Center, University of Turku
          </p>
        <p class="affiliation">
            Turku Bioimaging, University of Turku and Åbo Akademi University
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<!--Your first header will be the chapter's upper-level table of contents title.-->
<!--If you'd like to have a subtitle, include it in the Quarto header above -->
<section id="sec-9.1" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="sec-9.1"><span class="header-section-number">9.1</span> Introduction</h2>
<section id="sec-9.1.1" class="level3" data-number="9.1.1">
<h3 data-number="9.1.1" class="anchored" data-anchor-id="sec-9.1.1"><span class="header-section-number">9.1.1</span> The challenge</h3>
<p>The growing volume and complexity of image data necessitate increasingly advanced analytical tools. One example of challenging tasks is image segmentation, the process of identifying and delineating structures of interest within images. Segmentation can be particularly difficult and time-consuming when dealing with large, multidimensional datasets, such as 3D volumes or time-lapse sequences, where <a href="./glossary.html#manual-annotation">manual annotation</a> becomes impractical. Machine learning (ML), especially deep learning (DL), can provide effective solutions to these challenges<span class="citation" data-cites="heinrich2021"><sup><a href="references.html#ref-heinrich2021" role="doc-biblioref">1</a></sup></span>.</p>
<p>ML algorithms learn patterns from data to perform tasks such as <a href="./glossary.html#image-classification">image classification</a> and segmentation. Traditional ML methods, like random forest classifiers, depend on manually defined image features to classify pixels<span class="citation" data-cites="arganda-carreras2017 arzt2022 berg2019"><sup><a href="references.html#ref-arganda-carreras2017" role="doc-biblioref">2</a>–<a href="references.html#ref-berg2019" role="doc-biblioref">4</a></sup></span>. In contrast, DL algorithms can automatically discover and extract relevant features directly from image data using multilayer neural networks, which eliminates the need for manual feature selection. DL techniques are widely applied in complex image analysis tasks, including segmentation, <a href="./glossary.html#object-detection">object detection</a>, feature extraction, denoising, and restoration<span class="citation" data-cites="moen2019 pylvanainen2023"><sup><a href="references.html#ref-moen2019" role="doc-biblioref">5</a>,<a href="references.html#ref-pylvanainen2023" role="doc-biblioref">6</a></sup></span>. Due to their ability to automatically learn hierarchical features, DL methods usually achieve greater accuracy and efficiency than traditional ML techniques<span class="citation" data-cites="krizhevsky2012 ronneberger2015"><sup><a href="references.html#ref-krizhevsky2012" role="doc-biblioref">7</a>,<a href="references.html#ref-ronneberger2015" role="doc-biblioref">8</a></sup></span>.</p>
<p>Segmentation greatly benefits from ML and DL, as manual segmentation is extremely time-consuming and impractical for large datasets. This chapter offers practical guidance on preparing a segmentation project and emphasises effective DL applications to tackle these challenges. While we focus on segmentation as a case study, the principles, workflows, and considerations discussed here are broadly applicable to other image analysis tasks, such as classification or denoising. Readers interested in these areas can adapt the described approaches to their specific needs.</p>
</section>
</section>
<section id="sec-9.2" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="sec-9.2"><span class="header-section-number">9.2</span> Preparing for your project</h2>
<section id="sec-9.2.1" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="sec-9.2.1"><span class="header-section-number">9.2.1</span> Defining your task and success criteria</h3>
<p>Every image analysis project should begin by clearly defining the scientific question you wish to answer, along with the criteria by which success will be measured. These foundational decisions will fundamentally shape your entire workflow. Careful planning of your objectives ensures that the chosen approach closely aligns with your scientific goals and will guide critical decisions about data annotation, model selection and performance evaluation.</p>
<p>Since segmentation serves as the central example in this chapter, it is important to first understand the different types of segmentation tasks encountered in microscopy before discussing how deep learning methods can be applied. These tasks can typically be categorised into three main types (<a href="#fig-1" class="quarto-xref">Figure&nbsp;<span>9.1</span></a>). Each segmentation type presents distinct challenges and is suited to different biological questions:</p>
<p><strong>Binary segmentation:</strong> This is the <a href="./glossary.html#binary-segmentation">simplest form of segmentation</a> that separates the foreground from the background. For example, in a microscopy image, this involves distinguishing cell nuclei (foreground) from the rest of the image (background). This method is useful for detecting whether a structure is present or absent without distinguishing individual objects.</p>
<p><strong>Instance segmentation:</strong> This <a href="./glossary.html#instance-segmentation">type of segmentation</a> identifies and labels each object independently. For instance, each cell in an image obtains a unique label. This method is crucial for tracking individual cells over time or measuring specific characteristics of each cell separately.</p>
<p><strong>Semantic segmentation:</strong> This <a href="./glossary.html#semantic-segmentation">segmentation strategy</a> involves labelling every pixel in an image according to its class, such as “nucleus,” “cytoplasm,” or “background.” Unlike instance segmentation, semantic segmentation does not differentiate between individual objects within the same class. This method is beneficial for analysing the spatial relationships and distribution of various cellular components.</p>
<div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="9-figures/chapter9_1_segmentation_types.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1: The three main types of segmentation in microscopy images. <strong>original</strong>: A raw grayscale fluorescence microscopy image showing cell nuclei stained with a nuclear marker. <strong>binary segmentation</strong>: Simplifies the image into two classes—foreground (white, nuclei) and background (black). <strong>instance segmentation</strong>: Assigns a unique label (shown in different colours) to each nucleus, facilitating individual object identification. <strong>semantic segmentation</strong>: Categorises each pixel into predefined classes—nucleus (purple), nucleus edge (yellow), and background (teal)—without distinguishing between individual objects.
</figcaption>
</figure>
</div>
<p>Consider whether your segmentation solution is meant for a specific experiment or needs to generalise across various imaging techniques, sample types, or experimental conditions. Additionally, evaluate the volume of data to analyse, the feasibility of manual analysis, and the resources available to create a tailored image analysis pipeline. Avoid overengineering a solution when a simple analysis could provide the answer you seek.</p>
<p>Alongside task-specific considerations, it is equally important to clearly define the success criteria based on your objectives. For example, be prepared to answer the question, <em>“What do I need to accomplish for my analysis to be sufficient?”</em> – see <a href="10-output-quality.html" class="quarto-xref"><span>Chapter 10</span></a> for more information. This is important because no analysis is ever 100% accurate. Establishing these criteria early streamlines both the development and evaluation processes, ensuring that your outcomes are scientifically meaningful and practically useful (see <a href="10-output-quality.html" class="quarto-xref"><span>Chapter 10</span></a>).</p>
<p>While the following steps focus on segmentation, the underlying principles can be readily adapted to a wide range of DL tasks in microscopy.</p>
</section>
<section id="sec-9.2.2" class="level3" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored" data-anchor-id="sec-9.2.2"><span class="header-section-number">9.2.2</span> Evaluating alternatives: Is DL the right choice?</h3>
<div id="fig-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="9-figures/chapter9_2_yes or no DL.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.2: Is DL the right choice for your segmentation project? This decision tree guides the selection of appropriate segmentation approaches based on data complexity and project needs. Begin by testing classical image processing methods, such as intensity-based thresholding, which are efficient and easy to apply for well-defined features. If these methods prove insufficient, consider using a pixel classifier, which provides a user-friendly and effective solution for smaller datasets. Only consider DL if you possess a large annotated dataset and previous methods have failed. In the absence of suitable data or methods, manual annotation may be necessary.
</figcaption>
</figure>
</div>
<p>Choosing the right computational method is essential for consistent and reproducible image analysis. For example in segmentation tasks, while DL can deliver exceptional segmentation performance, traditional methods and pixel classifiers still offer straightforward and efficient solutions for most tasks (<a href="#fig-2" class="quarto-xref">Figure&nbsp;<span>9.2</span></a>).</p>
<p>Traditional image processing techniques—such as intensity-based thresholding, morphological operations, edge detection, and filtering—are ideal for objects with clear, distinguishable features. These methods are well-documented, easy to understand, and usually require minimal computing resources. <a href="./glossary.html#pixel-classifiers">Pixel classifiers</a>, in particular, are user-friendly and can efficiently tackle many segmentation challenges with minimal manual annotation, making them highly effective for simpler analyses or smaller datasets.</p>
<p>DL methods excel in complex scenarios where traditional approaches fail, especially when dealing with noisy or context-dependent data. When trained on large, annotated datasets, DL models can effectively generalise across diverse imaging conditions and sample types, rapidly processing significant volumes of images. However, in the absence of pre-trained models, DL methods rarely offer shortcuts for data analysis. DL methods generally take effort and time to implement.</p>
<p>If you are unsure which approach to use, we usually recommend first trying classical image processing methods and pixel classifiers (<a href="#fig-1" class="quarto-xref">Figure&nbsp;<span>9.1</span></a>). We typically initiate a DL project only if these methods fail to produce satisfactory results (see <a href="#sec-9.3.3.2" class="quarto-xref"><span>Section 9.3.3.2</span></a>).</p>
</section>
</section>
<section id="sec-9.3" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="sec-9.3"><span class="header-section-number">9.3</span> Implementing a DL segmentation workflow</h2>
<p>Although we use segmentation as our primary example, the workflow outlined in this section can be adapted to other deep learning tasks in microscopy and bioimage analysis.</p>
<section id="sec-9.3.1" class="level3" data-number="9.3.1">
<h3 data-number="9.3.1" class="anchored" data-anchor-id="sec-9.3.1"><span class="header-section-number">9.3.1</span> Overview of a typical DL segmentation workflow</h3>
<p>Once you decide to implement a DL approach for segmentation, the workflow can be divided into a series of steps (<a href="#fig-3" class="quarto-xref">Figure&nbsp;<span>9.3</span></a>).</p>
<p>The process starts by clearly defining your task (in this example, segmentation) and selecting the right DL approach (Step 1). Next, evaluate whether any existing pre-trained models can be used directly on your data or adapted for use (Step 2). If additional training is required—either from scratch or through transfer learning—prepare an appropriate training dataset that reflects your segmentation problem (Step 3). Then, train your model using the prepared dataset (Step 4) and thoroughly evaluate its performance using validation or test data (Step 5). Based on the results, you may need to refine the model by adjusting hyperparameters, improving annotations, or expanding the dataset. Once the model performs satisfactorily, it can be used to segment new, unseen data (Step 6). We will now discuss each step in more detail.</p>
<div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="9-figures/chapter9_3_steps for training.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.3: Conceptual workflow for training a DL segmentation model. The workflow begins with defining the segmentation task (Step 1), followed by searching for suitable pre-trained models (Step 2). If no such model exists, a training dataset must be prepared (Step 3), and the model is trained (Step 4). The trained model is then evaluated on validation or test data (Step 5). If it performs well, it can be applied to new data (Step 6); otherwise, the model is iteratively refined by returning to earlier steps. Feedback loops from evaluation to earlier stages help refine and improve the model accuracy.
</figcaption>
</figure>
</div>
</section>
<section id="sec-9.3.2" class="level3" data-number="9.3.2">
<h3 data-number="9.3.2" class="anchored" data-anchor-id="sec-9.3.2"><span class="header-section-number">9.3.2</span> Selecting a suitable DL approach</h3>
<p>The first step in choosing a DL approach for image segmentation is to clearly define your segmentation task, whether it’s binary, semantic, or instance segmentation (<a href="#fig-1" class="quarto-xref">Figure&nbsp;<span>9.1</span></a>), and to determine if you require 2D or 3D segmentation. Next, you should consider whether the model or tool you plan to use makes assumptions about the shapes or structures of the objects you want to segment. Understanding these assumptions will aid in selecting a model that fits your specific biological problem (see <a href="#sec-9.2.1" class="quarto-xref"><span>Section 9.2.1</span></a>). Additionally, consider the amount of data that needs annotation for a particular DL approach (see <a href="#sec-9.3.4.5" class="quarto-xref"><span>Section 9.3.4.5</span></a>). Finally, take into account your available computational resources (see <a href="#sec-9.4.2" class="quarto-xref"><span>Section 9.4.2</span></a>). More complex models typically demand more GPU memory, longer training times, and additional storage, especially for 3D data or large datasets.</p>
<p>For example, StarDist<span class="citation" data-cites="schmidt2018"><sup><a href="references.html#ref-schmidt2018" role="doc-biblioref">9</a></sup></span>, a widely used tool for nuclei segmentation, assumes that objects are <a href="./glossary.html#star-convex-polygon">star-convex polygons</a>: i.e., a shape for which any two points on its boundary can be connected by a single line that does not intersect the boundary. This assumption works well for round or oval shapes but makes StarDist less suitable for segmenting irregularly shaped or elongated structures. In contrast, Cellpose<span class="citation" data-cites="stringer2021"><sup><a href="references.html#ref-stringer2021" role="doc-biblioref">10</a></sup></span> uses spatial vector flows to direct pixels toward object centres. This approach enables Cellpose to segment objects of various shapes and sizes, including irregular, elongated, or non-convex forms.</p>
<p>Choosing the right DL strategy requires aligning your goal, object shape, data dimensionality, and computing capacity with the strengths and assumptions of the available DL architectures.</p>
</section>
<section id="sec-9.3.3" class="level3" data-number="9.3.3">
<h3 data-number="9.3.3" class="anchored" data-anchor-id="sec-9.3.3"><span class="header-section-number">9.3.3</span> Deciding whether to train a new model</h3>
<div id="fig-4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="9-figures/chapter9_4_training approach.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.4: Decision workflow for selecting a DL model training approach. This flowchart outlines how to determine an appropriate training approach based on the availability and performance of pre-trained models.
</figcaption>
</figure>
</div>
<section id="sec-9.3.3.1" class="level4" data-number="9.3.3.1">
<h4 data-number="9.3.3.1" class="anchored" data-anchor-id="sec-9.3.3.1"><span class="header-section-number">9.3.3.1</span> Leveraging pre-trained models</h4>
<p>The increasing availability of already trained (pre-trained) DL models has greatly simplified image analysis. Many of these models can be directly applied to your data, removing the need to train your own model<span class="citation" data-cites="bejarano2024 fisch2024"><sup><a href="references.html#ref-bejarano2024" role="doc-biblioref">11</a>,<a href="references.html#ref-fisch2024" role="doc-biblioref">12</a></sup></span>. This reduces the technical barrier and saves time, making advanced analysis more accessible. However, it is essential to evaluate the quality of any pre-trained model before relying on its results (<a href="#fig-4" class="quarto-xref">Figure&nbsp;<span>9.4</span></a>). A model that performs well in one context may not be as effective on your specific data. Always conduct quality control by visually inspecting the outputs and assessing performance with quantitative metrics such as Intersection over Union (IoU) or F1-score, using a small, representative test set. This step is vital when model predictions are used in downstream analyses (see <a href="#sec-9.3.7" class="quarto-xref"><span>Section 9.3.7</span></a>).</p>
<p>Another significant benefit of pre-trained models is their adaptability. Instead of starting from scratch, you can often fine-tune an existing model (see <a href="#sec-9.3.6" class="quarto-xref"><span>Section 9.3.6</span></a>). This method entails retraining the model with a smaller, task-specific dataset, enabling it to adjust to your images while requiring far fewer annotations.</p>
<p>Several excellent resources host pre-trained models suitable for microscopy. Researchers also increasingly share trained models alongside their datasets and publications, promoting open science. Platforms like Zenodo are commonly used for this purpose<span class="citation" data-cites="fazeli2020 follain2024"><sup><a href="references.html#ref-fazeli2020" role="doc-biblioref">13</a>,<a href="references.html#ref-follain2024" role="doc-biblioref">14</a></sup></span>, although deployment may require handling specific file formats or environments (see <a href="8-existing-tools.html" class="quarto-xref"><span>Chapter 8</span></a> for more information).</p>
</section>
<section id="sec-9.3.3.2" class="level4" data-number="9.3.3.2">
<h4 data-number="9.3.3.2" class="anchored" data-anchor-id="sec-9.3.3.2"><span class="header-section-number">9.3.3.2</span> When to train your model</h4>
<p>Pre-trained models serve as an excellent starting point for various microscopy tasks. However, there are many scenarios where training a custom model becomes essential. Custom training enables the model to learn the specific characteristics of your dataset, experiment, or imaging modality, resulting in enhanced performance<span class="citation" data-cites="archit2025 pachitariu2022 von_chamier2021"><sup><a href="references.html#ref-archit2025" role="doc-biblioref">15</a>–<a href="references.html#ref-von_chamier2021" role="doc-biblioref">17</a></sup></span>. This is particularly crucial when your data differs significantly from the data used to train existing models. Thus, their performance should always be validated. If quality assessment metrics are poor or key features are not accurately segmented, consider training your own model.</p>
<p>Ultimately, always evaluate the model’s performance against your defined success criteria (see <a href="#sec-9.3.7" class="quarto-xref"><span>Section 9.3.7</span></a>). Custom training may be the best path forward if the current model does not meet your needs.</p>
</section>
</section>
<section id="sec-9.3.4" class="level3" data-number="9.3.4">
<h3 data-number="9.3.4" class="anchored" data-anchor-id="sec-9.3.4"><span class="header-section-number">9.3.4</span> Preparing your dataset for training</h3>
<p>A well-designed training dataset is essential for developing successful DL models on tasks such as segmentation. The number of images and the quality of annotations needed vary based on factors such as task complexity and the architecture of the intended model.</p>
<section id="sec-9.3.4.1" class="level4" data-number="9.3.4.1">
<h4 data-number="9.3.4.1" class="anchored" data-anchor-id="sec-9.3.4.1"><span class="header-section-number">9.3.4.1</span> Types of model training</h4>
<p>For segmentation, most DL models are trained using supervised learning, where each input image is paired with a manually annotated ground truth mask. In this context, all objects that need segmentation must be annotated in the training dataset. This approach enables the model to learn a direct mapping from raw images to segmentation outputs (<a href="#fig-7" class="quarto-xref">Figure&nbsp;<span>9.7</span></a>).</p>
<p>However, alternative approaches can help reduce the need for extensive manual annotations:</p>
<ul>
<li><strong>Unsupervised learning</strong> trains models without paired input and output data. Instead, the network identifies patterns or similarities in unlabelled images<span class="citation" data-cites="kochetov2024"><sup><a href="references.html#ref-kochetov2024" role="doc-biblioref">18</a></sup></span>.<br>
</li>
<li><strong>Self-supervised learning</strong> involves designing tasks in which the model learns useful features directly from the input data without needing explicit labels<span class="citation" data-cites="liu2025"><sup><a href="references.html#ref-liu2025" role="doc-biblioref">19</a></sup></span>.<br>
</li>
<li><strong>Weakly supervised learning</strong> uses partial, noisy, or imprecise labels to guide training, which can significantly reduce annotation effort<span class="citation" data-cites="caicedo2018 moshkov2024"><sup><a href="references.html#ref-caicedo2018" role="doc-biblioref">20</a>,<a href="references.html#ref-moshkov2024" role="doc-biblioref">21</a></sup></span>.</li>
</ul>
</section>
<section id="sec-9.3.4.2" class="level4" data-number="9.3.4.2">
<h4 data-number="9.3.4.2" class="anchored" data-anchor-id="sec-9.3.4.2"><span class="header-section-number">9.3.4.2</span> Creating Manual Annotations</h4>
<p>Creating accurate annotations manually is time-consuming, particularly for 3D datasets. Tools like Fiji<span class="citation" data-cites="schindelin2012"><sup><a href="references.html#ref-schindelin2012" role="doc-biblioref">22</a></sup></span>, Napari<span class="citation" data-cites="ahlers2023"><sup><a href="references.html#ref-ahlers2023" role="doc-biblioref">23</a></sup></span>, and QuPath<span class="citation" data-cites="bankhead2017"><sup><a href="references.html#ref-bankhead2017" role="doc-biblioref">24</a></sup></span> are frequently employed for manual labelling. Typically, manual annotation involves drawing each object on the image and converting it into a mask or label.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span><strong>Here it is an example pipeline for manually annotating data using Fiji</strong><span class="citation" data-cites="fazeli2020"><sup><a href="references.html#ref-fazeli2020" role="doc-biblioref">13</a></sup></span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><strong>Open Fiji – activate the LOCI update site and restart Fiji.</strong><br>
LOCI tools are required for exporting ROI maps. To enable them, go to <em>Help &gt; Update &gt; Manage Update Sites</em>, look for ‘LOCI’ and check the <em>Active</em> checkbox. Then, click on <em>Apply and Close</em> and <em>Apply Changes</em>, this update site ensures the necessary plugins are installed. Finally, restart Fiji.</p></li>
<li><p><strong>Open your image you wish to annotate.</strong><br>
Use <em>File › Open</em> to browse and load the microscopy image that you want to label manually. You can also drag and drop your image to Fiji.</p></li>
<li><p><strong>Select the Oval or Freehand selection tool.</strong><br>
These tools, found in the Fiji toolbar, allow you to manually outline the structures of interest in your image.</p></li>
<li><p><strong>Start drawing around each object (yes, each one!).</strong><br>
Carefully trace each cell or feature you want to annotate—precision is key to ensure useful training data for DL.</p></li>
<li><p><strong>After drawing each object, press “t” on your keyboard → the selection will be stored in the ROI manager.</strong><br>
This adds the drawn region to the ROI (Region of Interest) list, keeping track of all annotated objects in the image.</p></li>
<li><p><strong>Repeat until all objects are in the ROI manager.</strong><br>
Continue drawing and pressing “t” until you have annotated every relevant object in the image.</p></li>
<li><p><strong>When finished, go to <em>Plugins › LOCI › ROI Map</em>.</strong><br>
This plugin converts all saved ROIs into a single labeled ROI map image, assigning unique values to each region.</p></li>
<li><p><strong>Save the generated ROI map with the same title as the original image in a separate folder.</strong><br>
Consistent naming ensures each annotated map can be correctly matched with its corresponding raw image during training or analysis.</p></li>
<li><p><strong>At the end, you will have one folder with the original images and another for the ROI maps.</strong><br>
This separation makes it easier to organise and use your data with image analysis or DL pipelines.</p></li>
</ol>
</div>
</div>
</div>
</section>
<section id="sec-9.3.4.3" class="level4" data-number="9.3.4.3">
<h4 data-number="9.3.4.3" class="anchored" data-anchor-id="sec-9.3.4.3"><span class="header-section-number">9.3.4.3</span> Accelerating annotation with automatic initial segmentations</h4>
<p>Creating high-quality annotations often represents the most time-consuming aspect of training a DL model for segmentation. To alleviate this burden, you can start from automatically produced initial segmentations. For example, using simple thresholding methods such as Otsu’s thresholding to generate rough segmentations can decrease the total annotation time. Even more powerfully, pre-trained DL models such as those provided with StarDist<span class="citation" data-cites="schmidt2018"><sup><a href="references.html#ref-schmidt2018" role="doc-biblioref">9</a></sup></span> and Cellpose<span class="citation" data-cites="stringer2021"><sup><a href="references.html#ref-stringer2021" role="doc-biblioref">10</a></sup></span> can generate more accurate initial segmentations that users can manually refine. These annotations can then be used to retrain the model, establishing an iterative cycle that accelerates both labelling and model refinement.</p>
<p>New tools are also pushing the boundaries of interactive annotation. For example, Segment Anything for Microscopy (μSAM)<span class="citation" data-cites="archit2025"><sup><a href="references.html#ref-archit2025" role="doc-biblioref">15</a></sup></span> facilitates automatic and user-guided segmentation and allows the model to be retrained on user-provided data. Similarly, Cellpose 2.0<span class="citation" data-cites="pachitariu2022"><sup><a href="references.html#ref-pachitariu2022" role="doc-biblioref">16</a></sup></span> features a human-in-the-loop workflow, allowing users to edit DL-generated segmentations. This hybrid approach enhances accuracy while significantly reducing the time and effort required for manual annotation.</p>
</section>
<section id="sec-9.3.4.4" class="level4" data-number="9.3.4.4">
<h4 data-number="9.3.4.4" class="anchored" data-anchor-id="sec-9.3.4.4"><span class="header-section-number">9.3.4.4</span> Expanding your dataset with augmentation and synthetic data</h4>
<p>When the number of training samples is limited, <a href="./glossary.html#data-augmentation">augmentation</a> techniques can enhance dataset diversity to improve the model’s generalisation ability and performance on validation and testing<span class="citation" data-cites="lecun1998 shorten2019"><sup><a href="references.html#ref-lecun1998" role="doc-biblioref">25</a>,<a href="references.html#ref-shorten2019" role="doc-biblioref">26</a></sup></span>. Common augmentation strategies include image rotation, flipping, scaling, and contrast adjustment. However, it’s important to apply augmentation carefully, as excessive or unrealistic augmentation can confuse the model or cause it to learn patterns that do not exist in real data.</p>
<p>In the absence of sufficient real data, synthetic data generated through simulations or <a href="./glossary.html#domain-randomization">domain randomization</a> can help pre-training a model<span class="citation" data-cites="lin2022 rangel_dacosta2024"><sup><a href="references.html#ref-lin2022" role="doc-biblioref">27</a>,<a href="references.html#ref-rangel_dacosta2024" role="doc-biblioref">28</a></sup></span>. These synthetic samples can expose the model to a broader range of scenarios early in training, before transitioning to fine-tuning with real, annotated data.</p>
<p>In summary, a successful segmentation pipeline relies on a careful balance between data quantity and annotation quality. Augmentation strategies can efficiently help to scale and balance training datasets.</p>
</section>
<section id="sec-9.3.4.5" class="level4" data-number="9.3.4.5">
<h4 data-number="9.3.4.5" class="anchored" data-anchor-id="sec-9.3.4.5"><span class="header-section-number">9.3.4.5</span> Choosing the dataset size: specific vs.&nbsp;general models</h4>
<p>In supervised training, it is crucial that each image in the training set is accompanied by a corresponding label image (see <a href="#sec-9.3.4.1" class="quarto-xref"><span>Section 9.3.4.1</span></a>). The number of image-label pairs required depends on the number of labels per image, the complexity of the model and the desired level of generalisability. Still, the key is having enough representative examples and corresponding annotations for the model to learn meaningful patterns.</p>
<p>Small and well-curated datasets consisting of tens of images may suffice for highly specific applications, such as segmenting cells or nuclei using a defined imaging modality<span class="citation" data-cites="von_chamier2021"><sup><a href="references.html#ref-von_chamier2021" role="doc-biblioref">17</a></sup></span>. In these scenarios, <a href="./glossary.html#transfer-learning">transfer learning</a> can also be especially beneficial (see <a href="#sec-9.3.6" class="quarto-xref"><span>Section 9.3.6</span></a>). Models designed to generalise across a wide range of conditions, tissue types, or imaging modalities typically require much larger and more diverse datasets (hundreds to thousands of annotated images)<span class="citation" data-cites="stringer2021"><sup><a href="references.html#ref-stringer2021" role="doc-biblioref">10</a></sup></span>. These datasets are essential for capturing the inherent variability in broader use cases.</p>
</section>
</section>
<section id="sec-9.3.5" class="level3" data-number="9.3.5">
<h3 data-number="9.3.5" class="anchored" data-anchor-id="sec-9.3.5"><span class="header-section-number">9.3.5</span> Training a segmentation model from scratch</h3>
<p>Once you have annotated your training dataset, the next steps are to organise your data for training, initialise your model by selecting appropriate <a href="./glossary.html#hyperparameters">hyperparameters</a>, and start the training process (<a href="#fig-7" class="quarto-xref">Figure&nbsp;<span>9.7</span></a>).</p>
<section id="sec-9.3.5.1" class="level4" data-number="9.3.5.1">
<h4 data-number="9.3.5.1" class="anchored" data-anchor-id="sec-9.3.5.1"><span class="header-section-number">9.3.5.1</span> Splitting your training data: training, validation, and test sets</h4>
<p>A crucial part of preparing your dataset is dividing it into three subsets: training, validation, and test sets. Each subset should contain the original microscopy images paired with their corresponding ground truth segmentations. A common strategy is to allocate 70–80% of the data for training, 10–15% for validation, and the remainder for testing. To ensure unbiased evaluation, ensure these subsets do not overlap in terms of fields of view, represent the variability of your entire dataset, and are randomly assigned to each set respectively.</p>
<p>The <strong>training set</strong> is used to train the model to recognise relevant features. To enhance generalisation, it must encompass a broad spectrum of scenarios and image conditions. Otherwise, the model risks overfitting—excelling with the training data but faltering with new images (<a href="#fig-6" class="quarto-xref">Figure&nbsp;<span>9.6</span></a>).</p>
<p>The <strong>validation set</strong> is used during training to provide feedback on the model’s performance with unseen data. This feedback, conveyed as validation loss, assists in detecting overfitting (<a href="#fig-6" class="quarto-xref">Figure&nbsp;<span>9.6</span></a>), guiding hyperparameter tuning (see <a href="#sec-9.3.5.3" class="quarto-xref"><span>Section 9.3.5.3</span></a>), and informing training decisions. Although a separate validation set is ideal, many workflows create one in practice by reserving a portion (typically 10% to 30%) of the training data.</p>
<p>The <strong>test set</strong>, which serves a separate role, evaluates the model’s performance on entirely unseen data. Unlike the validation set, the test set is not utilised during training, ensuring an unbiased performance assessment. Test images should also include ground truth annotations to facilitate quantitative quality control. Reporting test set performance, using metrics such as accuracy, IoU, or F1-score, is crucial, especially when publishing or benchmarking your model<span class="citation" data-cites="laine2021"><sup><a href="references.html#ref-laine2021" role="doc-biblioref">29</a></sup></span>.</p>
</section>
<section id="sec-9.3.5.2" class="level4" data-number="9.3.5.2">
<h4 data-number="9.3.5.2" class="anchored" data-anchor-id="sec-9.3.5.2"><span class="header-section-number">9.3.5.2</span> Understanding the training process</h4>
<p>A DL model is composed of multiple layers (<a href="#fig-5" class="quarto-xref">Figure&nbsp;<span>9.5</span></a>). Each layer contains tens to hundreds of image processing operations (typically multiplications or convolutions), each controlled by multiple adjustable parameters (called weights). Altogether, a DL model may contain millions of adjustable weights. When an input image is processed by a DL model, it is sequentially processed by each layer until an output is generated. Segmentation tasks typically involve converting input images into labelled outputs. During training, the model weights are modified as the model learns how to perform a specific task.</p>
<div id="fig-5" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="9-figures\chapter9_5_UNET_architecture.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.5: 2D U-Net architecture for image segmentation. It applies layers of convolutions, pooling, and upsampling to extract features and generate labelled segmentation masks. During training, model weights are iteratively adjusted based on the difference between predictions and ground truth labels, using a loss function and backpropagation.
</figcaption>
</figure>
</div>
<p>Training begins with initialising these weights. When training from scratch, the initialisation is often random. However, when using a pre-trained model, the weights are already optimized based on previous training, providing the model with a significant head start (see <a href="#sec-9.3.6" class="quarto-xref"><span>Section 9.3.6</span></a>).</p>
<p>The training process is iterative (<a href="#fig-7" class="quarto-xref">Figure&nbsp;<span>9.7</span></a>). Each cycle of training is called an <a href="./glossary.html#epoch">epoch</a>. During each epoch, the model typically learns from every image in the training set. Since datasets are often too large to fit into memory all at once, each epoch is divided into steps or iterations, with each step processing a smaller subset of the data known as a batch. The batch size determines how many samples are processed simultaneously.</p>
<p>During each step, the model generates predictions for the current data batch. These predictions are compared to the ground truth labels using a loss function that calculates the similarity between the predictions and the ground truths. This score is called the training loss. The model utilises this feedback to adjust its weights through a process known as <a href="./glossary.html#backpropagation">backpropagation</a>, guided by an optimisation algorithm, to improve its accuracy in future iterations.</p>
<p>At the end of each epoch, the model assesses its performance on the validation set, which comprises data it has not encountered during training. This produces the validation loss, indicating how well the model generalises to new data.</p>
<p>Monitoring both training and validation losses during training helps determine whether the model is learning effectively. A consistently decreasing validation loss indicates that the model is improving and generalising well (see <a href="#sec-9.3.5.4" class="quarto-xref"><span>Section 9.3.5.4</span></a>).</p>
</section>
<section id="sec-9.3.5.3" class="level4" data-number="9.3.5.3">
<h4 data-number="9.3.5.3" class="anchored" data-anchor-id="sec-9.3.5.3"><span class="header-section-number">9.3.5.3</span> Choosing your model hyperparameters</h4>
<p>Now that you understand the training process, the next step is to configure the model’s hyperparameters, which are the settings that dictate how the model learns. While the model’s parameters (its weights) are updated during training, hyperparameters are established beforehand, defining the structure and behaviour of the training process. Below are some of the most common hyperparameters and their effects on training:</p>
<ul>
<li><p><strong>Batch size:</strong> This refers to the number of images processed simultaneously in each training step. Smaller batch sizes are less demanding on memory and may enhance generalisation, although they can result in slower training. In contrast, larger batch sizes accelerate training but necessitate more GPU memory.</p></li>
<li><p><strong>Epochs:</strong> An epoch refers to a training cycle in which the model processes the entire training dataset. Increasing the number of epochs allows the model to learn more, but also raises the risk of overfitting. More is not always better; it is essential to monitor performance on the validation set.</p></li>
<li><p><strong>Learning rate:</strong> It determines the extent to which the model’s weights are adjusted during training. A high learning rate can result in quicker training but may overshoot the optimal solution. Conversely, a low learning rate provides more stable progress, although it may slow down convergence.</p></li>
<li><p><strong>Optimizer:</strong> An algorithm that updates weights to minimise the loss function. Common optimisers include SGD (stochastic gradient descent) and Adam (adaptive moment estimation), the latter being widely used for its adaptive learning rate and robust performance.</p></li>
<li><p><strong>Learning rate scheduler:</strong> Dynamically adjusts the learning rate during training, typically decreasing it after a specific number of epochs or when the validation loss plateaus. This approach helps balance rapid early learning with more refined convergence later on.</p></li>
<li><p><strong>Patch size:</strong> Instead of using full-resolution images, smaller patches are often utilised for training to reduce memory usage and enhance training speed. The patch size is determined by both available resources and the scale of the structures to be segmented.</p></li>
<li><p><strong>Patience (early stopping):</strong> This parameter defines the number of epochs to wait before halting training if the validation loss does not improve. It helps prevent wasting resources on overfitting and overtraining.</p></li>
</ul>
<p>Given the many possible configurations, tuning hyperparameters is often essential—especially when applying a model to new data. Start with the recommended values from the model’s original publication, but you might need to conduct a hyperparameter search to optimize performance. This can range from a simple grid search to more advanced methods, such as <a href="./glossary.html#gaussian-process">Gaussian process-based</a> <a href="./glossary.html#bayesian-optimization">Bayesian optimisation</a><span class="citation" data-cites="ilievski2017"><sup><a href="references.html#ref-ilievski2017" role="doc-biblioref">30</a></sup></span> or <a href="./glossary.html#genetic-algorithms">genetic algorithms</a><span class="citation" data-cites="alibrahim2021"><sup><a href="references.html#ref-alibrahim2021" role="doc-biblioref">31</a></sup></span>.</p>
</section>
<section id="sec-9.3.5.4" class="level4" data-number="9.3.5.4">
<h4 data-number="9.3.5.4" class="anchored" data-anchor-id="sec-9.3.5.4"><span class="header-section-number">9.3.5.4</span> Monitoring training and validation Losses</h4>
<p>Once your model begins training, it is helpful to evaluate its learning progress. The two key metrics for assessment are the training loss and the validation loss (<a href="#fig-6" class="quarto-xref">Figure&nbsp;<span>9.6</span></a>). Monitoring both throughout the training process offers insight into whether your model is improving and learning to generalise beyond the training data. The three main behaviours that you may encounter during training are:</p>
<div id="fig-6" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="9-figures/chapter9_6_Underfitting_VS_Overfitting.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.6: Monitoring training and validation losses during model training. The plot illustrates three typical learning behaviours: <strong>underfitting</strong> (both losses remain high and similar), <strong>good fitting</strong> (both losses decrease, with validation loss slightly higher), and <strong>overfitting</strong> (training loss continues to decrease while validation loss plateaus or increases), highlighting the importance of tracking these metrics to assess model performance and generalisation.
</figcaption>
</figure>
</div>
<ul>
<li><strong>Underfitting:</strong> The model has been trained with insufficient data or for too few epochs, resulting in similar training and validation losses, which is far from optimal.<br>
</li>
<li><strong>Good fitting:</strong> Both training and validation losses decrease, with the validation loss slightly higher (worse) than the training loss, which is expected. This represents the ideal scenario.<br>
</li>
<li><strong>Overfitting:</strong> The model achieves an excellent training loss, but the validation loss does not improve and may in fact diverge. This may indicate overly similar training data or excessive training epochs, preventing the model from generalising to new data.</li>
</ul>
</section>
</section>
<section id="sec-9.3.6" class="level3" data-number="9.3.6">
<h3 data-number="9.3.6" class="anchored" data-anchor-id="sec-9.3.6"><span class="header-section-number">9.3.6</span> Fine-Tuning Pre-existing Models</h3>
<p>Instead of training a model from scratch, fine-tuning an existing DL model is usually more efficient, especially when your data resembles the dataset used to train the original model. This approach utilises pre-trained weights and previously learned features, significantly decreasing the amount of required annotated data, training time, and computational resources.</p>
<section id="sec-9.3.6.1" class="level4" data-number="9.3.6.1">
<h4 data-number="9.3.6.1" class="anchored" data-anchor-id="sec-9.3.6.1"><span class="header-section-number">9.3.6.1</span> Applying Transfer Learning</h4>
<p>Transfer learning refers to the process of taking a pre-trained model and adapting it to a new but related task by providing task-specific training data, typically in the form of manually annotated image pairs (<a href="#fig-7" class="quarto-xref">Figure&nbsp;<span>9.7</span></a>). Transfer learning typically involves freezing part of the model (for instance, the initial layers or all layers except the last ones), so their weights are not updated during training. Only the unfrozen layers are updated when the model is trained on new data. Then, the model is trained on the new data, but only the layers that you have unfrozen will be updated. Since the base model already encodes many useful low-level features (e.g., edges, shapes, textures), this approach allows researchers to focus on refining the model for their specific biological structures or imaging modalities<span class="citation" data-cites="li2018 morid2021"><sup><a href="references.html#ref-li2018" role="doc-biblioref">32</a>,<a href="references.html#ref-morid2021" role="doc-biblioref">33</a></sup></span>.</p>
<p>This method is especially effective when:</p>
<ul>
<li>You have limited training data available.<br>
</li>
<li>Your imaging conditions closely match those of the pre-trained model.<br>
</li>
<li>You wish to quickly adapt a general model to a specific dataset.</li>
</ul>
</section>
<section id="sec-9.3.6.2" class="level4" data-number="9.3.6.2">
<h4 data-number="9.3.6.2" class="anchored" data-anchor-id="sec-9.3.6.2"><span class="header-section-number">9.3.6.2</span> Conducting fine-tuning</h4>
<p>In classic fine-tuning, all layers of the pre-trained model are retrained, with their weights initialised from the original training (<a href="#fig-7" class="quarto-xref">Figure&nbsp;<span>9.7</span></a>)<span class="citation" data-cites="archit2025 pachitariu2022"><sup><a href="references.html#ref-archit2025" role="doc-biblioref">15</a>,<a href="references.html#ref-pachitariu2022" role="doc-biblioref">16</a></sup></span>. Thus, you continue training the full model using the new data. This approach allows the model to adjust more comprehensively to new data while still preserving the advantages of pre-learned features.</p>
<p>Classic fine-tuning is ideal when:</p>
<ul>
<li>Your dataset is moderately different from the original training data (e.g., the same biological structure but different staining or modality).<br>
</li>
<li>You expect that earlier layers may need to adapt, not just the final classifier or output layers. Early layers in deep networks typically learn to detect general features such as edges, textures, or simple shapes, while later layers capture more complex, task-specific patterns. If your new data differs in basic appearance or imaging modality, updating the early layers helps the model better extract relevant low-level features from your images.<br>
</li>
<li>You have enough annotated data to avoid overfitting during full model training. Although this method is more computationally demanding than transfer learning, where only a subset of layers are retrained, it often leads to better results on diverse datasets..</li>
</ul>
</section>
<section id="sec-9.3.6.3" class="level4" data-number="9.3.6.3">
<h4 data-number="9.3.6.3" class="anchored" data-anchor-id="sec-9.3.6.3"><span class="header-section-number">9.3.6.3</span> Iterative training: keeping humans in the loop</h4>
<p>Iterative fine-tuning is an interactive approach that combines model prediction with human annotation (see <a href="#sec-9.3.4.3" class="quarto-xref"><span>Section 9.3.4.3</span></a>). The workflow typically starts with a pre-trained model predicting new images. A user then manually corrects or annotates these predictions, and the improved annotations update the model (<a href="#fig-7" class="quarto-xref">Figure&nbsp;<span>9.7</span></a>). This cycle continues, progressively enhancing the model’s accuracy with each iteration until it performs as expected<span class="citation" data-cites="chen2020 conrad2023 pachitariu2022"><sup><a href="references.html#ref-pachitariu2022" role="doc-biblioref">16</a>,<a href="references.html#ref-chen2020" role="doc-biblioref">34</a>,<a href="references.html#ref-conrad2023" role="doc-biblioref">35</a></sup></span>.</p>
<p>This method is particularly powerful when:</p>
<ul>
<li>Annotated data is scarce or expensive to generate.<br>
</li>
<li>You work with rare structures, unusual imaging conditions, or new experimental systems.<br>
</li>
<li>You want to efficiently build a custom model using feedback from domain experts.</li>
</ul>
<div id="fig-7" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="9-figures\chapter9_7_training_types.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.7: Strategies for training DL models for image segmentation. A model can be <strong>trained from scratch</strong> using a large annotated dataset, <strong>fine-tuned</strong> from a pre-trained model with task-specific data, or refined through <strong>human-in-the-loop</strong> workflows where model predictions are manually corrected and fed back for retraining. These approaches balance performance, data availability, and annotation effort.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-9.3.7" class="level3" data-number="9.3.7">
<h3 data-number="9.3.7" class="anchored" data-anchor-id="sec-9.3.7"><span class="header-section-number">9.3.7</span> Evaluating the performance of your model</h3>
<div id="fig-8" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="9-figures\chapter9_8_monitoring.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.8: Workflow for evaluating DL model performance during training and on test data. Evaluating model performance is essential before deploying any DL model for image segmentation. This diagram outlines a two-stage process: assessment during training and on a separate test set. During training, validation and training losses (see <a href="#sec-9.3.5.4" class="quarto-xref"><span>Section 9.3.5.4</span></a>) guide whether to continue training, stop, or expand the dataset. After training, performance is evaluated using a test set. High test metrics (e.g., IoU, F1-score) indicate readiness for deployment. Borderline or poor results suggest reviewing errors, refining training data, or trying a different model. This approach ensures model reliability and task-specific performance.
</figcaption>
</figure>
</div>
<p>With the rapid increase in DL tools and pre-trained models, it has become easier to use DL for image segmentation, but harder to determine which model will work best for your data. Regardless of how promising a model appears, you must always evaluate its performance before trusting its results. Evaluation is not optional; it is a critical step to ensure that the model meets the requirements of your specific task<span class="citation" data-cites="laine2021"><sup><a href="references.html#ref-laine2021" role="doc-biblioref">29</a></sup></span> (<a href="#fig-8" class="quarto-xref">Figure&nbsp;<span>9.8</span></a>).</p>
<p>There are two main ways to evaluate a model:</p>
<ul>
<li>Qualitative evaluation entails visually inspecting the model’s predictions. This approach can help you quickly identify clear errors or failures. It is effective for a small number of images, but it becomes impractical for large datasets or for comparing similar-looking outputs across multiple models.<br>
</li>
<li>Quantitative evaluation provides objective metrics for comparing models and tracking improvements. To achieve this, you need a small, labelled test set (typically 5 to 10 images with accurate ground truth segmentations). This test set must remain independent of your training and validation data to ensure an unbiased assessment.</li>
</ul>
<p>Common metrics used in quantitative evaluation include:</p>
<ul>
<li>Intersection over Union (IoU), also known as the Jaccard Index, measures the overlap between the predicted segmentation and the ground truth.<br>
</li>
<li>F1-score (Dice coefficient): This is especially valuable when the object of interest covers a small area in the image, as it balances precision and recall.<br>
</li>
<li><a href="./glossary.html#true-positives">True Positives</a> (TP), <a href="./glossary.html#false-positives">False Positives</a> (FP), and <a href="./glossary.html#false-negatives">False Negatives</a> (FN) are particularly important in semantic segmentation and can be used to calculate the <a href="./glossary.html#iou">IoU</a> or <a href="./glossary.html#f1-score">F1 score</a>.</li>
</ul>
<p>For more information on these metrics, we recommend<span class="citation" data-cites="laine2021"><sup><a href="references.html#ref-laine2021" role="doc-biblioref">29</a></sup></span> (also see <a href="10-output-quality.html" class="quarto-xref"><span>Chapter 10</span></a> for more information).</p>
<p>If a model fails to produce reasonable results, even on simple examples, you can often reject it based solely on qualitative inspection. However, in these cases, quantitative metrics can still help you understand how and where the model fails.</p>
<p>If your evaluation metrics indicate weak performance, especially for certain structures or image types, you may need to fine-tune the model (see <a href="#sec-9.3.6" class="quarto-xref"><span>Section 9.3.6</span></a>). Consistently strong scores across various test images suggest that a model could be dependable and ready for deployment. If no pre-trained model meets your expectations, the best course may be to train your model using your images (see <a href="#sec-9.3.5" class="quarto-xref"><span>Section 9.3.5</span></a>).</p>
<p>In summary, <strong>never skip evaluation</strong>. Every model must be tested—both visually and quantitatively—to ensure it truly works for your data and provides results you can trust.</p>
</section>
<section id="sec-9.3.8" class="level3" data-number="9.3.8">
<h3 data-number="9.3.8" class="anchored" data-anchor-id="sec-9.3.8"><span class="header-section-number">9.3.8</span> Deploying your model on new data</h3>
<p>Once a segmentation model has been trained and validated, it can be used on new, unseen images. This step typically involves feeding new images into the model to generate segmentation predictions. The deployment approach relies on the computational resources (see <a href="#sec-9.4.2" class="quarto-xref"><span>Section 9.4.2</span></a>) as well as the size and complexity of your dataset (<a href="#fig-9" class="quarto-xref">Figure&nbsp;<span>9.9</span></a>).</p>
<div id="fig-9" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="9-figures/chapter9_9_deployment.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.9: Decision workflow for model deployment strategy based on computational resources. The choice of deployment strategy depends on the availability of computational resources (see <a href="#sec-9.4.2" class="quarto-xref"><span>Section 9.4.2</span></a>) and the sensitivity of the data . If high-performance computing resources are available locally, these should be used for deployment. In their absence, consider whether the data can be transferred to the cloud. If so, cloud-based resources offer an efficient solution. However, if data transfer is restricted—due to size or sensitivity—local deployment remains the only option, though it may require significantly more time.
</figcaption>
</figure>
</div>
</section>
<section id="sec-9.3.9" class="level3" data-number="9.3.9">
<h3 data-number="9.3.9" class="anchored" data-anchor-id="sec-9.3.9"><span class="header-section-number">9.3.9</span> Troubleshooting Common Problems</h3>
<p><strong>I found a tool or DL model online, but it does not work. What should I do?</strong></p>
<div id="fig-10" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="9-figures/chapter9_10_Does yout model work.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.10: Common DL segmentation problems and troubleshooting tips.
</figcaption>
</figure>
</div>
<p><strong>When should I train a model or segment manually?</strong></p>
<p>Refer to (<a href="#sec-9.3.3.2" class="quarto-xref"><span>Section 9.3.3.2</span></a>) for more details, but generally, this decision depends on your dataset and the performance of existing pre-trained models (<a href="#fig-10" class="quarto-xref">Figure&nbsp;<span>9.10</span></a>). If you only need to segment a small number of images, manually segmenting them is often the quickest and simplest solution. However, if you are dealing with a large dataset, it may be more efficient to annotate a small subset and use it to train a deep-learning model that can automate the segmentation of the rest.</p>
<p><strong>I decided to train my DL model, but it is not performing correctly. What should I do?</strong></p>
<p>First, ensure that you have trained the model for a sufficient number of epochs—this depends on the size of your dataset and the architecture of the model. Check the training loss: if it has plateaued, your model may be fully trained. If it is still decreasing, continue training.</p>
<p>If training is completed but results are poor, examine your data. Is the model missing specific features? Are there types of cells or structures that it consistently fails to segment? If so, ensure those examples are well represented and correctly annotated in your training data. You may need to enhance or expand your annotations.</p>
<p>If performance is poor, you may need additional annotated data to help the model generalise more effectively (<a href="#fig-8" class="quarto-xref">Figure&nbsp;<span>9.8</span></a>). Consider the following questions:</p>
<ul>
<li>Is my dataset balanced? Does it include sufficient examples of each structure or class I want to segment?<br>
</li>
<li>Am I training on one experimental batch while validating or testing on another?</li>
</ul>
<p><strong>How many images should I have to train my model?</strong></p>
<p>Refer to <a href="#sec-9.2.1" class="quarto-xref"><span>Section 9.2.1</span></a> for more details. There’s no one-size-fits-all answer—it depends on the complexity of your task, your model architecture, and the variability in your data. More complex tasks typically require more data. Larger images can also be broken into more patches, effectively increasing your dataset size. While few-shot models are being developed for small datasets, most established DL models require a substantial amount of data.</p>
<p><strong>Possible technical issues that you may encounter when training your DL model.</strong></p>
<ul>
<li>The model predicts the same class for all pixels or segments in every cell. Your dataset might be unbalanced, containing too many similar examples. Adding more diverse or underrepresented examples can help the model learn to differentiate between classes.<br>
</li>
<li>Out-of-memory errors during training: Consider reducing the batch size or the image patch size. If that doesn’t resolve the issue, consider switching to a workstation or cloud service with greater computational capacity.<br>
</li>
<li>The model performs well on training data but poorly on new images, suggesting overfitting (<a href="#fig-6" class="quarto-xref">Figure&nbsp;<span>9.6</span></a>). Implement data augmentation and increase dataset diversity to help the model generalise better.<br>
</li>
<li>Inconsistent results across different computers: Differences in GPUs or environments can cause slight variations in outcomes. If the differences are significant, verify that all systems use consistent software versions and configurations. For further information on this topic, refer to <a href="#sec-9.4.3" class="quarto-xref"><span>Section 9.4.3</span></a>.</li>
</ul>
</section>
</section>
<section id="sec-9.4" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="sec-9.4"><span class="header-section-number">9.4</span> Further considerations for DL segmentation</h2>
<section id="sec-9.4.1" class="level3" data-number="9.4.1">
<h3 data-number="9.4.1" class="anchored" data-anchor-id="sec-9.4.1"><span class="header-section-number">9.4.1</span> Choosing the Right Tools for DL</h3>
<p>Selecting the right tools to train and use DL models depends mainly on your level of programming experience and comfort with technical interfaces.</p>
<p>If you prefer not to write code or use command-line tools, opt for platforms that offer graphical user interfaces (GUIs) or interactive notebooks with pre-configured workflows. These tools let you perform powerful segmentation tasks using intuitive interfaces and simple widgets.</p>
<p>GUI-based tools include, for instance (see <a href="8-existing-tools.html" class="quarto-xref"><span>Chapter 8</span></a> for more tools):</p>
<ul>
<li>Cellpose GUI<br>
</li>
<li>Fiji with DeepImageJ and StarDist plugins<br>
</li>
<li>Napari<br>
</li>
<li>Ilastik<br>
</li>
<li>QuPath</li>
</ul>
<p>Interactive Jupyter notebooks provide a flexible balance between code and GUI. They enable you to execute code in manageable steps (cells) and immediately see the results. Tools like BiaPy, and DL4MicEverywhere<span class="citation" data-cites="hidalgo-cenalmor2024"><sup><a href="references.html#ref-hidalgo-cenalmor2024" role="doc-biblioref">36</a></sup></span> leverage Jupyter notebooks, concealing complex code behind user-friendly interfaces. These platforms cater to users with little or no coding experience while still allowing advanced users to access and modify code as needed. DL4MicEverywhere, in particular, established a widely adopted framework for training DL models via notebooks, contributing to the standardisation and simplification of the workflow.</p>
<p>If you are comfortable with programming, you will have even more flexibility. Languages such as Python, MATLAB, Julia, Java, and Rust provide options for building and customizing DL workflows. Python stands out as the most beginner-friendly and widely supported choice, boasting a large ecosystem of libraries and community support. Popular Python libraries for DL include PyTorch, TensorFlow, Keras, and JAX.</p>
<p>While coding can involve a steeper learning curve, it allows you to create customized pipelines, integrate various tools, and troubleshoot intricate workflows, unlocking the full potential of DL for microscopy segmentation.</p>
</section>
<section id="sec-9.4.2" class="level3" data-number="9.4.2">
<h3 data-number="9.4.2" class="anchored" data-anchor-id="sec-9.4.2"><span class="header-section-number">9.4.2</span> Managing Computational Resources</h3>
<p>When using DL for microscopy, an important consideration is the availability and capacity of your computational resources (<a href="#fig-9" class="quarto-xref">Figure&nbsp;<span>9.9</span></a>). High-performance DL models, particularly those used for 3D image data, can be very demanding regarding memory and processing power.</p>
<p>When selecting or designing a DL model, evaluate your available infrastructure:</p>
<ul>
<li>GPU memory: Determines how large your model and batch size can be.<br>
</li>
<li>Training time: Influences your ability to iterate quickly; simpler models train faster.<br>
</li>
<li>Dataset size: Larger datasets benefit from more powerful hardware and longer training times.</li>
</ul>
<p>A practical strategy involves starting with lightweight models that demand fewer resources and scaling up to more complex architectures only if performance improvements become necessary. Tools like StarDist and Cellpose, for example, provide efficient options that function effectively with relatively modest hardware.</p>
<p>Additionally, consider whether to train and deploy your model locally or in the cloud (<a href="#fig-11" class="quarto-xref">Figure&nbsp;<span>9.11</span></a>). Local training is often more feasible if you already have access to a compatible workstation and want full control over data and execution. However, cloud-based services like Google Colab or AWS offer access to more powerful hardware, removing the need for local infrastructure—this is especially beneficial when working with large models or 3D datasets.</p>
<p>There are four typical combinations of training and prediction workflows:</p>
<ul>
<li><p>Training and prediction locally is well suited for small to medium-sized datasets, especially when computational demands are moderate and data privacy is a priority. This approach also supports some user-friendly desktop applications, such as the Cellpose 2.0<span class="citation" data-cites="pachitariu2022"><sup><a href="references.html#ref-pachitariu2022" role="doc-biblioref">16</a></sup></span>, which can be run locally without requiring cloud access or advanced technical setup.</p></li>
<li><p>Training locally, prediction in the cloud may be useful when models are trained in-house but need to be deployed at scale or integrated into cloud-based pipelines.</p></li>
<li><p>Training in the cloud, prediction locally enables researchers to take advantage of powerful cloud GPUs for model development, while keeping inference close to the data source (e.g., a microscope workstation or in case of sensitive data).</p></li>
<li><p>Training and prediction in the cloud is well suited for collaborative projects or large-scale deployments, where access to centralized, scalable infrastructure is critical.</p></li>
</ul>
<p>Choosing between these strategies depends on your data size, hardware access, choice of software, collaboration needs, and whether your workflow prioritizes flexibility, scalability, or control.</p>
<div id="fig-11" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="9-figures/chapter9_11_what resources to use when trainig.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.11: Training and deployment strategies for DL models in microscopy. Depending on the available tools and infrastructure, models can be trained and deployed locally or in the cloud. Modified from<span class="citation" data-cites="von_chamier2021"><sup><a href="references.html#ref-von_chamier2021" role="doc-biblioref">17</a></sup></span>.
</figcaption>
</figure>
</div>
</section>
<section id="sec-9.4.3" class="level3" data-number="9.4.3">
<h3 data-number="9.4.3" class="anchored" data-anchor-id="sec-9.4.3"><span class="header-section-number">9.4.3</span> Ensuring Reproducibility in DL</h3>
<p>When sharing how you trained a DL model, two key elements often come to mind: the dataset used and the code that runs the model. However, in practice, reproducibility extends beyond just data and code. In programming environments like Python, which rely heavily on external libraries, ensuring reproducibility also requires capturing the exact configuration of the environment in which the model was trained.</p>
<p>DL models are sensitive to changes in library versions and dependencies. Even minor differences in the software stack can result in inconsistent outcomes or training failures. While sharing a list of dependencies (e.g., a requirements.txt or a Conda environment file) is a constructive step, differences in operating systems or local setups can still lead to issues.</p>
<p>A robust and increasingly popular solution is containerisation. Containers package software, dependencies, and environment settings into a portable and self-contained unit. One of the most widely used containerization tools is Docker. A Docker container can be considered a lightweight, standalone <a href="./glossary.html#virtual-machine">virtual machine</a> that includes everything needed to run code, such as the operating system, libraries, and runtime, ensuring applications run consistently across different machines.</p>
<p>Using containers ensures that your model training and inference processes remain consistent, no matter who executes them or where they are conducted. This greatly simplifies the ability of collaborators or reviewers to reproduce your results.</p>
<p>For researchers unfamiliar with software development, tools like DL4MicEverywhere<span class="citation" data-cites="hidalgo-cenalmor2024"><sup><a href="references.html#ref-hidalgo-cenalmor2024" role="doc-biblioref">36</a></sup></span> and bia-binder<span class="citation" data-cites="russell2024"><sup><a href="references.html#ref-russell2024" role="doc-biblioref">37</a></sup></span> simplify the use of containers by integrating them into user-friendly Jupyter notebook environments. These platforms enable researchers to benefit from the reproducibility of containers without needing to manage complex setups or command-line tools.</p>
<p>Reproducibility is crucial for establishing trust in computational results and facilitating long-term scientific collaboration. To ensure your DL workflows are reproducible, follow these best practices:</p>
<ul>
<li>Pin every software version used in your workflow.<br>
</li>
<li>Document your environment setup thoroughly.<br>
</li>
<li>Provide a containerised version of your training and inference pipeline when possible.<br>
</li>
<li>Taking these steps will make it easier for others to reproduce your results, build on your work, and apply your models in different research settings.</li>
</ul>
<p>For more information on best practices, consult<span class="citation" data-cites="laine2021"><sup><a href="references.html#ref-laine2021" role="doc-biblioref">29</a></sup></span>.</p>
</section>
</section>
<section id="sec-9.5" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="sec-9.5"><span class="header-section-number">9.5</span> Summary &amp; Outlook</h2>
<p>Segmenting microscopy images remains a critical yet challenging task in bioimage analysis. In this chapter, we have used segmentation as a representative example to illustrate deep learning workflows and considerations. However, the strategies and best practices described here—such as data preparation, model selection, training, evaluation, and deployment—are relevant to a wide range of image analysis tasks, including classification, detection, and tracking. DL has undeniably transformed this field, offering robust solutions for segmenting complex and variable structures. However, as this chapter emphasizes, DL is not always the fastest or the best approach. Classical image processing techniques or pixel classifiers often provide faster, simpler, and highly effective alternatives in many scenarios.</p>
<p>The decision to use DL should be driven by the complexity of the task, the availability of annotated data, and the specific goals of the segmentation project. Successful DL implementations often require significant investments in data curation, annotation, and computational resources. Furthermore, training from scratch is frequently avoidable thanks to the growing ecosystem of pre-trained models and resources shared by the community.</p>
<p>Notably, the landscape of DL segmentation is rapidly evolving. The emergence of foundation models, which are large, versatile networks pre-trained on vast and diverse datasets, promises to further lower the barriers to entry<span class="citation" data-cites="archit2025"><sup><a href="references.html#ref-archit2025" role="doc-biblioref">15</a></sup></span>. These models enable transfer learning, fine-tuning, and even zero-shot segmentation, where accurate predictions can be made on previously unseen data with minimal or no task-specific training. This shift opens exciting new avenues for researchers who previously lacked the resources or expertise to apply DL in their work.</p>
<p>The ongoing development and democratization of DL tools, along with enhancements in model generalisability, human-in-the-loop workflows, and reproducibility, are changing how microscopy data is analyzed. Still, the key to successful segmentation will always involve careful planning, quality control, and selecting the right tool for the task, whether it involves DL or not.</p>
<!-- 
## References {#sec-9.6}

   [**Ahlers, J., Althviz Moré, D., Amsalem, O., Anderson, A., Bokota, G., Boone, P., Bragantini, J., Buckley, G., Burt, A., Bussonnier, M., et al.** (2023). napari: a multi-dimensional image viewer for Python.]

   [**Archit, A., Freckmann, L., Nair, S., Khalid, N., Hilt, P., Rajashekar, V., Freitag, M., Teuber, C., Buckley, G., von Haaren, S., et al.** (2025). Segment Anything for Microscopy. *Nat Methods* **22**, 579–591.]

   [**Arganda-Carreras, I., Kaynig, V., Rueden, C., Eliceiri, K. W., Schindelin, J., Cardona, A. and Sebastian Seung, H.** (2017). Trainable Weka Segmentation: a machine learning tool for microscopy pixel classification. *Bioinformatics* **33**, 2424–2426.]

   [**Arzt, M., Deschamps, J., Schmied, C., Pietzsch, T., Schmidt, D., Tomancak, P., Haase, R. and Jug, F.** (2022). LABKIT: Labeling and Segmentation Toolkit for Big Image Data. *Frontiers in Computer Science* **4**,.]

   [**Bankhead, P., Loughrey, M. B., Fernández, J. A., Dombrowski, Y., McArt, D. G., Dunne, P. D., McQuaid, S., Gray, R. T., Murray, L. J., Coleman, H. G., et al.** (2017). QuPath: Open source software for digital pathology image analysis. *Sci Rep* **7**, 16878\.]

   [**Bannon, D., Moen, E., Schwartz, M., Borba, E., Kudo, T., Greenwald, N., Vijayakumar, V., Chang, B., Pao, E., Osterman, E., et al.** (2021). DeepCell Kiosk: scaling deep learning–enabled cellular image analysis with Kubernetes. *Nat Methods* **18**, 43–45.]

   [**Bejarano, L., Kauzlaric, A., Lamprou, E., Lourenco, J., Fournier, N., Ballabio, M., Colotti, R., Maas, R., Galland, S., Massara, M., et al.** (2024). Interrogation of endothelial and mural cells in brain metastasis reveals key immune-regulatory mechanisms. *Cancer Cell* **42**, 378-395.e10.]

   [**Berg, S., Kutra, D., Kroeger, T., Straehle, C. N., Kausler, B. X., Haubold, C., Schiegg, M., Ales, J., Beier, T., Rudy, M., et al.** (2019). ilastik: interactive machine learning for (bio)image analysis. *Nat Methods* **16**, 1226–1232.]

   [**Caicedo, J. C., McQuin, C., Goodman, A., Singh, S. and Carpenter, A. E.** (2018). Weakly Supervised Learning of Single-Cell Feature Embeddings. In *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 9309–9318.]

   [**Cardoso, M. J., Li, W., Brown, R., Ma, N., Kerfoot, E., Wang, Y., Murrey, B., Myronenko, A., Zhao, C., Yang, D., et al.** (2022). MONAI: An open-source framework for deep learning in healthcare.]

   [**Chen, J., Ding, L., Viana, M. P., Lee, H., Sluezwski, M. F., Morris, B., Hendershott, M. C., Yang, R., Mueller, I. A. and Rafelski, S. M.** (2020). The Allen Cell and Structure Segmenter: a new open source toolkit for segmenting 3D intracellular structures in fluorescence microscopy images. 491035\.]

   [**Conrad, R. and Narayan, K.** (2023). Instance segmentation of mitochondria in electron microscopy images with a generalist deep learning model trained on a diverse dataset. *Cell Systems* **14**, 58-71.e5.](https://www.

   [**Fazeli, E., Roy, N. H., Follain, G., Laine, R. F., von Chamier, L., Hänninen, P. E., Eriksson, J. E., Tinevez, J.-Y. and Jacquemet, G.** (2020). Automated cell tracking using StarDist and TrackMate. *F1000Res* **9**, 1279\.]

   [**Fisch, D., Zhang, T., Sun, H., Ma, W., Tan, Y., Gygi, S. P., Higgins, D. E. and Kagan, J. C.** (2024). Molecular definition of the endogenous Toll-like receptor signalling pathways. *Nature* **631**, 635–644.]

   [**Follain, G., Ghimire, S., Pylvänäinen, J. W., Vaitkevičiūtė, M., Wurzinger, D., Guzmán, C., Conway, J. R., Dibus, M., Oikari, S., Rilla, K., et al.** (2024). Fast label-free live imaging reveals key roles of flow dynamics and CD44-HA interaction in cancer cell arrest on endothelial monolayers. 2024.09.30.615654.]

   [**Heinrich, L., Bennett, D., Ackerman, D., Park, W., Bogovic, J., Eckstein, N., Petruncio, A., Clements, J., Pang, S., Xu, C. S., et al.** (2021). Whole-cell organelle segmentation in volume electron microscopy. *Nature* **599**, 141–146.]

   [**Hidalgo-Cenalmor, I., Pylvänäinen, J. W., G. Ferreira, M., Russell, C. T., Saguy, A., Arganda-Carreras, I., Shechtman, Y., AI4Life Horizon Europe Program Consortium, Muñoz-Barrutia, A., Serrano-Solano, B., et al.** (2024). DL4MicEverywhere: deep learning for microscopy made flexible, shareable and reproducible. *Nat Methods*.]

   [**Kochetov, B., Bell, P. D., Garcia, P. S., Shalaby, A. S., Raphael, R., Raymond, B., Leibowitz, B. J., Schoedel, K., Brand, R. M., Brand, R. E., et al.** (2024). UNSEG: unsupervised segmentation of cells and their nuclei in complex tissue samples. *Commun Biol* **7**, 1–14.]

   [**Krizhevsky, A., Sutskever, I. and Hinton, G. E.** (2012). ImageNet Classification with Deep Convolutional Neural Networks. In *Advances in Neural Information Processing Systems*, p. Curran Associates, Inc.]

   [**Laine, R. F., Arganda-Carreras, I., Henriques, R. and Jacquemet, G.** (2021). Avoiding a replication crisis in deep-learning-based bioimage analysis. *Nat Methods* **18**, 1136–1144.]

   [**Lecun, Y., Bottou, L., Bengio, Y. and Haffner, P.** (1998). Gradient-based learning applied to document recognition. *Proceedings of the IEEE* **86**, 2278–2324.]

   [**Li, Y. and Shen, L.** (2018). cC-GAN: A Robust Transfer-Learning Framework for HEp-2 Specimen Image Segmentation. *IEEE Access* **6**, 14048–14058.]

   [**Lin, B., Emami, N., Santos, D. A., Luo, Y., Banerjee, S. and Xu, B.-X.** (2022). A deep learned nanowire segmentation model using synthetic data augmentation. *npj Comput Mater* **8**, 1–12.]

   [**Liu, B., Polack, M., Coudray, N., Claudio Quiros, A., Sakellaropoulos, T., Le, H., Karimkhan, A., Crobach, A. S. L. P., van Krieken, J. H. J. M., Yuan, K., et al.** (2025). Self-supervised learning reveals clinically relevant histomorphological patterns for therapeutic strategies in colon cancer. *Nat Commun* **16**, 2328\.]

   [**Moen, E., Bannon, D., Kudo, T., Graf, W., Covert, M. and Van Valen, D.** (2019). Deep learning for cellular image analysis. *Nature Methods* **16**, 1233–1246.]

   [**Morid, M. A., Borjali, A. and Del Fiol, G.** (2021). A scoping review of transfer learning research on medical image analysis using ImageNet. *Computers in Biology and Medicine* **128**, 104115\.]

   [**Moshkov, N., Bornholdt, M., Benoit, S., Smith, M., McQuin, C., Goodman, A., Senft, R. A., Han, Y., Babadi, M., Horvath, P., et al.** (2024). Learning representations for image-based profiling of perturbations. *Nat Commun* **15**, 1594\.]

   [**Ouyang, W., Beuttenmueller, F., Gómez-de-Mariscal, E., Pape, C., Burke, T., Garcia-López-de-Haro, C., Russell, C., Moya-Sans, L., de-la-Torre-Gutiérrez, C., Schmidt, D., et al.** (2022). BioImage Model Zoo: A Community-Driven Resource for Accessible Deep Learning in BioImage Analysis. 2022.06.07.495102.]

   [**Pachitariu, M. and Stringer, C.** (2022). Cellpose 2.0: how to train your own model. *Nat Methods* 1–8.]

   [**Pylvänäinen, J. W., Gómez-de-Mariscal, E., Henriques, R. and Jacquemet, G.** (2023). Live-cell imaging in the deep learning era. *Current Opinion in Cell Biology* **85**, 102271\.]

   [**Rangel DaCosta, L., Sytwu, K., Groschner, C. K. and Scott, M. C.** (2024). A robust synthetic data generation framework for machine learning in high-resolution transmission electron microscopy (HRTEM). *npj Comput Mater* **10**, 1–11.]

   [**Ronneberger, O., Fischer, P. and Brox, T.** (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation.]

   [**Russell, C. T., Burel, J.-M., Athar, A., Li, S., Sarkans, U., Swedlow, J., Brazma, A., Hartley, M. and Uhlmann, V.** (2024). bia-binder: A web-native cloud compute service for the bioimage analysis community.]

   [**Schindelin, J., Arganda-Carreras, I., Frise, E., Kaynig, V., Longair, M., Pietzsch, T., Preibisch, S., Rueden, C., Saalfeld, S., Schmid, B., et al.** (2012). Fiji: an open-source platform for biological-image analysis. *Nature Methods* **9**, 676–682.]

   [**Schmidt, U., Weigert, M., Broaddus, C. and Myers, G.** (2018). Cell Detection with Star-Convex Polygons. In *Medical Image Computing and Computer Assisted Intervention – MICCAI 2018* (ed. Frangi, A. F.), Schnabel, J. A.), Davatzikos, C.), Alberola-López, C.), and Fichtinger, G.), pp. 265–273. Cham: Springer International Publishing.]

   [**Shorten, C. and Khoshgoftaar, T. M.** (2019). A survey on Image Data Augmentation for Deep Learning. *Journal of Big Data* **6**, 60\.]

   [**Stringer, C., Wang, T., Michaelos, M. and Pachitariu, M.** (2021). Cellpose: a generalist algorithm for cellular segmentation. *Nat Methods* **18**, 100–106.]

   [**von Chamier, L., Laine, R. F., Jukkala, J., Spahn, C., Krentzel, D., Nehme, E., Lerche, M., Hernández-Pérez, S., Mattila, P. K., Karinou, E., et al.** (2021). Democratising deep learning for microscopy with ZeroCostDL4Mic. *Nature Communications* **12**, 2276\.] -->


<div id="refs" class="references csl-bib-body" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-heinrich2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Heinrich, L. <em>et al.</em> <a href="https://doi.org/10.1038/s41586-021-03977-3">Whole-cell organelle segmentation in volume electron microscopy</a>. <em>Nature</em> <strong>599</strong>, 141–146 (2021).</div>
</div>
<div id="ref-arganda-carreras2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Arganda-Carreras, I. <em>et al.</em> <a href="https://doi.org/10.1093/bioinformatics/btx180">Trainable <span>Weka</span> <span>Segmentation</span>: A machine learning tool for microscopy pixel classification</a>. <em>Bioinformatics</em> <strong>33</strong>, 2424–2426 (2017).</div>
</div>
<div id="ref-arzt2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Arzt, M. <em>et al.</em> <a href="https://www.frontiersin.org/articles/10.3389/fcomp.2022.777728"><span>LABKIT</span>: <span>Labeling</span> and <span>Segmentation</span> <span>Toolkit</span> for <span>Big</span> <span>Image</span> <span>Data</span></a>. <em>Frontiers in Computer Science</em> <strong>4</strong>, (2022).</div>
</div>
<div id="ref-berg2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Berg, S. <em>et al.</em> <a href="https://doi.org/10.1038/s41592-019-0582-9">Ilastik: Interactive machine learning for (bio)image analysis</a>. <em>Nature Methods</em> <strong>16</strong>, 1226–1232 (2019).</div>
</div>
<div id="ref-moen2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Moen, E. <em>et al.</em> <a href="https://doi.org/10.1038/s41592-019-0403-1">Deep learning for cellular image analysis</a>. <em>Nature Methods</em> <strong>16</strong>, 1233–1246 (2019).</div>
</div>
<div id="ref-pylvanainen2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Pylvänäinen, J. W., Gómez-de-Mariscal, E., Henriques, R. &amp; Jacquemet, G. <a href="https://doi.org/10.1016/j.ceb.2023.102271">Live-cell imaging in the deep learning era</a>. <em>Current Opinion in Cell Biology</em> <strong>85</strong>, 102271 (2023).</div>
</div>
<div id="ref-krizhevsky2012" class="csl-entry" role="listitem">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. <a href="https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html"><span>ImageNet</span> <span>Classification</span> with <span>Deep</span> <span>Convolutional</span> <span>Neural</span> <span>Networks</span></a>. in <em>Advances in <span>Neural</span> <span>Information</span> <span>Processing</span> <span>Systems</span></em> vol. 25 (Curran Associates, Inc., 2012).</div>
</div>
<div id="ref-ronneberger2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Ronneberger, O., Fischer, P. &amp; Brox, T. U-<span>Net</span>: <span>Convolutional</span> <span>Networks</span> for <span>Biomedical</span> <span>Image</span> <span>Segmentation</span>. in <em>Medical <span>Image</span> <span>Computing</span> and <span>Computer</span>-<span>Assisted</span> <span>Intervention</span> – <span>MICCAI</span> 2015</em> (eds. Navab, N., Hornegger, J., Wells, W. M. &amp; Frangi, A. F.) 234–241 (Springer International Publishing, Cham, 2015). doi:<a href="https://doi.org/10.1007/978-3-319-24574-4_28">10.1007/978-3-319-24574-4_28</a>.</div>
</div>
<div id="ref-schmidt2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Schmidt, U., Weigert, M., Broaddus, C. &amp; Myers, G. Cell <span>Detection</span> with <span>Star</span>-<span>Convex</span> <span>Polygons</span>. in <em>Medical <span>Image</span> <span>Computing</span> and <span>Computer</span> <span>Assisted</span> <span>Intervention</span> – <span>MICCAI</span> 2018</em> (eds. Frangi, A. F., Schnabel, J. A., Davatzikos, C., Alberola-López, C. &amp; Fichtinger, G.) 265–273 (Springer International Publishing, Cham, 2018). doi:<a href="https://doi.org/10.1007/978-3-030-00934-2_30">10.1007/978-3-030-00934-2_30</a>.</div>
</div>
<div id="ref-stringer2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">Stringer, C., Wang, T., Michaelos, M. &amp; Pachitariu, M. <a href="https://doi.org/10.1038/s41592-020-01018-x">Cellpose: A generalist algorithm for cellular segmentation</a>. <em>Nature Methods</em> <strong>18</strong>, 100–106 (2021).</div>
</div>
<div id="ref-bejarano2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">11. </div><div class="csl-right-inline">Bejarano, L. <em>et al.</em> <a href="https://doi.org/10.1016/j.ccell.2023.12.018">Interrogation of endothelial and mural cells in brain metastasis reveals key immune-regulatory mechanisms</a>. <em>Cancer Cell</em> <strong>42</strong>, 378–395.e10 (2024).</div>
</div>
<div id="ref-fisch2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">12. </div><div class="csl-right-inline">Fisch, D. <em>et al.</em> <a href="https://doi.org/10.1038/s41586-024-07614-7">Molecular definition of the endogenous <span>Toll</span>-like receptor signalling pathways</a>. <em>Nature</em> <strong>631</strong>, 635–644 (2024).</div>
</div>
<div id="ref-fazeli2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">13. </div><div class="csl-right-inline">Fazeli, E. <em>et al.</em> <a href="https://doi.org/10.12688/f1000research.27019.1">Automated cell tracking using <span>StarDist</span> and <span>TrackMate</span></a>. <em>F1000Research</em> <strong>9</strong>, 1279 (2020).</div>
</div>
<div id="ref-follain2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">14. </div><div class="csl-right-inline">Follain, G. <em>et al.</em> Fast label-free live imaging reveals key roles of flow dynamics and <span>CD44</span>-<span>HA</span> interaction in cancer cell arrest on endothelial monolayers. (2024) doi:<a href="https://doi.org/10.1101/2024.09.30.615654">10.1101/2024.09.30.615654</a>.</div>
</div>
<div id="ref-archit2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">15. </div><div class="csl-right-inline">Archit, A. <em>et al.</em> <a href="https://doi.org/10.1038/s41592-024-02580-4">Segment <span>Anything</span> for <span>Microscopy</span></a>. <em>Nature Methods</em> <strong>22</strong>, 579–591 (2025).</div>
</div>
<div id="ref-pachitariu2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">16. </div><div class="csl-right-inline">Pachitariu, M. &amp; Stringer, C. Cellpose 2.0: How to train your own model. <em>Nature Methods</em> 1–8 (2022) doi:<a href="https://doi.org/10.1038/s41592-022-01663-4">10.1038/s41592-022-01663-4</a>.</div>
</div>
<div id="ref-von_chamier2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">17. </div><div class="csl-right-inline">Chamier, L. von <em>et al.</em> <a href="https://doi.org/10.1038/s41467-021-22518-0">Democratising deep learning for microscopy with <span>ZeroCostDL4Mic</span></a>. <em>Nature Communications</em> <strong>12</strong>, 2276 (2021).</div>
</div>
<div id="ref-kochetov2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">18. </div><div class="csl-right-inline">Kochetov, B. <em>et al.</em> <a href="https://doi.org/10.1038/s42003-024-06714-4"><span>UNSEG</span>: Unsupervised segmentation of cells and their nuclei in complex tissue samples</a>. <em>Communications Biology</em> <strong>7</strong>, 1–14 (2024).</div>
</div>
<div id="ref-liu2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">19. </div><div class="csl-right-inline">Liu, B. <em>et al.</em> <a href="https://doi.org/10.1038/s41467-025-57541-y">Self-supervised learning reveals clinically relevant histomorphological patterns for therapeutic strategies in colon cancer</a>. <em>Nature Communications</em> <strong>16</strong>, 2328 (2025).</div>
</div>
<div id="ref-caicedo2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">20. </div><div class="csl-right-inline">Caicedo, J. C., McQuin, C., Goodman, A., Singh, S. &amp; Carpenter, A. E. Weakly <span>Supervised</span> <span>Learning</span> of <span>Single</span>-<span>Cell</span> <span>Feature</span> <span>Embeddings</span>. in <em>2018 <span>IEEE</span>/<span>CVF</span> <span>Conference</span> on <span>Computer</span> <span>Vision</span> and <span>Pattern</span> <span>Recognition</span></em> 9309–9318 (2018). doi:<a href="https://doi.org/10.1109/CVPR.2018.00970">10.1109/CVPR.2018.00970</a>.</div>
</div>
<div id="ref-moshkov2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">21. </div><div class="csl-right-inline">Moshkov, N. <em>et al.</em> <a href="https://doi.org/10.1038/s41467-024-45999-1">Learning representations for image-based profiling of perturbations</a>. <em>Nature Communications</em> <strong>15</strong>, 1594 (2024).</div>
</div>
<div id="ref-schindelin2012" class="csl-entry" role="listitem">
<div class="csl-left-margin">22. </div><div class="csl-right-inline">Schindelin, J. <em>et al.</em> <a href="https://doi.org/10.1038/nmeth.2019">Fiji: An open-source platform for biological-image analysis</a>. <em>Nature Methods</em> <strong>9</strong>, 676–682 (2012).</div>
</div>
<div id="ref-ahlers2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">23. </div><div class="csl-right-inline">Ahlers, J. <em>et al.</em> Napari: A multi-dimensional image viewer for <span>Python</span>. (2023) doi:<a href="https://doi.org/10.5281/zenodo.8115575">10.5281/zenodo.8115575</a>.</div>
</div>
<div id="ref-bankhead2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">24. </div><div class="csl-right-inline">Bankhead, P. <em>et al.</em> <a href="https://doi.org/10.1038/s41598-017-17204-5"><span>QuPath</span>: <span>Open</span> source software for digital pathology image analysis</a>. <em>Scientific Reports</em> <strong>7</strong>, 16878 (2017).</div>
</div>
<div id="ref-lecun1998" class="csl-entry" role="listitem">
<div class="csl-left-margin">25. </div><div class="csl-right-inline">Lecun, Y., Bottou, L., Bengio, Y. &amp; Haffner, P. <a href="https://doi.org/10.1109/5.726791">Gradient-based learning applied to document recognition</a>. <em>Proceedings of the IEEE</em> <strong>86</strong>, 2278–2324 (1998).</div>
</div>
<div id="ref-shorten2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">26. </div><div class="csl-right-inline">Shorten, C. &amp; Khoshgoftaar, T. M. <a href="https://doi.org/10.1186/s40537-019-0197-0">A survey on <span>Image</span> <span>Data</span> <span>Augmentation</span> for <span>Deep</span> <span>Learning</span></a>. <em>Journal of Big Data</em> <strong>6</strong>, 60 (2019).</div>
</div>
<div id="ref-lin2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">27. </div><div class="csl-right-inline">Lin, B. <em>et al.</em> <a href="https://doi.org/10.1038/s41524-022-00767-x">A deep learned nanowire segmentation model using synthetic data augmentation</a>. <em>npj Computational Materials</em> <strong>8</strong>, 1–12 (2022).</div>
</div>
<div id="ref-rangel_dacosta2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">28. </div><div class="csl-right-inline">Rangel DaCosta, L., Sytwu, K., Groschner, C. K. &amp; Scott, M. C. <a href="https://doi.org/10.1038/s41524-024-01336-0">A robust synthetic data generation framework for machine learning in high-resolution transmission electron microscopy (<span>HRTEM</span>)</a>. <em>npj Computational Materials</em> <strong>10</strong>, 1–11 (2024).</div>
</div>
<div id="ref-laine2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">29. </div><div class="csl-right-inline">Laine, R. F., Arganda-Carreras, I., Henriques, R. &amp; Jacquemet, G. <a href="https://doi.org/10.1038/s41592-021-01284-3">Avoiding a replication crisis in deep-learning-based bioimage analysis</a>. <em>Nature methods</em> <strong>18</strong>, 1136–1144 (2021).</div>
</div>
<div id="ref-ilievski2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">30. </div><div class="csl-right-inline">Ilievski, I., Akhtar, T., Feng, J. &amp; Shoemaker, C. <a href="https://doi.org/10.1609/aaai.v31i1.10647">Efficient hyperparameter optimization for deep learning algorithms using deterministic RBF surrogates</a>. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> <strong>31</strong>, (2017).</div>
</div>
<div id="ref-alibrahim2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">31. </div><div class="csl-right-inline">Alibrahim, H. &amp; Ludwig, S. A. Hyperparameter optimization: Comparing genetic algorithm against grid search and bayesian optimization. in <em>2021 IEEE congress on evolutionary computation (CEC)</em> 1551–1559 (2021). doi:<a href="https://doi.org/10.1109/CEC45853.2021.9504761">10.1109/CEC45853.2021.9504761</a>.</div>
</div>
<div id="ref-li2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">32. </div><div class="csl-right-inline">Li, Y. &amp; Shen, L. <a href="https://doi.org/10.1109/ACCESS.2018.2808938"><span class="nocase">cC</span>-<span>GAN</span>: <span>A</span> <span>Robust</span> <span>Transfer</span>-<span>Learning</span> <span>Framework</span> for <span>HEp</span>-2 <span>Specimen</span> <span>Image</span> <span>Segmentation</span></a>. <em>IEEE Access</em> <strong>6</strong>, 14048–14058 (2018).</div>
</div>
<div id="ref-morid2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">33. </div><div class="csl-right-inline">Morid, M. A., Borjali, A. &amp; Del Fiol, G. <a href="https://doi.org/10.1016/j.compbiomed.2020.104115">A scoping review of transfer learning research on medical image analysis using <span>ImageNet</span></a>. <em>Computers in Biology and Medicine</em> <strong>128</strong>, 104115 (2021).</div>
</div>
<div id="ref-chen2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">34. </div><div class="csl-right-inline">Chen, J. <em>et al.</em> The <span>Allen</span> <span>Cell</span> and <span>Structure</span> <span>Segmenter</span>: A new open source toolkit for segmenting <span>3D</span> intracellular structures in fluorescence microscopy images. (2020) doi:<a href="https://doi.org/10.1101/491035">10.1101/491035</a>.</div>
</div>
<div id="ref-conrad2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">35. </div><div class="csl-right-inline">Conrad, R. &amp; Narayan, K. <a href="https://doi.org/10.1016/j.cels.2022.12.006">Instance segmentation of mitochondria in electron microscopy images with a generalist deep learning model trained on a diverse dataset</a>. <em>Cell Systems</em> <strong>14</strong>, 58–71.e5 (2023).</div>
</div>
<div id="ref-hidalgo-cenalmor2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">36. </div><div class="csl-right-inline">Hidalgo-Cenalmor, I. <em>et al.</em> <span>DL4MicEverywhere</span>: Deep learning for microscopy made flexible, shareable and reproducible. <em>Nature Methods</em> (2024) doi:<a href="https://doi.org/10.1038/s41592-024-02295-6">10.1038/s41592-024-02295-6</a>.</div>
</div>
<div id="ref-russell2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">37. </div><div class="csl-right-inline">Russell, C. T. <em>et al.</em> Bia-binder: <span>A</span> web-native cloud compute service for the bioimage analysis community. (2024) doi:<a href="https://doi.org/10.48550/arXiv.2411.12662">10.48550/arXiv.2411.12662</a>.</div>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./8-existing-tools.html" class="pagination-link" aria-label="Finding and Using Existing Tools">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Finding and Using Existing Tools</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./10-output-quality.html" class="pagination-link" aria-label="Output Quality">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Output Quality</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>