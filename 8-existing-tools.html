<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Beth Cimini">
<meta name="author" content="Erin Weisbart">

<title>8&nbsp; Chapter 8: How do you select and find a tool? – AI in Microscopy: A BioImaging Guide</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./9-train-models.html" rel="next">
<link href="./7-smart-microscopy.html" rel="prev">
<link href="./settings/favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b651517ce65839d647a86e2780455cfb.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-2641b481724464e61c86985d8c912b6f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-4c6d65c679321d81af7ec8b61b1d5a24.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-2641b481724464e61c86985d8c912b6f.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-Y29EKZ8LWD"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y29EKZ8LWD', { 'anonymize_ip': true});
</script>


</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./8-existing-tools.html">Image Analysis</a></li><li class="breadcrumb-item"><a href="./8-existing-tools.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Chapter 8: How do you select and find a tool?</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">AI in Microscopy: A BioImaging Guide</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/aicjanelia/BioImagingAI" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./AI-in-Microscopy--A-BioImaging-Guide.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Getting Started with AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">AI Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-llms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Foundations of Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Architectures and Loss Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Image Acquisition</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-training-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Collecting Training Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-image-restoration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Extending Your Hardware With AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-smart-microscopy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Adding AI to Your Hardware</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Image Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-existing-tools.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Chapter 8: How do you select and find a tool?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-train-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">How to Train and Use Deep Learning Models in Microscopy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-output-quality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Output Quality</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-outlook.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Outlook</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Glossary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">8.1</span> Introduction</a></li>
  <li><a href="#assessing-your-requirements-for-a-tool-in-a-perfect-world" id="toc-assessing-your-requirements-for-a-tool-in-a-perfect-world" class="nav-link" data-scroll-target="#assessing-your-requirements-for-a-tool-in-a-perfect-world"><span class="header-section-number">8.2</span> Assessing your requirements for a tool (in a perfect world)</a>
  <ul class="collapse">
  <li><a href="#questions-about-your-overall-workflow" id="toc-questions-about-your-overall-workflow" class="nav-link" data-scroll-target="#questions-about-your-overall-workflow"><span class="header-section-number">8.2.1</span> Questions about your overall workflow:</a></li>
  <li><a href="#questions-about-your-resources-time-expertise-and-compute" id="toc-questions-about-your-resources-time-expertise-and-compute" class="nav-link" data-scroll-target="#questions-about-your-resources-time-expertise-and-compute"><span class="header-section-number">8.2.2</span> Questions about your resources: time, expertise, and compute:</a></li>
  <li><a href="#questions-about-your-input-data" id="toc-questions-about-your-input-data" class="nav-link" data-scroll-target="#questions-about-your-input-data"><span class="header-section-number">8.2.3</span> Questions about your input data:</a></li>
  </ul></li>
  <li><a href="#assessing-your-requirements-for-a-tool-in-the-real-world" id="toc-assessing-your-requirements-for-a-tool-in-the-real-world" class="nav-link" data-scroll-target="#assessing-your-requirements-for-a-tool-in-the-real-world"><span class="header-section-number">8.3</span> Assessing your requirements for a tool (in the real world)</a></li>
  <li><a href="#how-do-i-decide-when-my-workflow-is-good-enough" id="toc-how-do-i-decide-when-my-workflow-is-good-enough" class="nav-link" data-scroll-target="#how-do-i-decide-when-my-workflow-is-good-enough"><span class="header-section-number">8.4</span> How do I decide when my workflow is “good enough”?</a></li>
  <li><a href="#choosing-the-kind-of-tool-to-use" id="toc-choosing-the-kind-of-tool-to-use" class="nav-link" data-scroll-target="#choosing-the-kind-of-tool-to-use"><span class="header-section-number">8.5</span> Choosing the kind of tool to use</a></li>
  <li><a href="#finding-tools-and-models" id="toc-finding-tools-and-models" class="nav-link" data-scroll-target="#finding-tools-and-models"><span class="header-section-number">8.6</span> Finding tools and models</a>
  <ul class="collapse">
  <li><a href="#where-to-look-for-tools-and-models" id="toc-where-to-look-for-tools-and-models" class="nav-link" data-scroll-target="#where-to-look-for-tools-and-models"><span class="header-section-number">8.6.1</span> Where to look for tools and models</a></li>
  </ul></li>
  <li><a href="#case-studies" id="toc-case-studies" class="nav-link" data-scroll-target="#case-studies"><span class="header-section-number">8.7</span> Case Studies</a>
  <ul class="collapse">
  <li><a href="#case-study-1" id="toc-case-study-1" class="nav-link" data-scroll-target="#case-study-1"><span class="header-section-number">8.7.1</span> Case Study 1:</a></li>
  <li><a href="#case-study-2" id="toc-case-study-2" class="nav-link" data-scroll-target="#case-study-2"><span class="header-section-number">8.7.2</span> Case Study 2:</a></li>
  </ul></li>
  <li><a href="#conclusion-2" id="toc-conclusion-2" class="nav-link" data-scroll-target="#conclusion-2"><span class="header-section-number">8.8</span> Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./8-existing-tools.html">Image Analysis</a></li><li class="breadcrumb-item"><a href="./8-existing-tools.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Chapter 8: How do you select and find a tool?</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-8" class="quarto-section-identifier"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Chapter 8: How do you select and find a tool?</span></span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Beth Cimini <a href="https://orcid.org/0000-0001-9640-9318" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Imaging Platform, Broad Institute
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Erin Weisbart <a href="https://orcid.org/0000-0002-6437-2458" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Imaging Platform, Broad Institute
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">8.1</span> Introduction</h2>
<p>We are now transitioning away from a discussion of hardware and into a discussion of software. And, more specifically, in this chapter we focus on image analysis, particularly the human thought processes and decisions needed for selecting what software to use and figuring out how to appropriately/best use it. Amazing technological developments have occured since the invention of microscopy almost half a millenium ago; Our current relationship to biological microscopy is profoundly shaped by the development of both quantitative microscopy and artificial intelligence in the mid-twentieth century. However, underlying the actual use of any technological development is a human (in our case, likely a biologist) selecting a specific technology to solve a specific problem.</p>
<p>In this chapter, we answer the questions of how do you find new-to-you AI models and how do you assess whether a new-to-you model or tool will meet your needs? In this chapter, we first help you assess your needs. We then introduce a way of categorizing tools and help you use your needs assessment to select the right tool category. We suggest several places where you can find AI models for bioimage analysis and describe how to assess how well tools in those locations meet your needs. Finally, we show a couple of case studies that fulfill different requirements.</p>
</section>
<section id="assessing-your-requirements-for-a-tool-in-a-perfect-world" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="assessing-your-requirements-for-a-tool-in-a-perfect-world"><span class="header-section-number">8.2</span> Assessing your requirements for a tool (in a perfect world)</h2>
<p>Before finding and selecting an AI tool, you first need to decide on the type of tool you need. Your tool selection will be influenced by a number of factors and you should first honestly answer the questions that follow.</p>
<div id="Figure1" class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Figure 1: Workflows contain modular components
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="8-existing-tools_files/workflows.png" title="Workflows contain modular components" class="img-fluid" alt="Workflows contain modular components"> <strong>Figure 1.</strong> When thinking of your workflow and how you might incorporate an AI tool, consider that a workflow (left of image) typically consists of multiple, modular subcomponents (right of image). You may be able to replace your whole workflow with an appropriate AI tool or you may be able to replace only a subcomponent while still keeping many of the original workflow subcomponents the same. Replacing a single subcomponent with an AI tool may force you to change surrounding subcomponents in your workflow - either with new tools or new configurations of existing tools - as the AI tool may require different inputs and produce different outputs.</p>
<p>Though you may have built a workflow across multiple softwares, it is worth considering whether a single software (or a reduced number of softwares) can run your workflow and part of this consideration includes assessing how well your desired AI tool can run from within other software. Some examples of softwares that can be used to run other softwares include CellProfiler<span class="citation" data-cites="Stirling2021"><sup><a href="references.html#ref-Stirling2021" role="doc-biblioref">1</a></sup></span>, Fiji<span class="citation" data-cites="Schindelin2012"><sup><a href="references.html#ref-Schindelin2012" role="doc-biblioref">2</a></sup></span>, Bilayers<span class="citation" data-cites="Shah"><sup><a href="references.html#ref-Shah" role="doc-biblioref">3</a></sup></span>, ZeroCostDL4Mic<span class="citation" data-cites="von_chamier2021"><sup><a href="references.html#ref-von_chamier2021" role="doc-biblioref">4</a></sup></span> or DL4MicEverywhere<span class="citation" data-cites="Hidalgo-Cenalmor2024"><sup><a href="references.html#ref-Hidalgo-Cenalmor2024" role="doc-biblioref">5</a></sup></span>, and Jupyter notebooks<span class="citation" data-cites="jupyter"><sup><a href="references.html#ref-jupyter" role="doc-biblioref">6</a></sup></span>.</p>
<p>Figure adapted from<span class="citation" data-cites="Haase2024"><sup><a href="references.html#ref-Haase2024" role="doc-biblioref">7</a></sup></span>.</p>
</div>
</div>
<section id="questions-about-your-overall-workflow" class="level3" data-number="8.2.1">
<h3 data-number="8.2.1" class="anchored" data-anchor-id="questions-about-your-overall-workflow"><span class="header-section-number">8.2.1</span> Questions about your overall workflow:</h3>
<ul>
<li><strong>What is your desired output?</strong><br>
You must be able to concisely and specifically state what you would like output by your workflow. Do you need to segment objects? If so, do you need <a href="./glossary.html#semantic-segmentation">semantic</a>, <a href="./glossary.html#instance-segmentation">instance</a>, or <a href="./glossary.html#panoptic-segmentation">panoptic segmentation</a> (<a href="#Box1">Box 1</a>)? (see <a href="4-architectures.html" class="quarto-xref"><span>Chapter 4</span></a> and <a href="9-train-models.html" class="quarto-xref"><span>Chapter 9</span></a> for more information) Do you need measurements and if so should they be made on individual objects or on images?<br>
</li>
<li><strong>How will you assess your outputs?</strong><br>
Will your assessment be qualitative or quantitative? Do you have ground truth? Do you have the expertise to assess output quality? (see <a href="10-output-quality.html" class="quarto-xref"><span>Chapter 10</span></a> for more information)</li>
<li><strong>What is your desired “quality” of your outputs?</strong><br>
Are you expecting outputs that approach <a href="./glossary.html#ground-truth">ground truth</a> or will “quick and dirty” be enough for you?</li>
<li><strong>What does your ideal workflow look like?</strong><br>
Do you need all of the outputs to be made in a single software or are you comfortable using multiple softwares? If your images require preprocessing, does that need to happen in the same step? The same software?</li>
</ul>
<div id="groundtruth" class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Ground Truth
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Ground truth</strong> can mean many things. The Broad Bioimage Benchmark Collection (BBBC)<span class="citation" data-cites="Ljosa2012"><sup><a href="references.html#ref-Ljosa2012" role="doc-biblioref">8</a></sup></span> provides an nice example of this. In the BBBC, all datasets include ground truth, but that ground truth can be one or many of counts, foreground/background, outlines of objects, biological labels, location, or bounding boxes.</p>
</div>
</div>
</section>
<section id="questions-about-your-resources-time-expertise-and-compute" class="level3" data-number="8.2.2">
<h3 data-number="8.2.2" class="anchored" data-anchor-id="questions-about-your-resources-time-expertise-and-compute"><span class="header-section-number">8.2.2</span> Questions about your resources: time, expertise, and compute:</h3>
<ul>
<li><strong>How much time are you able to put into the task?</strong><br>
An honest assessment at the beginning of any project about the time you are willing to invest is critical. If you are in a rush, you’ll probably want to select a method that is already familiar or best matches your existing competencies.<br>
</li>
<li><strong>What is your priority?</strong><br>
Determining what your priority is goes hand in hand with assessing the amount of time you can put into your task. Perhaps you’re in a time crunch and speed is the most important consideration for you. Perhaps you have a specific <a href="./glossary.html#quality-control-metric">quality control (QC) metric</a> and you need the tool that will give you the outputs that optimize this metric. Perhaps you’ve always wanted to gain experience with a particular class of tool, so you want to figure out if this is the right use case for trying it.<br>
</li>
<li><strong>What is your level of computational domain expertise and comfort?</strong><br>
If you don’t have a high level of computational comfort, do you have the time and motivation to expand your skillset by building new skills outside of your comfort zone? Do you have a computational collaborator and how much time, in either teaching you or in handling the data themselves, are they able to contribute?&nbsp; Computation domain expertise has two critical, but separable, components. The first is the ability to understand what you are doing thoroughly enough that you can design, analyze, and interpret your experiment. The other component is the ability to comfortably interact with software as a computationalist might (e.g.&nbsp;are you comfortable working from the command line (which is text only) or would you prefer a GUI (graphical user interface), where you can point and click).</li>
<li><strong>How do you like to learn things?</strong><br>
Are you a person who likes to jump in and just turn all the knobs and buttons to see what they do? Do you love to cozy up with a good written manual at the end of the day? Do video walkthroughs bring you joy or frustration? How important is it to you that you be able to ask the tool creator questions directly? The type of documentation and instructional materials as well as their depth can vary greatly between tools.<br>
</li>
<li><strong>What is your level of biological domain expertise?</strong><br>
Are you confident that you fully understand the task so you can assess how well your AI model is performing? Do you have a biologist collaborator and how much time are they able to contribute to designing the experiment and/or analyzing data? Do you understand what controls you will need and/or corrections (such as single-color controls and/or measured flat-field corrections) you will need to make to be able to interpret your outputs?<br>
</li>
<li><strong>What access do you have to compute resources?</strong><br>
Do you need to be able to run everything on your laptop or do you have ready access to a high performance cluster or scalable cloud compute? Do you have access to GPUs? Different tools have different compute requirements, especially as you scale them, and those requirements don’t always scale linearly with data size.</li>
</ul>
</section>
<section id="questions-about-your-input-data" class="level3" data-number="8.2.3">
<h3 data-number="8.2.3" class="anchored" data-anchor-id="questions-about-your-input-data"><span class="header-section-number">8.2.3</span> Questions about your input data:</h3>
<ul>
<li><strong>How difficult are your images?</strong><br>
Perfect, clean data is the ideal input to any analysis. But that’s not always the regime we’re in. There are many sources of “difficult”. A couple examples and corresponding questions are below.
<ul>
<li>Do your images have debris or other technical artifacts such as out-of-focus regions? Do those artifacts need to be identified and removed? If identified, should the whole image be dropped or do you need to keep non-artifact areas of the image?<br>
</li>
<li>Do you have <a href="./glossary.html#metadata">metadata</a> that needs to be associated with your images? Is that metadata organized? How is that metadata organized (e.g.&nbsp;in the file name, in a .csv file, in a picture of a handwritten lab notebook) and does it play nicely with the tool you would like to use?<br>
</li>
<li>Are your images in a friendly format? Are they in a proprietary file format? Are they in a file format that allows access to chunks?<br>
</li>
</ul></li>
<li><strong>How big is your data?</strong><br>
The larger data is, the harder it can be to work with. If your data is large, what dimensionality is big? e.g.&nbsp;many images, large individual file sizes, many channels<br>
</li>
<li><strong>Do you need outputs in multiple dimensions?</strong><br>
If you have z-planes, do you need 3D objects or can each plane be handled separately? If you have multiple timepoints, do you need objects tracked across timepoints or can each timepoint be handled separately?<br>
</li>
<li><strong>Do your images require preprocessing?</strong><br>
There are many different reasons that images might require preprocessing and some of those reasons may be a way to overcome concerns/technical challenges raised in other questions above. Some examples of preprocessing include stitching or cropping of images, denoising, background subtraction, or flat field correction.</li>
</ul>
<div id="Box1" class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Box 1: Segmentation methods
</div>
</div>
<div class="callout-body-container callout-body">
<p>In computer vision, there are several discrete kinds of segmentation, although in the field of bioimaging we often refer to all kinds of segmentation under the single blanket term of “segmentation”. <strong>Semantic segmentation</strong> divides an image into classes. Ilastik is an example of a popular image analysis software that performs semantic segmentation. <strong>Instance segmentation</strong> detects individual, specific objects within an image. CellProfiler is an example of a popular image analysis software that is most commonly used for instance segmentation. <strong>Panoptic segmentation</strong> is a combination of semantic segmentation and instance segmentation that separates an image into regions while also detecting individual object instances within those regions. Deep learning can be used for semantic, instance, or panoptic segmentation. Most classic image analysis methods are built around instance segmentation for cell-based images, though semantic segmentation is not uncommon for tissue/histology images.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-existing-tools_files/segmentation_cellprofiler_ilastik.png" title="Segmentation comparison of CellProfiler and Ilastik" class="img-fluid figure-img"></p>
<figcaption>Segmentation comparison of CellProfiler and Ilastik</figcaption>
</figure>
</div>
<p>Example of different types of segmentations produced between CellProfiler and Ilastik. A) Input image of Drosophila Kc167 cells, provided in the Example pipeline packaged with CellProfiler. B) CellProfiler uses classic image processing to create instance segmentations. The individual objects identified are shown as individually colored masks in the upper panel. The lower image is the input image overlaid with nuclei objects outlined in green and cell objects outlined in pink.<br>
C) Ilastik uses pixel-based machine learning to create semantic segmentation. The image is the input image with green shaded areas in the class “Cells” and red shaded areas in the class “Background”.</p>
</div>
</div>
</section>
</section>
<section id="assessing-your-requirements-for-a-tool-in-the-real-world" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="assessing-your-requirements-for-a-tool-in-the-real-world"><span class="header-section-number">8.3</span> Assessing your requirements for a tool (in the real world)</h2>
<p>In several previous surveys we have run, scientists have reported that they generally don’t actually follow any thoughtful, rigorous process for deciding what tool they will use for a particular process. Instead, they sit down with a thing they know how to use and hope they can make it follow their use case<span class="citation" data-cites="Jamali2021"><sup><a href="references.html#ref-Jamali2021" role="doc-biblioref">9</a></sup></span> <span class="citation" data-cites="Sivagurunathan2023"><sup><a href="references.html#ref-Sivagurunathan2023" role="doc-biblioref">10</a></sup></span>. If they don’t already have familiarity with a tool that is appropriate for their task, they ask their labmates or maybe do a quick internet search. So know that if you found the section above overwhelming, you are not alone.</p>
<p>If you already have familiarity with a tool and it fits your newest use case (especially if your newest use case isn’t <em>very</em> different from a use case where you’ve previously successfully used it) go ahead and stick with that tool. “If it ain’t broke, don’t fix it” is a common adage for a reason. But even for existing use cases, it’s worth actively contemplating your priorities every-so-often - maybe your GPU cluster is filling up these days and is harder to access, or after a new operating system upgrade a tool that was working great is now randomly crashing, or maybe there simply is something new (or new-to-you) available that will make your life easier. In our experience, the best way to be sure you’re best serving your scientific needs is simply to step back and consider your big picture goals and whether your tool choices are serving those goals with some regularity.</p>
<p>Once you have conducted a thorough needs assessment, it should <em>theoretically</em> be simple to determine if a given tool or model is the right one for you. Unfortunately, no tool is likely to perfectly fit every single need that you have! You will likely need to break your needs into “must haves” and “nice-to-haves”, and rank tools accordingly. Make sure you consider not just how well the tool will perform your specific task, but also how well it will fit in your workflow (<a href="#Figure1">Figure 1</a>)</p>
</section>
<section id="how-do-i-decide-when-my-workflow-is-good-enough" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="how-do-i-decide-when-my-workflow-is-good-enough"><span class="header-section-number">8.4</span> How do I decide when my workflow is “good enough”?</h2>
<p>Deciding when a workflow is “good enough” can be really hard! We generally factor in two major considerations when deciding what quality level we are aiming to achieve: Is this a pilot or final data? and How big is the <a href="./glossary.html#effect-size">effect size</a> we’re looking for?</p>
<p>We generally hold pilot data to a lower quality standard than final or production data. We are looking for a proof of concept that our assay/method can work but understand that there are likely to be refinements and improvements at many points in the workflow that will increase our final quality. Additionally, during the piloting phase, there are likely to be changes introduced that change the inputs to our workflow so time spent beyond a certain point of workflow refinement is lost with the retuning/adjustments required for the next pilot batch.</p>
<p>Phenotypic effect size has a large impact on necessary quality level. Simple, easily distinguished phenotypes (e.g.&nbsp;GFP is either in the nucleus or cytoplasm) may reach statistical significance with many fewer individual measurements and using much simple/cruder regions of interest (e.g.&nbsp;cell bounding boxes instead of careful cell margin segmentations) than more subtle phenotypes.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Research progress is rarely linear
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="8-existing-tools_files/research_progress.png" title="Research progress is rarely linear" class="img-fluid" alt="Research progress is rarely linear"> <strong>Figure 2.</strong> As scientists who care about the quality of our work, it’s tempting to always try to maximize the accuracy of our analysis. But it’s worth considering that “accuracy” vs “time spent” is often an asymptotic curve - you can keep pushing the accuracy higher and higher, but the gains may be marginal past a particular point. Time and effort spent on a project can have very different relationships with the output quality. Though one might hope to be in a regime where the amount of time spent is linearly correlated with the quality of a project’s output (blue line) so that it is clear exactly how much time must be put in to get a desired product, research rarely proceeds in such a fashion. Instead, it progresses in unpredictable fits and starts (green line) and/or it is in a state of diminishing returns (red line) where the closer you get to “perfect”, the longer it takes to improve the quality.</p>
</div>
</div>
<p>In<span class="citation" data-cites="Cimini2019"><sup><a href="references.html#ref-Cimini2019" role="doc-biblioref">11</a></sup></span> we detail and provide examples for a few key heuristics that we think about when deciding if a workflow is “good enough”. Those heuristics are:</p>
<ol type="1">
<li>How close to accurate do I need to be to assess the % or fold change I expect to see in this experiment?<br>
</li>
<li>How close to 100% accurate is it possible to get with my current workflow?<br>
</li>
<li>How important is this aspect of my experiment to my overall hypothesis?<br>
</li>
<li>How wrong is my current pipeline output?<br>
</li>
<li>What else could I do (e.g.&nbsp;develop a different approach/workflow, learn a new tool, generate new input data) in the time it will take me to make my pipeline maximally accurate?</li>
</ol>
</section>
<section id="choosing-the-kind-of-tool-to-use" class="level2" data-number="8.5">
<h2 data-number="8.5" class="anchored" data-anchor-id="choosing-the-kind-of-tool-to-use"><span class="header-section-number">8.5</span> Choosing the kind of tool to use</h2>
<p>Let’s consider the use case of instance segmentation (see <a href="#Box1">Box 1</a>), which is often a critical step in image analysis workflows. We have defined 6 categories of segmentation methods, each of which has its valid use cases. Ordered by increasing levels of computation comfort required they are:</p>
<ul>
<li><a href="./glossary.html#manual-annotation">Manual annotation</a><br>
</li>
<li>Classical image processing<br>
</li>
<li>Pixel based machine learning<br>
</li>
<li>Pretrained deep learning models<br>
</li>
<li>Finetuned deep learning models<br>
</li>
<li>From-scratch deep learning models</li>
</ul>
<p>We will not cover from-scratch deep learning models in this chapter. They are covered in <a href="9-train-models.html" class="quarto-xref"><span>Chapter 9</span></a>.</p>
<p>It is worth noting that many of the tool suggestions below are not confined to a single “class”. Many tools listed in the non-deep learning categories allow you to run pre-trained deep learning models inside them (e.g.&nbsp;CellProfiler has a RunCellPose module, ImageJ and QuPath also contain ability to run <a href="./glossary.html#pixel-classifiers">pixel classifiers</a>). All deep learning models <em>can</em> be re-tuned with some effort (they may just not have a friendly interface for doing so).</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Method category</th>
<th style="text-align: left;">Suggested light microscopy tools</th>
<th style="text-align: left;">Time required</th>
<th style="text-align: left;">Best if your priorities include</th>
<th style="text-align: left;">Biological expertise level required</th>
<th style="text-align: left;">Ability to handle <em>reasonable</em> technical noise and variability</th>
<th style="text-align: left;">Largest drawback</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Manual Annotation</strong>: Hand-drawing in boundaries of all objects</td>
<td style="text-align: left;">ImageJ<span class="citation" data-cites="Schindelin2012"><sup><a href="references.html#ref-Schindelin2012" role="doc-biblioref">2</a></sup></span> <span class="citation" data-cites="Schindelin2015"><sup><a href="references.html#ref-Schindelin2015" role="doc-biblioref">12</a></sup></span>, Napari<span class="citation" data-cites="napari"><sup><a href="references.html#ref-napari" role="doc-biblioref">13</a></sup></span>, see also <a href="https://forum.image.sc/t/comparison-of-some-tools-for-3d-dense-ground-truth-annotations/38918">this forum thread</a> for 3D annotation</td>
<td style="text-align: left;">Highest</td>
<td style="text-align: left;">High accuracy/ outputs approaching ground truth</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Inter- and even intra-operator manual annotations (e.g.&nbsp;after time or image rotation) variability. Irreproducibility. Can not be automated.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Classical Image Processing</strong>: Combining mathematically well-defined operations such as thresholding and watershedding to create objects</td>
<td style="text-align: left;">CellProfiler<span class="citation" data-cites="Stirling2021"><sup><a href="references.html#ref-Stirling2021" role="doc-biblioref">1</a></sup></span>, QuPath<span class="citation" data-cites="Bankhead2017"><sup><a href="references.html#ref-Bankhead2017" role="doc-biblioref">14</a></sup></span>, Icy<span class="citation" data-cites="de-Chaumont2012"><sup><a href="references.html#ref-de-Chaumont2012" role="doc-biblioref">15</a></sup></span></td>
<td style="text-align: left;">Medium</td>
<td style="text-align: left;">Needing to have a good sense of what the failure cases will be (e.g.&nbsp;when cells are too big, too small, too crowded, etc); needing to run in minimal computational footprints</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Initially low, can be improved with experience</td>
<td style="text-align: left;">Most performant on very bright objects on dark backgrounds</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Pixel based machine learning</strong>: Using properties of individual pixels to classify each pixel into belonging to a given set of “classes” (e.g.&nbsp;“true signal”, “autofluorescence”, and “background”)</td>
<td style="text-align: left;">Ilastik<span class="citation" data-cites="Berg2019"><sup><a href="references.html#ref-Berg2019" role="doc-biblioref">16</a></sup></span>, Labkit<span class="citation" data-cites="Arzt2022"><sup><a href="references.html#ref-Arzt2022" role="doc-biblioref">17</a></sup></span></td>
<td style="text-align: left;">Low to Medium</td>
<td style="text-align: left;">Identifying well-separated objects that are not bright-on-dark or are not the <em>only</em> bright-on-dark objects present</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">High if properly represented in the training set</td>
<td style="text-align: left;">Must be followed by Classical Image Processing if individual object masks are required</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Pretrained deep learning models</strong>: Using a set of learned transformations (created by the developer) to define the positions and boundaries of objects</td>
<td style="text-align: left;">Stardist<span class="citation" data-cites="Schmidt2018"><sup><a href="references.html#ref-Schmidt2018" role="doc-biblioref">18</a></sup></span>, Instanseg<span class="citation" data-cites="Goldsborough2024"><sup><a href="references.html#ref-Goldsborough2024" role="doc-biblioref">19</a></sup></span></td>
<td style="text-align: left;">Low</td>
<td style="text-align: left;">Maximum likelihood for good quality with minimal human time required</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">High if properly represented in the training set (which you may not know)</td>
<td style="text-align: left;">Unlikely to work if data similar to your data is not present in the training set. Can be hard to package into workflows.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Fine tuned deep learning models</strong>: Updating or adding to a pretrained deep learning model with your data</td>
<td style="text-align: left;">Cellpose<span class="citation" data-cites="Pachitariu2022"><sup><a href="references.html#ref-Pachitariu2022" role="doc-biblioref">20</a></sup></span>, µSAM<span class="citation" data-cites="Archit2025"><sup><a href="references.html#ref-Archit2025" role="doc-biblioref">21</a></sup></span></td>
<td style="text-align: left;">Medium</td>
<td style="text-align: left;">Maximum likelihood for very good quality with less human time than manual annotation</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">High if properly represented in the training set</td>
<td style="text-align: left;">Most computationally intensive. Many available models are not provided in a non-computationalist friendly format. Can be hard to package into workflows.</td>
</tr>
</tbody>
</table>
<p>You may have noticed that all methods are listed as requiring a high level of biological expertise. It can be tempting for researchers without deep biological expertise to assume that they could just use a pretrained or fine tuned deep learning model and call it a day. However, we would caution that, while they may require less of a biologist’s time to tune at a first pass, it is still essential to involve a biologist with subject matter expertise to verify that the models are producing results that match known biology.</p>
</section>
<section id="finding-tools-and-models" class="level2" data-number="8.6">
<h2 data-number="8.6" class="anchored" data-anchor-id="finding-tools-and-models"><span class="header-section-number">8.6</span> Finding tools and models</h2>
<section id="where-to-look-for-tools-and-models" class="level3" data-number="8.6.1">
<h3 data-number="8.6.1" class="anchored" data-anchor-id="where-to-look-for-tools-and-models"><span class="header-section-number">8.6.1</span> Where to look for tools and models</h3>
<p>There are many places that one can look to find new (or new-to-them) tools and models. A few suggestions are as follows:</p>
<ul>
<li>BioImage Model Zoo<span class="citation" data-cites="Ouyang2022"><sup><a href="references.html#ref-Ouyang2022" role="doc-biblioref">22</a></sup></span> is a community-driven AI model repository that offers a variety of pretrained AI models. All models are described by a common Model Resource Description File Specification which simplifies using models in the BioImage Model Zoo with partner software such as ilastik<span class="citation" data-cites="Berg2019"><sup><a href="references.html#ref-Berg2019" role="doc-biblioref">16</a></sup></span>, ImJoy<span class="citation" data-cites="Ouyang2019"><sup><a href="references.html#ref-Ouyang2019" role="doc-biblioref">23</a></sup></span>, Fiji<span class="citation" data-cites="Schindelin2012"><sup><a href="references.html#ref-Schindelin2012" role="doc-biblioref">2</a></sup></span>, deepImageJ<span class="citation" data-cites="Gomez-de-Mariscal2021"><sup><a href="references.html#ref-Gomez-de-Mariscal2021" role="doc-biblioref">24</a></sup></span> and ZeroCostDL4Mic<span class="citation" data-cites="von_chamier2021"><sup><a href="references.html#ref-von_chamier2021" role="doc-biblioref">4</a></sup></span>.</li>
<li>Bioimage Informatics Index (Biii)<span class="citation" data-cites="Zhang2023"><sup><a href="references.html#ref-Zhang2023" role="doc-biblioref">25</a></sup></span> is a community-curated index that includes software and helps researchers find tools whether their search is problem-based (e.g.&nbsp;“find nuclei in cells”), method-based (e.g.&nbsp;“active contour-based segmentation”), or tool-based (e.g.&nbsp;“CellProfiler”).<br>
</li>
<li>The Scientific Community Image Forum (<a href="http://forum.image.sc">forum.image.sc</a>)<span class="citation" data-cites="Rueden2019"><sup><a href="references.html#ref-Rueden2019" role="doc-biblioref">26</a></sup></span> is a platform where users can pose bioimage software-related questions and receive feedback from a broad user community and maintainers from &gt;75 partner softwares.<br>
</li>
<li>Academic literature can include biology-focused research where users describe applications or computer science research where the tools and models themselves are more likely to be described. Keeping an eye on preprint servers like arXiv (particularly in Computing Research Repository or Quantitative Biology sections) and bioRxiv can help you find the most cutting-edge developments.</li>
</ul>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Bioimage analysts
</div>
</div>
<div class="callout-body-container callout-body">
<p>There is a whole discipline of people who specialize in figuring out workflows for analyzing microscopy images - their official titles often vary, but many think of themselves as <em>bioimage analysts</em>. If you want to find a local bioimage analyst, or even become one, you can check out the <a href="https://www.globias.org/">Global Bioimage Analysts’ Society (GloBIAS)</a>, which holds events, coordinates trainings (including free help sessions), and hosts a database of bioimage analysts and bioimage analysis trainers.</p>
</div>
</div>
</section>
</section>
<section id="case-studies" class="level2" data-number="8.7">
<h2 data-number="8.7" class="anchored" data-anchor-id="case-studies"><span class="header-section-number">8.7</span> Case Studies</h2>
<section id="case-study-1" class="level3" data-number="8.7.1">
<h3 data-number="8.7.1" class="anchored" data-anchor-id="case-study-1"><span class="header-section-number">8.7.1</span> Case Study 1:</h3>
<section id="introduction-1" class="level4" data-number="8.7.1.1">
<h4 data-number="8.7.1.1" class="anchored" data-anchor-id="introduction-1"><span class="header-section-number">8.7.1.1</span> Introduction</h4>
<p>Elena is an image analyst who uses high content microscopy as a method for exploring drug mechanism of action. She uses an automated microscope to acquire 9 fields of view in each well of a 384-well plate and just acquired a batch of 4 plates worth of images. She often performs the same assay so she has a series of pipelines that she has nicely tuned for her usual experimental parameters. Elena has a new collaborator that would like to perform a similar screen in a different cell line as the collaborator is interested in a different specific area of biology. Because this is the first time that Elena has performed this assay in a new cell line, she needs to carefully tune her existing pipelines so that she has high-quality segmentation of Nuclei and Cell objects.</p>
</section>
<section id="workflow" class="level4" data-number="8.7.1.2">
<h4 data-number="8.7.1.2" class="anchored" data-anchor-id="workflow"><span class="header-section-number">8.7.1.2</span> Workflow</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-existing-tools_files/cellprofiler_identifyobjects.png" title="CellProfiler for nuclei and cell segmentation in easy to segment cells" class="img-fluid figure-img"></p>
<figcaption>CellProfiler for nuclei and cell segmentation</figcaption>
</figure>
</div>
<p>Elena usually uses CellProfiler as an image analysis workflow tool, using classical image processing methods for object identification in her pipeline. Elena typically performs her screens in A549, U2OS, or HeLa cells which are all considered easy to use for high-content microscopy because they are relatively consistent in size, relatively round, and can grow both sparsely and confluently while remaining in a monolayer. She’s quite used to making parameter changes to the IdentifyPrimaryObjects and IdentifySecondaryObjects modules to fine-tune her nuclei and cell segmentations, respectively. The image above shows shows the modules in her CellProfiler pipeline (left) and the outputs of her IdentifyPrimaryObjects (center) and IdentifySecondaryObjects modules (right).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-existing-tools_files/fiji_composite.png" title="Fiji for compositing a new png image" class="img-fluid figure-img"></p>
<figcaption>Fiji for compositing a new png image</figcaption>
</figure>
</div>
<p>Elena’s collaborator has acquired images in an adipocyte cell line and they are morphologically quite different from her typical screening cell lines. After being unsatisfied with what she was able to tune using classical segmentation in her standard Cellprofiler pipeline, Elena decides to turn to an AI tool. She starts by opening Cellpose in its web console as she has heard from colleagues that it is both very good out of the box and allows a user to fine tune it for their own data. She initially struggles to load one of her own images into the web tool but notices that the web page says “upload one PNG or JPG &lt;10 MB”. She also notices that the example images that are most similar to hers are in color, with a nuclear channel in blue and a cell marker in green. Elena’s raw images are in .tif format with a single grayscale channel per image so she uses Fiji to make a composite .png image. The image above shows her single-channel .tif images opened with FIJI (left, center) and the composite .png image that she created (right).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-existing-tools_files/cellpose_console_lipocytes.png" title="Cellpose web console" class="img-fluid figure-img"></p>
<figcaption>Cellpose web console</figcaption>
</figure>
</div>
<p>Elena loads her new composite .png into the Cellpose web console and sees that it has identified a number of reasonable cell objects but is not of the quality that she wants for use in her screen. However, Elena is excited to explore a new tool and has been actively builing her computational skills so she decides to proceed with Cellpose and see if she can improve the outputs. The image above shows her example image loaded into the Cellpose web console (far left) along with the predicted object outlines (center left), masks (center right), and flows (far right).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-existing-tools_files/runcellpose_in_cellprofiler.png" title="RunCellpose in CellProfiler" class="img-fluid figure-img"></p>
<figcaption>RunCellpose in CellProfiler</figcaption>
</figure>
</div>
<p>Elena takes advantage of the fact that Cellpose allows for fine-tuning of the pretrained deep learning models it provides. She re-trains Cellpose using a Colab notebook and sees that it now performs quite well on test images from her dataset. Because CellProfiler is also an image analysis workflow tool, Elena is able to insert a RunCellpose module in her original pipeline instead of the original secondary object identification module and keep the rest of her image handling and analysis the same while using the newer, better performing object identification that Cellpose provides. The image above shows her new CellProfiler pipeline with the RunCellpose module pointing to her custom fine-tuned Cellpose model.</p>
</section>
<section id="conclusion" class="level4" data-number="8.7.1.3">
<h4 data-number="8.7.1.3" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">8.7.1.3</span> Conclusion</h4>
<p>This case study starts with a scientist trying to use her standard classical image processing methodds for object identification but finding that she gets better results with a new challenging segmentation task by moving to an AI tool. It shows iterative improvements using the same tool in different ways and highlights some of the simple changes she needed to make to her overall workflow to use the new tool.</p>
</section>
</section>
<section id="case-study-2" class="level3" data-number="8.7.2">
<h3 data-number="8.7.2" class="anchored" data-anchor-id="case-study-2"><span class="header-section-number">8.7.2</span> Case Study 2:</h3>
<section id="introduction-2" class="level4" data-number="8.7.2.1">
<h4 data-number="8.7.2.1" class="anchored" data-anchor-id="introduction-2"><span class="header-section-number">8.7.2.1</span> Introduction</h4>
<p>Esteban is an image analyst, working with imaging data acquired by a collaborator. The collaborator studies zebrafish brains and sent Esteban fluorescent images taken in a large z-stack in live zebrafish. The zebrafish express a GFP-tagged protein in their microglia and the collaborators would like to be able to identify the microglia using an automated pipeline so that the researchers can ask biological questions such as where they are within the brain and whether they are close to other cell types (labeled in other fluorescent channels). The task is complicated because there is additional autofluorescence in the skin that is the same intensity as the microglia and they do not want skin autofluorescence identified. The images also have technical challenges because of the nature of their acquisition: because they are 3D live images, as you go deeper in the tissue the signal gets dimmer. This makes using normal <a href="./glossary.html#computer-vision">computer vision</a> segmentation quite hard.</p>
</section>
<section id="workflow-1" class="level4" data-number="8.7.2.2">
<h4 data-number="8.7.2.2" class="anchored" data-anchor-id="workflow-1"><span class="header-section-number">8.7.2.2</span> Workflow</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-existing-tools_files/napari_point_annotations.png" title="Point Annotations in napari" class="img-fluid figure-img"></p>
<figcaption>Point Annotations in napari</figcaption>
</figure>
</div>
<p>To help orient Esteban to the image data, he was given a spreadsheet of x,y,z coordinates of microglia that the collaborators manually annotated. Esteban started by opening the images and their point annotations in napari to orient himself to the data. The image above shows an example image opened in napari with red circles around the point annotations.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-existing-tools_files/napari_sam_guided_annotations.png" title="Guided Annotations in napari with SAM" class="img-fluid figure-img"></p>
<figcaption>Guided Annotations in napari with SAM</figcaption>
</figure>
</div>
<p>Esteban then loaded µSAM in napari to use as an annotation tool so that he could use the point annotations to make object masks. He tried using automatic segmentation but it didn’t work on his images. Instead, he used µSAM assisted segmentation, manually providing positive prompts, and got out reasonable segmentations of the microglia in the image. The image above is quite similar to the previous image but also has the µSAM segmentations in color overlays.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-existing-tools_files/napari_sam_3D_guided_annotations.png" title="Visualizing 3D annotations in napari" class="img-fluid figure-img"></p>
<figcaption>Visualizing 3D annotations in napari</figcaption>
</figure>
</div>
<p>Though Esteban has three dimensional data, the segmentations he is creating are 2D so he confirms that he is happy with his final masks by examining the image and masks in a 3D viewer. The above image shows a 3 dimensional view of his data that contains all of the image slices and annotations viewed together.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-existing-tools_files/cellpose_python_microglia.png" title="Cellpose predictions" class="img-fluid figure-img"></p>
<figcaption>Cellpose predictions</figcaption>
</figure>
</div>
<p>Now that Esteban has both images and masks, he calls Cellpose in Python and retrains the cyto model with his annotations. He runs the retrained model on a new image (left image) and finds that the cell probability prediction seems to identify microglia and autofluorescence (center image), but, unfortunately, the gradient prediction is not finding any cell objects (right image). Esteban’s training images are quite sparse (the vast majority of the image is background) so the Cellpose model learned that it can be mostly correct by deciding that <em>everything</em> is background.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-existing-tools_files/ilastik_3class_segmentation.png" title="ilastik 3 class predictions" class="img-fluid figure-img"></p>
<figcaption>ilastik 3 class predictions</figcaption>
</figure>
</div>
<p>Since neither a pre-trained model (µSAM) nor a fine-tuned model (Cellpose) worked on his images, Esteban decided to try pixel classification. He used ilastik, first providing annotations for microglia and background. The classifier worked quite well at distinguishing microglia from background, but classified the skin autofluorescence as microglia. Esteban built on this by using larger filters so that the classifier had more contextual information, training 3 classes (microglia, skin, and background) instead of 2, and adding in autocontext<span class="citation" data-cites="Kreshuk2019"><sup><a href="references.html#ref-Kreshuk2019" role="doc-biblioref">27</a></sup></span> which improves classification by running it in multiple stages. Esteban’s final 3-class pixel classifier performs well to identify microglia separately from skin across the images in his datasets. The above image shows a zoom-in of one of his images with the three classes overlaid - red for background pixels, blue for skin pixels, and yellow for microglia pixels.</p>
</section>
<section id="conclusion-1" class="level4" data-number="8.7.2.3">
<h4 data-number="8.7.2.3" class="anchored" data-anchor-id="conclusion-1"><span class="header-section-number">8.7.2.3</span> Conclusion</h4>
<p>This case study starts with a scientist trying multiple state-of-the-art deep learning segmentation tools for a specific, challenging segmentation task. He ultimately discovers that a machine learning pixel classifier works better for this task, illustrating that 1) “less-advanced” tools can perform better than deep learning tools and 2) biological expertise is always needed when inspecting any tool’s outputs.</p>
</section>
</section>
</section>
<section id="conclusion-2" class="level2" data-number="8.8">
<h2 data-number="8.8" class="anchored" data-anchor-id="conclusion-2"><span class="header-section-number">8.8</span> Conclusion</h2>
<p>As we shift our focus from hardware to software, it is common and understandable to feel overwhelmed - the bioimage analysis tool space is a constantly-changing array of best practices, and not all microscopists are highly computationally comfortable. This chapter therefore mostly talks about <em>how to make decisions</em>, as opposed to <em>use X tool for Y task</em>. Ultimately, the “right tool for your task” is the tool (or chain of tools) that allows you to be confident that your analysis has been performed <strong>well enough</strong>, and helps you understand the answer to your biological question and how confident you can be in your conclusion. Even if you try a tool and decide it isn’t the right solution for this question, this time still isn’t wasted - it simply enlarged the toolbox you can use for answering future questions. In the subsequent chapters, we will teach you about how to train models which could be important parts of your toolbox, but we suggest you keep our guidelines in mind as you finalize the decisions around when your model is “trained enough” and how you decide on your overall workflow.</p>


<div id="refs" class="references csl-bib-body" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-Stirling2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Stirling, D. R. <em>et al.</em> <span>CellProfiler</span> 4: Improvements in speed, utility and usability. <em>BMC Bioinformatics</em> <strong>22</strong>, 433 (2021).</div>
</div>
<div id="ref-Schindelin2012" class="csl-entry" role="listitem">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Schindelin, J. <em>et al.</em> Fiji: An open-source platform for biological-image analysis. <em>Nat. Methods</em> <strong>9</strong>, 676–682 (2012).</div>
</div>
<div id="ref-Shah" class="csl-entry" role="listitem">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Shah, R., Gogoberidze, N. &amp; Cimini, B. <a href="https://github.com/bilayer-containers/bilayers">Bilayers</a>.</div>
</div>
<div id="ref-von_chamier2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Chamier, L. von <em>et al.</em> <a href="https://doi.org/10.1038/s41467-021-22518-0">Democratising deep learning for microscopy with <span>ZeroCostDL4Mic</span></a>. <em>Nature Communications</em> <strong>12</strong>, 2276 (2021).</div>
</div>
<div id="ref-Hidalgo-Cenalmor2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Hidalgo-Cenalmor, I. <em>et al.</em> <span>DL4MicEverywhere</span>: Deep learning for microscopy made flexible, shareable and reproducible. <em>Nat. Methods</em> <strong>21</strong>, 925–927 (2024).</div>
</div>
<div id="ref-jupyter" class="csl-entry" role="listitem">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Kluyver, T. <em>et al.</em> <a href="https://eprints.soton.ac.uk/403913/">Jupyter notebooks - a publishing format for reproducible computational workflows</a>. in <em>Positioning and power in academic publishing: Players, agents and agendas</em> (eds. Loizides, F. &amp; Scmidt, B.) 87–90 (IOS Press, Netherlands, 2016).</div>
</div>
<div id="ref-Haase2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">Haase, R., Tischer, C., Bankhead, P., Miura, K. &amp; Cimini, B. A call for <span>FAIR</span> and open-access training materials to advance <span>BioImage</span> analysis. (2024).</div>
</div>
<div id="ref-Ljosa2012" class="csl-entry" role="listitem">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Ljosa, V., Sokolnicki, K. L. &amp; Carpenter, A. E. Annotated high-throughput microscopy image sets for validation. <em>Nat. Methods</em> <strong>9</strong>, 637 (2012).</div>
</div>
<div id="ref-Jamali2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Jamali, N., Dobson, E. T. A., Eliceiri, K. W., Carpenter, A. E. &amp; Cimini, B. A. 2020 <span>BioImage</span> analysis survey: Community experiences and needs for the future. <em>Biological Imaging</em> <strong>1</strong>, e4 (2021).</div>
</div>
<div id="ref-Sivagurunathan2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">Sivagurunathan, S. <em>et al.</em> Bridging imaging users to imaging analysis - a community survey. <em>J. Microsc.</em> (2023).</div>
</div>
<div id="ref-Cimini2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">11. </div><div class="csl-right-inline">Cimini, B. A. When to say ’good enough’. (2019).</div>
</div>
<div id="ref-Schindelin2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">12. </div><div class="csl-right-inline">Schindelin, J., Rueden, C. T., Hiner, M. C. &amp; Eliceiri, K. W. The <span>ImageJ</span> ecosystem: An open platform for biomedical image analysis. <em>Mol. Reprod. Dev.</em> <strong>82</strong>, 518–529 (2015).</div>
</div>
<div id="ref-napari" class="csl-entry" role="listitem">
<div class="csl-left-margin">13. </div><div class="csl-right-inline">Napari: A multi-dimensional image viewer for python. doi:<a href="https://doi.org/10.5281/zenodo.3555620">10.5281/zenodo.3555620</a>.</div>
</div>
<div id="ref-Bankhead2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">14. </div><div class="csl-right-inline">Bankhead, P. <em>et al.</em> <span>QuPath</span>: Open source software for digital pathology image analysis. <em>Sci. Rep.</em> <strong>7</strong>, 16878 (2017).</div>
</div>
<div id="ref-de-Chaumont2012" class="csl-entry" role="listitem">
<div class="csl-left-margin">15. </div><div class="csl-right-inline">Chaumont, F. de <em>et al.</em> Icy: An open bioimage informatics platform for extended reproducible research. <em>Nat. Methods</em> <strong>9</strong>, 690–696 (2012).</div>
</div>
<div id="ref-Berg2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">16. </div><div class="csl-right-inline">Berg, S. <em>et al.</em> Ilastik: Interactive machine learning for (bio)image analysis. <em>Nat. Methods</em> <strong>16</strong>, 1226–1232 (2019).</div>
</div>
<div id="ref-Arzt2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">17. </div><div class="csl-right-inline">Arzt, M. <em>et al.</em> <span>LABKIT</span>: Labeling and segmentation toolkit for big image data. <em>Front. Comput. Sci.</em> <strong>4</strong>, (2022).</div>
</div>
<div id="ref-Schmidt2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">18. </div><div class="csl-right-inline">Schmidt, U., Weigert, M., Broaddus, C. &amp; Myers, G. Cell detection with star-convex polygons. in <em>Medical image computing and computer assisted intervention – MICCAI 2018</em> 265–273 (Springer International Publishing, 2018).</div>
</div>
<div id="ref-Goldsborough2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">19. </div><div class="csl-right-inline">Goldsborough, T. <em>et al.</em> <span>InstanSeg</span>: An embedding-based instance segmentation algorithm optimized for accurate, efficient and portable cell segmentation. <em>arXiv [cs.CV]</em> (2024).</div>
</div>
<div id="ref-Pachitariu2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">20. </div><div class="csl-right-inline">Pachitariu, M. &amp; Stringer, C. Cellpose 2.0: How to train your own model. <em>Nat. Methods</em> <strong>19</strong>, 1634–1641 (2022).</div>
</div>
<div id="ref-Archit2025" class="csl-entry" role="listitem">
<div class="csl-left-margin">21. </div><div class="csl-right-inline">Archit, A. <em>et al.</em> Segment anything for microscopy. <em>Nat. Methods</em> <strong>22</strong>, 579–591 (2025).</div>
</div>
<div id="ref-Ouyang2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">22. </div><div class="csl-right-inline">Ouyang, W. <em>et al.</em> <span>BioImage</span> model zoo: A community-driven resource for accessible deep learning in <span>BioImage</span> analysis. <em>bioRxiv</em> 2022.06.07.495102 (2022).</div>
</div>
<div id="ref-Ouyang2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">23. </div><div class="csl-right-inline">Ouyang, W., Mueller, F., Hjelmare, M., Lundberg, E. &amp; Zimmer, C. <span>ImJoy</span>: An open-source computational platform for the deep learning era. <em>Nat. Methods</em> <strong>16</strong>, 1199–1200 (2019).</div>
</div>
<div id="ref-Gomez-de-Mariscal2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">24. </div><div class="csl-right-inline">Gómez-de-Mariscal, E. <em>et al.</em> <span>DeepImageJ</span>: A user-friendly environment to run deep learning models in <span>ImageJ</span>. <em>Nat. Methods</em> <strong>18</strong>, 1192–1195 (2021).</div>
</div>
<div id="ref-Zhang2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">25. </div><div class="csl-right-inline">Zhang, C. <em>et al.</em> Bio-image informatics index <span>BIII</span>: A unique database of image analysis tools and workflows for and by the bioimaging community. <em>arXiv [q-bio.QM]</em> (2023).</div>
</div>
<div id="ref-Rueden2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">26. </div><div class="csl-right-inline">Rueden, C. T. <em>et al.</em> Scientific community image forum: A discussion forum for scientific image software. <em>PLoS Biol.</em> <strong>17</strong>, e3000340 (2019).</div>
</div>
<div id="ref-Kreshuk2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">27. </div><div class="csl-right-inline">Kreshuk, A. &amp; Zhang, C. Machine learning: Advanced image segmentation using ilastik. <em>Methods Mol. Biol.</em> <strong>2040</strong>, 449–463 (2019).</div>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./7-smart-microscopy.html" class="pagination-link" aria-label="Adding AI to Your Hardware">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Adding AI to Your Hardware</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./9-train-models.html" class="pagination-link" aria-label="How to Train and Use Deep Learning Models in Microscopy">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">How to Train and Use Deep Learning Models in Microscopy</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>